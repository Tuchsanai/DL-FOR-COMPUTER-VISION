# %% [markdown]
# # üî¨ Lab: YOLO26 + Depth Estimation
# ## ‡∏Å‡∏≤‡∏£‡∏£‡∏ß‡∏° Object Detection ‡∏Å‡∏±‡∏ö Monocular Depth Estimation
#
# **‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå (Objectives):**
# 1. ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£ Depth Estimation (Monocular Depth Estimation)
# 2. ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô YOLO26 ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Object Detection
# 3. ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô MiDaS ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Depth Estimation
# 4. ‡∏£‡∏ß‡∏° YOLO26 + MiDaS ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì‡∏£‡∏∞‡∏¢‡∏∞‡∏´‡πà‡∏≤‡∏á‡∏Ç‡∏≠‡∏á‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏à‡∏≤‡∏Å‡∏Å‡∏•‡πâ‡∏≠‡∏á
# 5. ‡∏™‡∏£‡πâ‡∏≤‡∏á Visualization ‡πÅ‡∏ö‡∏ö 3D-aware
#
# **‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ (Tools):**
# - Ultralytics YOLO26 (Object Detection)
# - Intel MiDaS (Monocular Depth Estimation)
# - OpenCV, Matplotlib, NumPy
#
# ---

# %% [markdown]
# ## Part 1: ‡∏ó‡∏§‡∏©‡∏é‡∏µ Depth Estimation
#
# ### Depth Estimation ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?
#
# **Depth Estimation** ‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì‡∏£‡∏∞‡∏¢‡∏∞‡∏´‡πà‡∏≤‡∏á‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ pixel ‡πÉ‡∏ô‡∏†‡∏≤‡∏û‡∏à‡∏≤‡∏Å‡∏Å‡∏•‡πâ‡∏≠‡∏á
# ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Ñ‡∏∑‡∏≠ **Depth Map** ‡∏ó‡∏µ‡πà‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏°‡∏Ç‡∏≠‡∏á pixel ‡πÅ‡∏™‡∏î‡∏á‡∏ñ‡∏∂‡∏á‡∏£‡∏∞‡∏¢‡∏∞‡∏´‡πà‡∏≤‡∏á
#
# ### ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡∏≠‡∏á Depth Estimation
#
# | ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó | ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢ | ‡∏Ç‡πâ‡∏≠‡∏î‡∏µ | ‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏µ‡∏¢ |
# |--------|---------|------|--------|
# | **Stereo Vision** | ‡πÉ‡∏ä‡πâ‡∏Å‡∏•‡πâ‡∏≠‡∏á 2 ‡∏ï‡∏±‡∏ß ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏à‡∏≤‡∏Å disparity | ‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥ | ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡∏Å‡∏•‡πâ‡∏≠‡∏á 2 ‡∏ï‡∏±‡∏ß |
# | **LiDAR / ToF** | ‡πÉ‡∏ä‡πâ‡πÅ‡∏™‡∏á‡πÄ‡∏•‡πÄ‡∏ã‡∏≠‡∏£‡πå‡∏ß‡∏±‡∏î‡∏£‡∏∞‡∏¢‡∏∞ | ‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏°‡∏≤‡∏Å | ‡πÅ‡∏û‡∏á, hardware ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ |
# | **Monocular Depth** | ‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏û‡πÄ‡∏î‡∏µ‡∏¢‡∏ß + Deep Learning | ‡πÉ‡∏ä‡πâ‡∏Å‡∏•‡πâ‡∏≠‡∏á‡∏õ‡∏Å‡∏ï‡∏¥‡πÑ‡∏î‡πâ | ‡πÄ‡∏õ‡πá‡∏ô relative depth |
#
# ### MiDaS (Multiple Depth from a Single Image)
#
# MiDaS ‡πÄ‡∏õ‡πá‡∏ô model ‡∏à‡∏≤‡∏Å Intel Labs ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ Encoder-Decoder architecture
# - **Encoder**: ResNet / DPT (Dense Prediction Transformer) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö feature extraction
# - **Decoder**: Upsampling + Feature Fusion ‡∏™‡∏£‡πâ‡∏≤‡∏á depth map
# - ‡∏ù‡∏∂‡∏Å‡∏à‡∏≤‡∏Å 12+ datasets ‚Üí ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏Å‡∏±‡∏ö‡∏†‡∏≤‡∏û‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó
#
# ### ‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î: YOLO + Depth Estimation
#
# ```
# ‡∏†‡∏≤‡∏û Input
#    ‚îú‚îÄ‚îÄ YOLO26 ‚Üí Bounding Boxes + Class Labels (2D Detection)
#    ‚îî‚îÄ‚îÄ MiDaS  ‚Üí Depth Map (‡∏£‡∏∞‡∏¢‡∏∞‡∏´‡πà‡∏≤‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ pixel)
#         ‚Üì
#    ‡∏£‡∏ß‡∏°‡∏Å‡∏±‡∏ô ‚Üí ‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ä‡∏¥‡πâ‡∏ô + ‡∏£‡∏∞‡∏¢‡∏∞‡∏´‡πà‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì (Pseudo-3D)
# ```

# %% [markdown]
# ## Part 2: ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Library

# %%
# ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á library ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
# !pip install ultralytics opencv-python matplotlib numpy torch torchvision timm --quiet

# %%
import warnings
warnings.filterwarnings("ignore")

import os
import cv2
import torch
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image

print(f"PyTorch version: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# %% [markdown]
# ## Part 3: YOLO26 Object Detection
#
# YOLO26 ‡πÄ‡∏õ‡πá‡∏ô model ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏à‡∏≤‡∏Å Ultralytics (2025) ‡∏°‡∏µ‡∏Ñ‡∏∏‡∏ì‡∏™‡∏°‡∏ö‡∏±‡∏ï‡∏¥‡∏´‡∏•‡∏±‡∏Å:
# - **NMS-Free**: ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ Non-Maximum Suppression ‚Üí inference ‡πÄ‡∏£‡πá‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô
# - **DFL Removed**: ‡∏•‡∏ö Distribution Focal Loss ‚Üí deploy ‡∏á‡πà‡∏≤‡∏¢‡∏Ç‡∏∂‡πâ‡∏ô
# - **Edge Optimized**: CPU inference ‡πÄ‡∏£‡πá‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô 43%
# - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö: Detection, Segmentation, Pose, OBB, Classification

# %%
from ultralytics import YOLO

# ‡πÇ‡∏´‡∏•‡∏î YOLO26 nano model (pretrained ‡∏ö‡∏ô COCO dataset ‚Äî 80 classes)
model = YOLO("yolo26n.pt")

# ‡∏ó‡∏≥ inference ‡∏ö‡∏ô‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û
IMAGE_PATH = ".././images/football_teamplay.jpeg"
results = model(IMAGE_PATH, imgsz=640)

# %%
# ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå detection
result = results[0]

print("=" * 60)
print("üìä YOLO26 Detection Results")
print("=" * 60)
print(f"‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏ß‡∏à‡∏û‡∏ö: {len(result.boxes)}")
print(f"‡∏Ç‡∏ô‡∏≤‡∏î‡∏†‡∏≤‡∏û: {result.orig_shape}")
print(f"Classes ‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏ß‡∏à‡∏û‡∏ö: {result.boxes.cls.unique().tolist()}")
print()

# ‡πÅ‡∏™‡∏î‡∏á‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÅ‡∏ï‡πà‡∏•‡∏∞ detection
for i, box in enumerate(result.boxes):
    cls_id = int(box.cls[0])
    cls_name = result.names[cls_id]
    conf = float(box.conf[0])
    x1, y1, x2, y2 = box.xyxy[0].tolist()
    print(f"  [{i}] {cls_name:15s} | conf: {conf:.2f} | bbox: ({x1:.0f}, {y1:.0f}, {x2:.0f}, {y2:.0f})")

# %%
# Visualize YOLO26 detection
annotated_img = result.plot()
annotated_img_rgb = cv2.cvtColor(annotated_img, cv2.COLOR_BGR2RGB)

plt.figure(figsize=(12, 8))
plt.imshow(annotated_img_rgb)
plt.title("YOLO26 Object Detection", fontsize=16)
plt.axis("off")
plt.tight_layout()
plt.show()

# %% [markdown]
# ## Part 4: MiDaS Depth Estimation
#
# ### MiDaS Model Variants
#
# | Model | ‡∏Ç‡∏ô‡∏≤‡∏î | ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥ | ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß |
# |-------|------|-----------|---------|
# | **DPT_Large** | ‡πÉ‡∏´‡∏ç‡πà | ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î | ‡∏ä‡πâ‡∏≤‡∏™‡∏∏‡∏î |
# | **DPT_Hybrid** | ‡∏Å‡∏•‡∏≤‡∏á | ‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á | ‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á |
# | **MiDaS_small** | ‡πÄ‡∏•‡πá‡∏Å | ‡∏ï‡πà‡∏≥‡∏™‡∏∏‡∏î | ‡πÄ‡∏£‡πá‡∏ß‡∏™‡∏∏‡∏î |
#
# > ‚ö†Ô∏è MiDaS ‡πÉ‡∏´‡πâ **relative depth** (‡∏Ñ‡πà‡∏≤‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ó‡∏ò‡πå) ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà absolute depth (‡∏´‡∏ô‡πà‡∏ß‡∏¢‡πÄ‡∏°‡∏ï‡∏£)
# > ‡∏Ñ‡πà‡∏≤‡∏™‡∏π‡∏á = ‡πÑ‡∏Å‡∏•‡∏à‡∏≤‡∏Å‡∏Å‡∏•‡πâ‡∏≠‡∏á, ‡∏Ñ‡πà‡∏≤‡∏ï‡πà‡∏≥ = ‡πÉ‡∏Å‡∏•‡πâ‡∏Å‡∏•‡πâ‡∏≠‡∏á (inverse depth)

# %%
# ‡πÇ‡∏´‡∏•‡∏î MiDaS model ‡∏à‡∏≤‡∏Å PyTorch Hub
# ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å model_type: "DPT_Large", "DPT_Hybrid", "MiDaS_small"

model_type = "MiDaS_small"  # ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å small model (‡πÄ‡∏£‡πá‡∏ß, ‡πÉ‡∏ä‡πâ RAM ‡∏ô‡πâ‡∏≠‡∏¢)

print(f"üîÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î MiDaS model: {model_type}...")
midas = torch.hub.load("intel-isl/MiDaS", model_type)
midas.to(device)
midas.eval()
print(f"‚úÖ ‡πÇ‡∏´‡∏•‡∏î MiDaS ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à! (device: {device})")

# ‡πÇ‡∏´‡∏•‡∏î transform ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö preprocessing
midas_transforms = torch.hub.load("intel-isl/MiDaS", "transforms")

if model_type in ["DPT_Large", "DPT_Hybrid"]:
    transform = midas_transforms.dpt_transform
else:
    transform = midas_transforms.small_transform

print(f"‚úÖ ‡πÇ‡∏´‡∏•‡∏î transform ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!")

# %%
# ‡∏≠‡πà‡∏≤‡∏ô‡∏†‡∏≤‡∏û‡πÅ‡∏•‡∏∞‡∏ó‡∏≥ Depth Estimation
img_bgr = cv2.imread(IMAGE_PATH)
img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)

# Preprocessing: transform ‡∏†‡∏≤‡∏û
input_batch = transform(img_rgb).to(device)

print(f"üìê Input shape: {input_batch.shape}")

# Inference
with torch.no_grad():
    prediction = midas(input_batch)
    
    # Resize depth map ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î‡∏†‡∏≤‡∏û‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö
    prediction = torch.nn.functional.interpolate(
        prediction.unsqueeze(1),
        size=img_rgb.shape[:2],
        mode="bicubic",
        align_corners=False,
    ).squeeze()

depth_map = prediction.cpu().numpy()

print(f"‚úÖ Depth map shape: {depth_map.shape}")
print(f"üìä Depth range: [{depth_map.min():.2f}, {depth_map.max():.2f}]")

# %%
# Visualize Depth Map
fig, axes = plt.subplots(1, 3, figsize=(18, 6))

# ‡∏†‡∏≤‡∏û‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö
axes[0].imshow(img_rgb)
axes[0].set_title("Original Image", fontsize=14)
axes[0].axis("off")

# Depth Map (Viridis colormap)
im1 = axes[1].imshow(depth_map, cmap="inferno")
axes[1].set_title("Depth Map (Inferno)", fontsize=14)
axes[1].axis("off")
plt.colorbar(im1, ax=axes[1], fraction=0.046, pad=0.04, label="Relative Depth")

# Depth Map (Plasma colormap ‚Äî inverted)
depth_inv = depth_map.max() - depth_map  # invert: ‡πÉ‡∏Å‡∏•‡πâ = ‡∏™‡∏ß‡πà‡∏≤‡∏á
im2 = axes[2].imshow(depth_inv, cmap="plasma")
axes[2].set_title("Inverted Depth (‡πÉ‡∏Å‡∏•‡πâ = ‡∏™‡∏ß‡πà‡∏≤‡∏á)", fontsize=14)
axes[2].axis("off")
plt.colorbar(im2, ax=axes[2], fraction=0.046, pad=0.04, label="Inverted Depth")

plt.suptitle("MiDaS Monocular Depth Estimation", fontsize=16, fontweight="bold")
plt.tight_layout()
plt.show()

# %% [markdown]
# ## Part 5: ‡∏£‡∏ß‡∏° YOLO26 + Depth Estimation
#
# ### ‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î
# 1. ‡πÉ‡∏ä‡πâ YOLO26 ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏ ‚Üí ‡πÑ‡∏î‡πâ bounding box
# 2. ‡πÉ‡∏ä‡πâ MiDaS ‡∏™‡∏£‡πâ‡∏≤‡∏á depth map
# 3. ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ bounding box ‚Üí crop ‡∏™‡πà‡∏ß‡∏ô‡∏Ç‡∏≠‡∏á depth map ‚Üí ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢
# 4. ‡∏Ñ‡πà‡∏≤ depth ‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢ = ‡∏£‡∏∞‡∏¢‡∏∞‡∏´‡πà‡∏≤‡∏á‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ó‡∏ò‡πå‡∏Ç‡∏≠‡∏á‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏à‡∏≤‡∏Å‡∏Å‡∏•‡πâ‡∏≠‡∏á
# 5. ‡∏à‡∏±‡∏î‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö: ‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡πÑ‡∏´‡∏ô‡πÉ‡∏Å‡∏•‡πâ/‡πÑ‡∏Å‡∏•‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î

# %%
def estimate_object_depth(boxes, depth_map, names, method="median"):
    """
    ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì depth ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ detected object
    
    Parameters:
    -----------
    boxes : ultralytics Boxes object
    depth_map : numpy array ‚Äî depth map ‡∏à‡∏≤‡∏Å MiDaS
    names : dict ‚Äî class names mapping
    method : str ‚Äî ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì depth ("mean", "median", "center")
    
    Returns:
    --------
    list of dict ‚Äî ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ï‡πà‡∏•‡∏∞ object ‡∏û‡∏£‡πâ‡∏≠‡∏° depth
    """
    objects_with_depth = []
    
    for i, box in enumerate(boxes):
        x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())
        cls_id = int(box.cls[0])
        cls_name = names[cls_id]
        conf = float(box.conf[0])
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡∏≠‡∏ö‡πÄ‡∏Ç‡∏ï
        x1 = max(0, x1)
        y1 = max(0, y1)
        x2 = min(depth_map.shape[1], x2)
        y2 = min(depth_map.shape[0], y2)
        
        # Crop depth region ‡∏ï‡∏≤‡∏° bounding box
        depth_region = depth_map[y1:y2, x1:x2]
        
        if depth_region.size == 0:
            continue
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì depth ‡∏ï‡∏≤‡∏° method ‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å
        if method == "mean":
            obj_depth = np.mean(depth_region)
        elif method == "median":
            obj_depth = np.median(depth_region)
        elif method == "center":
            # ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ depth ‡∏ó‡∏µ‡πà‡∏à‡∏∏‡∏î‡∏Å‡∏•‡∏≤‡∏á bounding box
            cx = (x1 + x2) // 2
            cy = (y1 + y2) // 2
            obj_depth = depth_map[cy, cx]
        else:
            obj_depth = np.mean(depth_region)
        
        objects_with_depth.append({
            "id": i,
            "class": cls_name,
            "confidence": conf,
            "bbox": (x1, y1, x2, y2),
            "depth_value": obj_depth,
            "depth_std": np.std(depth_region),
            "bbox_area": (x2 - x1) * (y2 - y1),
        })
    
    # ‡∏à‡∏±‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏° depth (MiDaS: ‡∏Ñ‡πà‡∏≤‡∏°‡∏≤‡∏Å = ‡πÑ‡∏Å‡∏•)
    objects_with_depth.sort(key=lambda x: x["depth_value"])
    
    return objects_with_depth

# %%
# ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì depth ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ object
objects = estimate_object_depth(result.boxes, depth_map, result.names, method="median")

# Normalize depth ‡πÄ‡∏õ‡πá‡∏ô 0-100 ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•
depth_values = [obj["depth_value"] for obj in objects]
if depth_values:
    d_min, d_max = min(depth_values), max(depth_values)
    for obj in objects:
        if d_max > d_min:
            obj["depth_normalized"] = ((obj["depth_value"] - d_min) / (d_max - d_min)) * 100
        else:
            obj["depth_normalized"] = 50.0

# ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
print("=" * 80)
print("üìä YOLO26 + Depth Estimation Results")
print("=" * 80)
print(f"{'#':>3} {'Class':>12} {'Conf':>6} {'Depth':>10} {'Norm':>8} {'Rank':>6}")
print("-" * 80)

for rank, obj in enumerate(objects, 1):
    distance_label = "üü¢ ‡πÉ‡∏Å‡∏•‡πâ" if obj["depth_normalized"] < 33 else "üü° ‡∏Å‡∏•‡∏≤‡∏á" if obj["depth_normalized"] < 66 else "üî¥ ‡πÑ‡∏Å‡∏•"
    print(f"{obj['id']:>3} {obj['class']:>12} {obj['confidence']:>6.2f} "
          f"{obj['depth_value']:>10.2f} {obj['depth_normalized']:>7.1f}% "
          f"{distance_label}")

# %% [markdown]
# ## Part 6: Visualization ‚Äî Annotated Image with Depth
#
# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡πÅ‡∏™‡∏î‡∏á‡∏ó‡∏±‡πâ‡∏á bounding box ‡πÅ‡∏•‡∏∞‡∏£‡∏∞‡∏¢‡∏∞‡∏´‡πà‡∏≤‡∏á‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏

# %%
def draw_depth_annotated_image(image_rgb, objects, depth_map):
    """
    ‡∏ß‡∏≤‡∏î bounding box ‡∏û‡∏£‡πâ‡∏≠‡∏° depth annotation ‡∏ö‡∏ô‡∏†‡∏≤‡∏û
    ‡∏™‡∏µ‡∏Ç‡∏≠‡∏á box ‡∏à‡∏∞‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ï‡∏≤‡∏° depth (‡πÉ‡∏Å‡∏•‡πâ = ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ß, ‡πÑ‡∏Å‡∏• = ‡πÅ‡∏î‡∏á)
    """
    img_annotated = image_rgb.copy()
    
    for obj in objects:
        x1, y1, x2, y2 = obj["bbox"]
        norm = obj["depth_normalized"] / 100.0  # 0 = ‡πÉ‡∏Å‡∏•‡πâ, 1 = ‡πÑ‡∏Å‡∏•
        
        # ‡∏™‡∏µ: ‡πÉ‡∏Å‡∏•‡πâ = ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ß (0,255,0), ‡πÑ‡∏Å‡∏• = ‡πÅ‡∏î‡∏á (255,0,0)
        r = int(255 * norm)
        g = int(255 * (1 - norm))
        b = 0
        color = (r, g, b)
        
        # ‡∏ß‡∏≤‡∏î bounding box
        cv2.rectangle(img_annotated, (x1, y1), (x2, y2), color, 2)
        
        # Label: class + depth
        label = f"{obj['class']} | D:{obj['depth_normalized']:.0f}%"
        
        # Background ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö text
        (tw, th), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)
        cv2.rectangle(img_annotated, (x1, y1 - th - 8), (x1 + tw + 4, y1), color, -1)
        
        # Text ‡∏™‡∏µ‡∏Ç‡∏≤‡∏ß
        cv2.putText(img_annotated, label, (x1 + 2, y1 - 4),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)
    
    return img_annotated

# %%
# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏†‡∏≤‡∏û annotated
annotated = draw_depth_annotated_image(img_rgb, objects, depth_map)

# ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•
fig, axes = plt.subplots(1, 2, figsize=(18, 8))

axes[0].imshow(annotated)
axes[0].set_title("YOLO26 Detection + Depth Annotation\n(üü¢ ‡πÉ‡∏Å‡∏•‡πâ ‚Üí üî¥ ‡πÑ‡∏Å‡∏•)", fontsize=14)
axes[0].axis("off")

# Depth Map overlay
depth_normalized = (depth_map - depth_map.min()) / (depth_map.max() - depth_map.min())
axes[1].imshow(img_rgb, alpha=0.5)
axes[1].imshow(depth_normalized, cmap="inferno", alpha=0.5)
axes[1].set_title("Original + Depth Map Overlay", fontsize=14)
axes[1].axis("off")

plt.suptitle("YOLO26 + MiDaS Depth Estimation", fontsize=16, fontweight="bold")
plt.tight_layout()
plt.show()

# %% [markdown]
# ## Part 7: Depth-based Object Sorting & Visualization
#
# ‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏ï‡∏≤‡∏°‡∏£‡∏∞‡∏¢‡∏∞‡∏´‡πà‡∏≤‡∏á: ‡πÉ‡∏Å‡∏•‡πâ (Near), ‡∏Å‡∏•‡∏≤‡∏á (Mid), ‡πÑ‡∏Å‡∏• (Far)

# %%
def categorize_by_depth(objects):
    """‡πÅ‡∏ö‡πà‡∏á‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡πÄ‡∏õ‡πá‡∏ô 3 ‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ï‡∏≤‡∏° depth"""
    near = [o for o in objects if o["depth_normalized"] < 33]
    mid  = [o for o in objects if 33 <= o["depth_normalized"] < 66]
    far  = [o for o in objects if o["depth_normalized"] >= 66]
    return near, mid, far

near_objects, mid_objects, far_objects = categorize_by_depth(objects)

print("=" * 60)
print("üìè Object Distance Categorization")
print("=" * 60)

print(f"\nüü¢ NEAR (‡πÉ‡∏Å‡∏•‡πâ) ‚Äî {len(near_objects)} objects:")
for o in near_objects:
    print(f"   ‚Ä¢ {o['class']} (conf: {o['confidence']:.2f}, depth: {o['depth_normalized']:.1f}%)")

print(f"\nüü° MID (‡∏Å‡∏•‡∏≤‡∏á) ‚Äî {len(mid_objects)} objects:")
for o in mid_objects:
    print(f"   ‚Ä¢ {o['class']} (conf: {o['confidence']:.2f}, depth: {o['depth_normalized']:.1f}%)")

print(f"\nüî¥ FAR (‡πÑ‡∏Å‡∏•) ‚Äî {len(far_objects)} objects:")
for o in far_objects:
    print(f"   ‚Ä¢ {o['class']} (conf: {o['confidence']:.2f}, depth: {o['depth_normalized']:.1f}%)")

# %%
# Bar Chart: Depth ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ Object
if objects:
    fig, axes = plt.subplots(1, 2, figsize=(16, 6))
    
    # Bar chart
    labels = [f"{o['class']}_{o['id']}" for o in objects]
    depths = [o["depth_normalized"] for o in objects]
    colors = [plt.cm.RdYlGn_r(d / 100.0) for d in depths]
    
    bars = axes[0].barh(labels, depths, color=colors, edgecolor="black", linewidth=0.5)
    axes[0].set_xlabel("Relative Depth (%)", fontsize=12)
    axes[0].set_title("Object Depth Ranking\n(0% = ‡πÉ‡∏Å‡∏•‡πâ‡∏™‡∏∏‡∏î, 100% = ‡πÑ‡∏Å‡∏•‡∏™‡∏∏‡∏î)", fontsize=14)
    axes[0].axvline(x=33, color="green", linestyle="--", alpha=0.5, label="Near/Mid boundary")
    axes[0].axvline(x=66, color="red", linestyle="--", alpha=0.5, label="Mid/Far boundary")
    axes[0].legend()
    axes[0].set_xlim(0, 105)
    
    # Scatter plot: BBox Area vs Depth
    areas = [o["bbox_area"] for o in objects]
    axes[1].scatter(depths, areas, c=colors, s=100, edgecolors="black", linewidth=0.5)
    for o in objects:
        axes[1].annotate(f"{o['class']}_{o['id']}", 
                        (o["depth_normalized"], o["bbox_area"]),
                        textcoords="offset points", xytext=(5, 5), fontsize=8)
    axes[1].set_xlabel("Relative Depth (%)", fontsize=12)
    axes[1].set_ylabel("Bounding Box Area (pixels¬≤)", fontsize=12)
    axes[1].set_title("BBox Area vs Depth\n(‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡πÉ‡∏Å‡∏•‡πâ‡∏°‡∏±‡∏Å‡∏°‡∏µ bbox ‡πÉ‡∏´‡∏ç‡πà‡∏Å‡∏ß‡πà‡∏≤)", fontsize=14)
    
    plt.tight_layout()
    plt.show()

# %% [markdown]
# ## Part 8: Depth Estimation ‡∏ö‡∏ô Video (Frame-by-Frame)
#
# ‡∏õ‡∏£‡∏∞‡∏¢‡∏∏‡∏Å‡∏ï‡πå‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö video ‡πÇ‡∏î‡∏¢‡∏ó‡∏≥ detection + depth estimation ‡∏ó‡∏µ‡∏•‡∏∞ frame

# %%
def process_video_with_depth(video_path, yolo_model, midas_model, midas_transform,
                              device, max_frames=30, conf_threshold=0.4):
    """
    ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• video: YOLO26 detection + MiDaS depth estimation
    
    Parameters:
    -----------
    video_path : str ‚Äî path ‡πÑ‡∏õ‡∏¢‡∏±‡∏á video file
    yolo_model : YOLO model
    midas_model : MiDaS model
    midas_transform : MiDaS preprocessing transform
    device : torch.device
    max_frames : int ‚Äî ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô frame ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•
    conf_threshold : float ‚Äî confidence threshold ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö YOLO
    
    Returns:
    --------
    list of dict ‚Äî ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÅ‡∏ï‡πà‡∏•‡∏∞ frame
    """
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏õ‡∏¥‡∏î video: {video_path}")
        return []
    
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    fps = cap.get(cv2.CAP_PROP_FPS)
    print(f"üìπ Video: {total_frames} frames, {fps:.1f} FPS")
    print(f"üìä ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• {min(max_frames, total_frames)} frames...")
    
    frame_results = []
    frame_count = 0
    
    while cap.isOpened() and frame_count < max_frames:
        ret, frame = cap.read()
        if not ret:
            break
        
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        
        # YOLO26 Detection
        yolo_results = yolo_model(frame_rgb, imgsz=640, conf=conf_threshold, verbose=False)
        
        # MiDaS Depth Estimation
        input_batch = midas_transform(frame_rgb).to(device)
        with torch.no_grad():
            depth_pred = midas_model(input_batch)
            depth_pred = torch.nn.functional.interpolate(
                depth_pred.unsqueeze(1),
                size=frame_rgb.shape[:2],
                mode="bicubic",
                align_corners=False,
            ).squeeze()
        depth_frame = depth_pred.cpu().numpy()
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì depth ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ object
        result = yolo_results[0]
        objects = estimate_object_depth(result.boxes, depth_frame, result.names, method="median")
        
        frame_results.append({
            "frame_id": frame_count,
            "num_objects": len(objects),
            "objects": objects,
            "depth_map": depth_frame,
            "frame_rgb": frame_rgb,
        })
        
        frame_count += 1
        if frame_count % 10 == 0:
            print(f"  ‚úÖ Frame {frame_count}/{min(max_frames, total_frames)}")
    
    cap.release()
    print(f"üé¨ ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à: {frame_count} frames")
    return frame_results

# %%
# ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏Å‡∏±‡∏ö video (uncomment ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏°‡∏µ video file)
# VIDEO_PATH = ".././videos/sample.mp4"
# 
# video_results = process_video_with_depth(
#     VIDEO_PATH, model, midas, transform, device,
#     max_frames=30, conf_threshold=0.4
# )
# 
# # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏• frame ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á
# if video_results:
#     sample = video_results[0]
#     annotated = draw_depth_annotated_image(
#         sample["frame_rgb"], sample["objects"], sample["depth_map"]
#     )
#     plt.figure(figsize=(12, 8))
#     plt.imshow(annotated)
#     plt.title(f"Frame {sample['frame_id']} ‚Äî {sample['num_objects']} objects detected")
#     plt.axis("off")
#     plt.show()

print("üìù Video processing function ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô!")
print("   Uncomment code ‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏°‡∏µ video file")

# %% [markdown]
# ## Part 9: ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö MiDaS Models
#
# ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö MiDaS 3 ‡∏£‡∏∏‡πà‡∏ô: DPT_Large, DPT_Hybrid, MiDaS_small

# %%
import time

def compare_midas_models(image_path, model_types=None):
    """
    ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö MiDaS models ‡∏ï‡πà‡∏≤‡∏á‡πÜ
    """
    if model_types is None:
        model_types = ["MiDaS_small"]  # ‡πÉ‡∏ä‡πâ‡πÅ‡∏Ñ‡πà small ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß
        # model_types = ["MiDaS_small", "DPT_Hybrid", "DPT_Large"]  # full comparison
    
    img = cv2.imread(image_path)
    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    
    results_compare = {}
    
    for mt in model_types:
        print(f"\nüîÑ Loading {mt}...")
        m = torch.hub.load("intel-isl/MiDaS", mt)
        m.to(device)
        m.eval()
        
        t = torch.hub.load("intel-isl/MiDaS", "transforms")
        if mt in ["DPT_Large", "DPT_Hybrid"]:
            tf = t.dpt_transform
        else:
            tf = t.small_transform
        
        input_batch = tf(img_rgb).to(device)
        
        # Warm up
        with torch.no_grad():
            _ = m(input_batch)
        
        # Benchmark
        start = time.time()
        n_runs = 5
        for _ in range(n_runs):
            with torch.no_grad():
                pred = m(input_batch)
        elapsed = (time.time() - start) / n_runs
        
        pred = torch.nn.functional.interpolate(
            pred.unsqueeze(1), size=img_rgb.shape[:2],
            mode="bicubic", align_corners=False
        ).squeeze()
        
        depth = pred.cpu().numpy()
        results_compare[mt] = {
            "depth_map": depth,
            "time_ms": elapsed * 1000,
        }
        print(f"  ‚úÖ {mt}: {elapsed*1000:.1f}ms/frame")
        
        # ‡∏•‡∏ö model ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å memory
        del m
        torch.cuda.empty_cache() if torch.cuda.is_available() else None
    
    return results_compare

# %%
# ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö (‡πÉ‡∏ä‡πâ‡πÅ‡∏Ñ‡πà small model ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß)
comparison = compare_midas_models(IMAGE_PATH, model_types=["MiDaS_small"])

# Uncomment ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ó‡∏∏‡∏Å model (‡πÉ‡∏ä‡πâ RAM ‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô):
# comparison = compare_midas_models(IMAGE_PATH, model_types=["MiDaS_small", "DPT_Hybrid", "DPT_Large"])

# %%
# Visualize comparison
n_models = len(comparison)
fig, axes = plt.subplots(1, n_models + 1, figsize=(6 * (n_models + 1), 5))

if n_models == 1:
    axes = [axes] if not isinstance(axes, np.ndarray) else axes.tolist()

# Original image
ax0 = axes[0] if isinstance(axes, list) else axes[0]
ax0.imshow(img_rgb)
ax0.set_title("Original", fontsize=12)
ax0.axis("off")

for idx, (mt, data) in enumerate(comparison.items(), 1):
    ax = axes[idx] if isinstance(axes, list) else axes[idx]
    ax.imshow(data["depth_map"], cmap="inferno")
    ax.set_title(f"{mt}\n({data['time_ms']:.1f} ms)", fontsize=12)
    ax.axis("off")

plt.suptitle("MiDaS Model Comparison", fontsize=14, fontweight="bold")
plt.tight_layout()
plt.show()

# %% [markdown]
# ## Part 10: Advanced ‚Äî Depth-Aware Object Priority
#
# ### Use Case: ‡∏£‡∏∞‡∏ö‡∏ö ADAS (Advanced Driver Assistance System)
# ‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏ó‡∏µ‡πà‡πÉ‡∏Å‡∏•‡πâ‡∏Å‡∏ß‡πà‡∏≤ ‚Üí ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤ ‚Üí ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô‡∏Å‡πà‡∏≠‡∏ô

# %%
def compute_danger_score(obj, weights=None):
    """
    ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì danger score ‡∏Ç‡∏≠‡∏á object (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö ADAS scenario)
    
    ‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏ó‡∏µ‡πà‡πÉ‡∏Å‡∏•‡πâ + ‡πÉ‡∏´‡∏ç‡πà + ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏ô ‚Üí ‡∏≠‡∏±‡∏ô‡∏ï‡∏£‡∏≤‡∏¢‡∏°‡∏≤‡∏Å
    
    Parameters:
    -----------
    obj : dict ‚Äî object info ‡∏à‡∏≤‡∏Å estimate_object_depth
    weights : dict ‚Äî ‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ factor
    
    Returns:
    --------
    float ‚Äî danger score (0-100)
    """
    if weights is None:
        weights = {"proximity": 0.5, "size": 0.2, "class_risk": 0.3}
    
    # Proximity score (‡πÉ‡∏Å‡∏•‡πâ = ‡∏≠‡∏±‡∏ô‡∏ï‡∏£‡∏≤‡∏¢‡∏°‡∏≤‡∏Å) ‚Äî invert depth
    proximity = 100 - obj["depth_normalized"]
    
    # Size score (‡πÉ‡∏´‡∏ç‡πà = ‡∏≠‡∏±‡∏ô‡∏ï‡∏£‡∏≤‡∏¢‡∏°‡∏≤‡∏Å)
    max_area = 500 * 500  # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö normalize
    size_score = min(100, (obj["bbox_area"] / max_area) * 100)
    
    # Class risk (‡∏ö‡∏≤‡∏á class ‡∏≠‡∏±‡∏ô‡∏ï‡∏£‡∏≤‡∏¢‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤)
    high_risk_classes = {"person": 100, "bicycle": 80, "motorcycle": 80, "car": 70, 
                         "bus": 60, "truck": 60, "dog": 70, "cat": 50}
    class_risk = high_risk_classes.get(obj["class"], 30)
    
    # Weighted sum
    danger = (weights["proximity"] * proximity +
              weights["size"] * size_score +
              weights["class_risk"] * class_risk)
    
    return min(100, danger)

# %%
# ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì danger score
if objects:
    for obj in objects:
        obj["danger_score"] = compute_danger_score(obj)
    
    # ‡∏à‡∏±‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏° danger score (‡∏™‡∏π‡∏á = ‡∏≠‡∏±‡∏ô‡∏ï‡∏£‡∏≤‡∏¢‡∏°‡∏≤‡∏Å)
    objects_by_danger = sorted(objects, key=lambda x: x["danger_score"], reverse=True)
    
    print("=" * 70)
    print("‚ö†Ô∏è  DANGER SCORE RANKING (ADAS Scenario)")
    print("=" * 70)
    print(f"{'Rank':>4} {'Class':>12} {'Depth%':>8} {'Area':>8} {'Danger':>8} {'Alert':>10}")
    print("-" * 70)
    
    for rank, obj in enumerate(objects_by_danger, 1):
        danger = obj["danger_score"]
        if danger >= 70:
            alert = "üö® HIGH"
        elif danger >= 40:
            alert = "‚ö†Ô∏è MEDIUM"
        else:
            alert = "‚úÖ LOW"
        
        print(f"{rank:>4} {obj['class']:>12} {obj['depth_normalized']:>7.1f}% "
              f"{obj['bbox_area']:>8} {danger:>7.1f} {alert:>10}")

# %% [markdown]
# ## Part 11: ‡∏™‡∏£‡πâ‡∏≤‡∏á Top-Down View (Bird's Eye View)
#
# ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡πÅ‡∏ö‡∏ö‡∏°‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô (‡πÅ‡∏ú‡∏ô‡∏ú‡∏±‡∏á)

# %%
def create_topdown_view(objects, img_width, img_height, figsize=(8, 10)):
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á top-down view ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á x (‡∏ã‡πâ‡∏≤‡∏¢-‡∏Ç‡∏ß‡∏≤) vs depth (‡πÉ‡∏Å‡∏•‡πâ-‡πÑ‡∏Å‡∏•)
    """
    if not objects:
        print("‡πÑ‡∏°‡πà‡∏°‡∏µ object ‡πÉ‡∏´‡πâ‡πÅ‡∏™‡∏î‡∏á")
        return
    
    fig, ax = plt.subplots(figsize=figsize)
    
    for obj in objects:
        x1, y1, x2, y2 = obj["bbox"]
        cx = (x1 + x2) / 2  # center x
        depth_pct = obj["depth_normalized"]
        danger = obj.get("danger_score", 50)
        
        # Normalize x position (0-100)
        x_norm = (cx / img_width) * 100
        
        # ‡∏™‡∏µ‡∏ï‡∏≤‡∏° danger score
        color = plt.cm.RdYlGn_r(danger / 100)
        
        # ‡∏Ç‡∏ô‡∏≤‡∏î marker ‡∏ï‡∏≤‡∏° bbox area
        marker_size = max(50, min(500, obj["bbox_area"] / 100))
        
        ax.scatter(x_norm, depth_pct, s=marker_size, c=[color], 
                  edgecolors="black", linewidth=1, zorder=5, alpha=0.8)
        ax.annotate(f"{obj['class']}\n({danger:.0f})", 
                   (x_norm, depth_pct),
                   textcoords="offset points", xytext=(10, 5),
                   fontsize=8, ha="left")
    
    # ‡∏Å‡∏•‡πâ‡∏≠‡∏á (‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á)
    ax.scatter(50, -5, s=200, c="blue", marker="^", zorder=10, label="üì∑ Camera")
    ax.annotate("üì∑ Camera", (50, -5), textcoords="offset points", 
               xytext=(0, -15), fontsize=10, ha="center", fontweight="bold")
    
    # ‡πÅ‡∏ö‡πà‡∏á‡πÇ‡∏ã‡∏ô
    ax.axhspan(-10, 33, alpha=0.1, color="green", label="Near Zone")
    ax.axhspan(33, 66, alpha=0.1, color="yellow", label="Mid Zone")
    ax.axhspan(66, 110, alpha=0.1, color="red", label="Far Zone")
    
    ax.set_xlim(-5, 105)
    ax.set_ylim(-10, 110)
    ax.set_xlabel("Horizontal Position (Left ‚Üí Right)", fontsize=12)
    ax.set_ylabel("Relative Depth (Near ‚Üí Far)", fontsize=12)
    ax.set_title("üó∫Ô∏è Top-Down View (Bird's Eye View)\n‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏à‡∏∏‡∏î = 1 detected object", fontsize=14)
    ax.legend(loc="upper right", fontsize=9)
    ax.grid(True, alpha=0.3)
    ax.invert_yaxis()  # ‡πÉ‡∏Å‡∏•‡πâ‡∏≠‡∏¢‡∏π‡πà‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô
    
    plt.tight_layout()
    plt.show()

# %%
if objects:
    h, w = img_rgb.shape[:2]
    create_topdown_view(objects, w, h)

# %% [markdown]
# ## Part 12: ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡πÅ‡∏•‡∏∞‡πÅ‡∏ö‡∏ö‡∏ù‡∏∂‡∏Å‡∏´‡∏±‡∏î
#
# ### ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÉ‡∏ô‡πÅ‡∏•‡πá‡∏ö‡∏ô‡∏µ‡πâ
#
# 1. **YOLO26** ‚Äî model ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏à‡∏≤‡∏Å Ultralytics ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö real-time object detection
#    - NMS-Free, DFL Removed, Edge Optimized
#    - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Detection, Segmentation, Pose, OBB, Classification
#
# 2. **MiDaS** ‚Äî Monocular Depth Estimation ‡∏à‡∏≤‡∏Å Intel Labs
#    - ‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì relative depth ‡∏à‡∏≤‡∏Å‡∏†‡∏≤‡∏û‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
#    - 3 ‡∏£‡∏∏‡πà‡∏ô: DPT_Large (‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥), DPT_Hybrid (‡∏™‡∏°‡∏î‡∏∏‡∏•), MiDaS_small (‡πÄ‡∏£‡πá‡∏ß)
#
# 3. **‡∏Å‡∏≤‡∏£‡∏£‡∏ß‡∏° YOLO + Depth** ‚Äî Pseudo-3D Object Detection
#    - ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏ (2D) + ‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì‡∏£‡∏∞‡∏¢‡∏∞‡∏´‡πà‡∏≤‡∏á (Z-axis)
#    - ‡∏õ‡∏£‡∏∞‡∏¢‡∏∏‡∏Å‡∏ï‡πå‡πÉ‡∏ä‡πâ: ADAS, Robotics, AR/VR, Surveillance
#
# ### ‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î (Limitations)
#
# - MiDaS ‡πÉ‡∏´‡πâ **relative depth** ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà absolute distance (‡πÄ‡∏°‡∏ï‡∏£)
# - ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏Ç‡∏∂‡πâ‡∏ô‡∏≠‡∏¢‡∏π‡πà‡∏Å‡∏±‡∏ö scene complexity
# - ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ calibration ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö metric depth
#
# ---
#
# ### üìù ‡πÅ‡∏ö‡∏ö‡∏ù‡∏∂‡∏Å‡∏´‡∏±‡∏î (Exercises)
#
# **Exercise 1:** ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô MiDaS model ‡πÄ‡∏õ‡πá‡∏ô `DPT_Large` ‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö depth map
# ‡∏Å‡∏±‡∏ö `MiDaS_small` ‚Äî ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£?
#
# **Exercise 2:** ‡πÉ‡∏ä‡πâ YOLO26 model ‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà‡∏Ç‡∏∂‡πâ‡∏ô ‡πÄ‡∏ä‡πà‡∏ô `yolo26s.pt` ‡∏´‡∏£‡∏∑‡∏≠ `yolo26m.pt`
# ‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏ß‡∏à‡∏û‡∏ö
#
# **Exercise 3:** ‡∏ó‡∏î‡∏•‡∏≠‡∏á‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô method ‡πÉ‡∏ô `estimate_object_depth()` 
# ‡∏à‡∏≤‡∏Å "median" ‡πÄ‡∏õ‡πá‡∏ô "mean" ‡∏´‡∏£‡∏∑‡∏≠ "center" ‚Äî ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£?
#
# **Exercise 4:** ‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏û‡∏≠‡∏∑‡πà‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏´‡∏•‡∏≤‡∏¢‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡∏∂‡∏Å (‡πÄ‡∏ä‡πà‡∏ô ‡∏†‡∏≤‡∏û‡∏ñ‡∏ô‡∏ô, ‡∏†‡∏≤‡∏û‡∏´‡πâ‡∏≠‡∏á)
# ‡πÅ‡∏•‡πâ‡∏ß‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ß‡πà‡∏≤ depth estimation ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏î‡∏µ‡πÅ‡∏Ñ‡πà‡πÑ‡∏´‡∏ô
#
# **Exercise 5 (Advanced):** ‡∏õ‡∏£‡∏±‡∏ö `compute_danger_score()` ‡πÉ‡∏´‡πâ‡∏£‡∏ß‡∏° velocity estimation
# ‡πÇ‡∏î‡∏¢‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö depth ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á frames ‡πÉ‡∏ô video
#
# ---
#
# ### üìö References
#
# - [Ultralytics YOLO26 Docs](https://docs.ultralytics.com/models/yolo26/)
# - [MiDaS GitHub](https://github.com/isl-org/MiDaS)
# - [MiDaS Paper: Towards Robust Monocular Depth Estimation (TPAMI 2022)](https://arxiv.org/abs/1907.01341)
# - [Ultralytics Depth Estimation Guide](https://www.ultralytics.com/glossary/depth-estimation)