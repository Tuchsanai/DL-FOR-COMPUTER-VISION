# %% [markdown]
# # Lab: SAM 3 Advanced Concept Segmentation and Video Tracking
# **Course:** Advanced Computer Vision with MLOps  
# **Topic:** Segment Anything Model 3 (SAM 3) - Promptable Concept Segmentation
#
# ---
#
# ## ‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ (Learning Objectives)
#
# ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏à‡∏ö Lab ‡∏ô‡∏µ‡πâ ‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏à‡∏∞‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ:
# 1. ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á SAM 2 ‡πÅ‡∏•‡∏∞ SAM 3
# 2. ‡πÉ‡∏ä‡πâ SAM 3 ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥ Concept Segmentation ‡∏î‡πâ‡∏ß‡∏¢ Text Prompts
# 3. ‡πÉ‡∏ä‡πâ Image Exemplar Prompts ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ Object ‡∏ó‡∏µ‡πà‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Å‡∏±‡∏ô
# 4. ‡∏õ‡∏£‡∏∞‡∏¢‡∏∏‡∏Å‡∏ï‡πå‡πÉ‡∏ä‡πâ SAM 3 ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ Track Objects ‡πÉ‡∏ô Video
# 5. ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏Ç‡∏≠‡∏á Model ‡∏î‡πâ‡∏ß‡∏¢ Metrics ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°
# 6. ‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö Multi-Object Tracking ‡πÅ‡∏•‡∏∞ Interactive Refinement
#
# ---
#
# ## ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô (Background)
#
# ### SAM 3 ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?
#
# SAM 3 (Segment Anything Model 3) ‡πÄ‡∏õ‡πá‡∏ô Foundation Model ‡∏à‡∏≤‡∏Å Meta ‡∏ó‡∏µ‡πà‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö‡∏°‡∏≤‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö **Promptable Concept Segmentation (PCS)** ‡πÇ‡∏î‡∏¢‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏´‡∏•‡∏±‡∏Å‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ:
#
# - **Text-based Segmentation**: ‡πÉ‡∏ä‡πâ Noun Phrases ‡πÄ‡∏ä‡πà‡∏ô "yellow school bus" ‡∏´‡∏£‡∏∑‡∏≠ "person wearing red hat"
# - **Image Exemplar Prompts**: ‡πÉ‡∏ä‡πâ Bounding Box ‡∏Ç‡∏≠‡∏á Object ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ Object ‡∏ó‡∏µ‡πà‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Å‡∏±‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
# - **Video Tracking**: Track ‡∏´‡∏•‡∏≤‡∏¢ Objects ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô‡∏î‡πâ‡∏ß‡∏¢ Concept-based Prompts
# - **Interactive Refinement**: ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÅ‡∏ö‡∏ö Iterative ‡∏î‡πâ‡∏ß‡∏¢ Positive/Negative Exemplars
#
# ### ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á SAM 2 vs SAM 3
#
# | ‡∏Ñ‡∏∏‡∏ì‡∏™‡∏°‡∏ö‡∏±‡∏ï‡∏¥ | SAM 2 | SAM 3 |
# |----------|-------|-------|
# | **Task** | Single object per prompt | All instances of concept |
# | **Prompts** | Points, boxes, masks | + Text, exemplars |
# | **Detection** | ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ Detector ‡∏†‡∏≤‡∏¢‡∏ô‡∏≠‡∏Å | Built-in open-vocabulary detector |
# | **Zero-Shot** | ‡πÑ‡∏°‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö | 47.0 AP ‡∏ö‡∏ô LVIS |
# | **Inference Speed** | ~23 ms/object | 30 ms (100+ objects) |
#
# ### Key Metrics
#
# - **CGF1 (Classification-Gated F1)**: ‡∏£‡∏ß‡∏° Localization ‡πÅ‡∏•‡∏∞ Classification
# - **pmF1 (Positive Macro F1)**: ‡∏ß‡∏±‡∏î Localization Quality
# - **IL_MCC**: ‡∏ß‡∏±‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏Ç‡∏≠‡∏á Binary Classification ("Concept ‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà?")
#
# ---
#
# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 1: Setup ‡πÅ‡∏•‡∏∞ Installation
#
# ### 1.1 ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Dependencies

# %%
# ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Ultralytics ‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î (‡∏ï‡πâ‡∏≠‡∏á >= 8.3.237)
!uv pip install -U ultralytics

# %%
# ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á CLIP Package ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Text Encoding)
!uv pip uninstall clip 
!uv pip install git+https://github.com/ultralytics/CLIP.git

# %%
# Import Libraries
import cv2
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
from ultralytics import SAM
from ultralytics.models.sam import SAM3SemanticPredictor, SAM3VideoSemanticPredictor
from ultralytics.utils.plotting import Annotator, colors

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Matplotlib
plt.rcParams['figure.figsize'] = (15, 10)
plt.rcParams['font.size'] = 12

# %% [markdown]
# ### 1.2 ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î Model Weights
#
# **‚ö†Ô∏è ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç:** SAM 3 Weights ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ Download ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥ ‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á:
# 1. Request access ‡∏ó‡∏µ‡πà: https://huggingface.co/facebook/sam3
# 2. Download `sam3.pt` ‡∏à‡∏≤‡∏Å: https://huggingface.co/facebook/sam3/resolve/main/sam3.pt
# 3. ‡∏ß‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô Working Directory ‡∏´‡∏£‡∏∑‡∏≠‡∏£‡∏∞‡∏ö‡∏∏ Full Path

# %%
from pathlib import Path
from huggingface_hub import hf_hub_download

# ‡πÉ‡∏™‡πà Token ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì (‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å https://huggingface.co/settings/tokens)
HF_TOKEN = "hf_hfobm"

model_path = Path("sam3.pt")

if not model_path.exists():
    print(f"‚è≥ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î {model_path.name}...")
    hf_hub_download(
        repo_id="facebook/sam3",
        filename="sam3.pt",
        local_dir=".",
        token=HF_TOKEN,  # ‡πÉ‡∏™‡πà token ‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ
        local_dir_use_symlinks=False
    )
    print(f"‚úÖ ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô")
else:
    print(f"‚úÖ ‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå {model_path}")

# %% [markdown]
# ---
#
# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 2: Text-based Concept Segmentation
#
# ‡πÉ‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ **Text Prompts** ‡πÄ‡∏û‡∏∑‡πà‡∏≠ Segment Objects

# %% [markdown]
# ### 2.1 Single Concept Segmentation

# %%
# ‡∏™‡∏£‡πâ‡∏≤‡∏á Predictor ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Concept Segmentation
overrides = dict(
    conf=0.25,              # Confidence Threshold
    task="segment",
    mode="predict",
    model=model_path,
    half=True,              # ‡πÉ‡∏ä‡πâ FP16 ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß
    save=True,
    verbose=True
)

predictor = SAM3SemanticPredictor(overrides=overrides)

# %%
import cv2
import numpy as np
import matplotlib.pyplot as plt


def display_results(image_path, results, title="SAM3 Segmentation Results", figsize=(15, 10)):
    """
    ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏†‡∏≤‡∏û‡∏û‡∏£‡πâ‡∏≠‡∏° Segmentation Masks ‡∏à‡∏≤‡∏Å SAM3
    
    Parameters:
    -----------
    image_path : str
        Path ‡∏Ç‡∏≠‡∏á‡∏†‡∏≤‡∏û‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö
    results : list
        ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å SAM3 predictor
    title : str
        ‡∏ä‡∏∑‡πà‡∏≠‡∏ó‡∏µ‡πà‡πÅ‡∏™‡∏î‡∏á‡∏ö‡∏ô‡∏†‡∏≤‡∏û
    figsize : tuple
        ‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡∏≠‡∏á Figure (width, height)
    
    Returns:
    --------
    None (‡πÅ‡∏™‡∏î‡∏á‡∏†‡∏≤‡∏û‡∏î‡πâ‡∏ß‡∏¢ matplotlib)
    """
    # ‡πÇ‡∏´‡∏•‡∏î‡∏†‡∏≤‡∏û‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö
    img = cv2.imread(image_path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ results ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    if not results or results[0].masks is None:
        plt.figure(figsize=figsize)
        plt.imshow(img)
        plt.title(f"{title}\nNo objects found")
        plt.axis('off')
        plt.tight_layout()
        plt.show()
        return
    
    # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• masks ‡πÅ‡∏•‡∏∞ boxes
    masks = results[0].masks.data.cpu().numpy()
    boxes = results[0].boxes.xyxy.cpu().numpy()
    
    # ‡∏î‡∏∂‡∏á confidence scores (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
    scores = None
    if hasattr(results[0].boxes, 'conf') and results[0].boxes.conf is not None:
        scores = results[0].boxes.conf.cpu().numpy()
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á Figure
    fig, axes = plt.subplots(1, 2, figsize=figsize)
    
    # ‡∏†‡∏≤‡∏û‡∏ã‡πâ‡∏≤‡∏¢: ‡∏†‡∏≤‡∏û‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö
    axes[0].imshow(img)
    axes[0].set_title("Original Image")
    axes[0].axis('off')
    
    # ‡∏†‡∏≤‡∏û‡∏Ç‡∏ß‡∏≤: ‡∏†‡∏≤‡∏û‡∏û‡∏£‡πâ‡∏≠‡∏° Masks
    axes[1].imshow(img)
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏µ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ mask
    np.random.seed(42)  # ‡πÉ‡∏´‡πâ‡∏™‡∏µ‡∏Ñ‡∏á‡∏ó‡∏µ‡πà
    colors = np.random.randint(0, 255, size=(len(masks), 3))
    
    # ‡∏ß‡∏≤‡∏î Masks
    overlay = np.zeros_like(img, dtype=np.float32)
    for i, mask in enumerate(masks):
        color = colors[i]
        overlay[mask > 0] = color
    
    # Blend overlay ‡∏Å‡∏±‡∏ö‡∏†‡∏≤‡∏û‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö
    axes[1].imshow(overlay.astype(np.uint8), alpha=0.5)
    
    # ‡∏ß‡∏≤‡∏î Bounding Boxes
    for i, box in enumerate(boxes):
        x1, y1, x2, y2 = box
        color = colors[i] / 255  # Normalize ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö matplotlib
        rect = plt.Rectangle((x1, y1), x2-x1, y2-y1, 
                              fill=False, edgecolor=color, linewidth=2)
        axes[1].add_patch(rect)
        
        # ‡πÅ‡∏™‡∏î‡∏á label ‡πÅ‡∏•‡∏∞ score
        label = f"ID:{i}"
        if scores is not None:
            label += f" ({scores[i]:.2f})"
        axes[1].text(x1, y1-5, label, color='white', fontsize=10,
                     bbox=dict(boxstyle='round', facecolor=color, alpha=0.8))
    
    axes[1].set_title(f"{title}\nFound {len(masks)} objects")
    axes[1].axis('off')
    
    plt.tight_layout()
    plt.show()


def display_results_grid(image_path, results, concepts, figsize=(18, 6)):
    """
    ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡πÅ‡∏ö‡∏ö Grid ‡πÇ‡∏î‡∏¢‡πÅ‡∏¢‡∏Å‡πÅ‡∏ï‡πà‡∏•‡∏∞ Concept (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏´‡∏•‡∏≤‡∏¢ concepts)
    
    Parameters:
    -----------
    image_path : str
        Path ‡∏Ç‡∏≠‡∏á‡∏†‡∏≤‡∏û‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö
    results : list
        ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å SAM3 predictor
    concepts : list
        ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ concepts ‡∏ó‡∏µ‡πà‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ ‡πÄ‡∏ä‡πà‡∏ô ["person", "car"]
    figsize : tuple
        ‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡∏≠‡∏á Figure
    """
    img = cv2.imread(image_path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    
    n_cols = len(concepts) + 1  # +1 ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏û‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö
    fig, axes = plt.subplots(1, n_cols, figsize=figsize)
    
    # ‡∏†‡∏≤‡∏û‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö
    axes[0].imshow(img)
    axes[0].set_title("Original")
    axes[0].axis('off')
    
    if not results or results[0].masks is None:
        for i, concept in enumerate(concepts):
            axes[i+1].imshow(img)
            axes[i+1].set_title(f"{concept}\nNot found")
            axes[i+1].axis('off')
        plt.tight_layout()
        plt.show()
        return
    
    masks = results[0].masks.data.cpu().numpy()
    
    # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡πÅ‡∏™‡∏î‡∏á‡∏ó‡∏∏‡∏Å masks ‡πÉ‡∏ô subplot ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
    # (SAM3 ‡∏≠‡∏≤‡∏à‡πÑ‡∏°‡πà‡πÅ‡∏¢‡∏Å masks ‡∏ï‡∏≤‡∏° concept ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á)
    for i, concept in enumerate(concepts):
        axes[i+1].imshow(img)
        
        # ‡∏ß‡∏≤‡∏î‡∏ó‡∏∏‡∏Å masks
        for j, mask in enumerate(masks):
            colored_mask = np.zeros_like(img)
            color = plt.cm.tab10(j % 10)[:3]
            color = (np.array(color) * 255).astype(np.uint8)
            colored_mask[mask > 0] = color
            axes[i+1].imshow(colored_mask, alpha=0.4)
        
        axes[i+1].set_title(f"Query: '{concept}'\n{len(masks)} objects")
        axes[i+1].axis('off')
    
    plt.tight_layout()
    plt.show()


# %%
# ‡∏•‡∏≠‡∏á generic terms
results = predictor(text=["object", "thing"])
display_results(test_image, results, title="Generic Objects")

# %%
# ‡∏ó‡∏î‡∏•‡∏≠‡∏á‡∏Å‡∏±‡∏ö Sample Image
test_image = "./envi2.png"

# Set Image (Extract Features ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß)
predictor.set_image(test_image)

# Query ‡∏î‡πâ‡∏ß‡∏¢ Text Prompt
results = predictor(text=["car"])

# ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏î‡πâ‡∏ß‡∏¢‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á
display_results(test_image, results, title="SAM3: person detection")



# %%
# ‡∏´‡∏£‡∏∑‡∏≠‡∏ñ‡πâ‡∏≤‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏´‡∏•‡∏≤‡∏¢ concepts
results_multi = predictor(text=["person", "car", "bicycle"])
display_results(test_image, results_multi, title="Multiple Concepts")



# %%
# ‡πÅ‡∏™‡∏î‡∏á‡πÅ‡∏ö‡∏ö Grid
display_results_grid(test_image, results_multi, ["person", "car", "bicycle"])

# %% [markdown]
# **‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢:**
# - `set_image()`: Extract Image Features ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß ‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ‡πÉ‡∏ô Memory
# - `text=["person"]`: ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ó‡∏∏‡∏Å Instance ‡∏Ç‡∏≠‡∏á "person" ‡πÉ‡∏ô‡∏†‡∏≤‡∏û
# - Model ‡∏à‡∏∞‡∏Ñ‡∏∑‡∏ô Masks ‡∏Ç‡∏≠‡∏á‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà‡∏û‡∏ö‡πÉ‡∏ô‡∏†‡∏≤‡∏û

# %% [markdown]
# ### 2.2 Multiple Concepts Segmentation

# %%
# Query ‡∏´‡∏•‡∏≤‡∏¢ Concepts ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô
results = predictor(text=["person", "car", "bicycle"])

# Visualize Results
if results and results[0].masks is not None:
    # ‡∏î‡∏∂‡∏á‡∏†‡∏≤‡∏û‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö
    img = cv2.imread(test_image)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    
    masks = results[0].masks.data.cpu().numpy()
    boxes = results[0].boxes.xyxy.cpu().numpy()
    
    plt.figure(figsize=(15, 10))
    plt.imshow(img)
    
    # ‡∏ß‡∏≤‡∏î Masks
    for i, mask in enumerate(masks):
        colored_mask = np.zeros_like(img)
        color = np.random.randint(0, 255, size=3)
        colored_mask[mask > 0] = color
        plt.imshow(colored_mask, alpha=0.5)
    
    plt.title(f"Found {len(masks)} objects")
    plt.axis('off')
    plt.tight_layout()
    plt.show()
else:
    print("‡πÑ‡∏°‡πà‡∏û‡∏ö Objects ‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö Text Prompts")

# %% [markdown]
# ### üí° ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ó‡∏µ‡πà 1: Descriptive Text Prompts
#
# **‡πÇ‡∏à‡∏ó‡∏¢‡πå:** ‡πÉ‡∏ä‡πâ Descriptive Phrases ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤ Objects ‡∏ó‡∏µ‡πà‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô
#
# **‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á:**
# - "person wearing red shirt"
# - "yellow school bus"
# - "dog with black spots"

# %%
# TODO: ‡∏•‡∏≠‡∏á‡πÉ‡∏ä‡πâ Descriptive Prompts
# Hint: SAM 3 ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à Simple Adjectives ‡πÅ‡∏•‡∏∞ Attributes

results = predictor(text=["person wearing red shirt", "person wearing blue shirt"])

# Visualize ‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå

# %% [markdown]
# ---
#
# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 3: Image Exemplar-based Segmentation
#
# ‡πÉ‡∏ä‡πâ **Bounding Boxes** ‡πÄ‡∏õ‡πá‡∏ô Visual Prompts ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤ Objects ‡∏ó‡∏µ‡πà‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Å‡∏±‡∏ô

# %% [markdown]
# ### 3.1 Single Exemplar

# %%
# ‡πÉ‡∏ä‡πâ Bounding Box ‡∏Ç‡∏≠‡∏á Object ‡∏ï‡∏±‡∏ß‡∏´‡∏ô‡∏∂‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á
# Format: [x1, y1, x2, y2] (Top-left ‡πÅ‡∏•‡∏∞ Bottom-right coordinates)

# TODO: ‡∏õ‡∏£‡∏±‡∏ö Coordinates ‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡∏†‡∏≤‡∏û‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì
example_bbox = [[100, 150, 300, 400]]  # ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: Bounding box ‡∏Ç‡∏≠‡∏á‡∏£‡∏ñ‡∏Ñ‡∏±‡∏ô‡∏´‡∏ô‡∏∂‡πà‡∏á

results = predictor(bboxes=example_bbox)

# %% [markdown]
# **‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢:**
# - SAM 3 ‡∏à‡∏∞‡πÉ‡∏ä‡πâ Object ‡πÉ‡∏ô Bounding Box ‡πÄ‡∏õ‡πá‡∏ô "Exemplar"
# - Model ‡∏à‡∏∞‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ó‡∏∏‡∏Å Object ‡∏ó‡∏µ‡πà‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Å‡∏±‡∏ö Exemplar ‡πÉ‡∏ô‡∏†‡∏≤‡∏û
# - ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏Å Object ‡∏ô‡∏µ‡πâ‡∏ß‡πà‡∏≤‡∏≠‡∏∞‡πÑ‡∏£

# %% [markdown]
# ### 3.2 Multiple Exemplars (Positive and Negative)

# %%
# ‡πÉ‡∏ä‡πâ‡∏´‡∏•‡∏≤‡∏¢ Bounding Boxes ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô
# Positive Examples: Objects ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏´‡∏≤
# Negative Examples: Objects ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£

positive_bboxes = [
    [100, 150, 300, 400],  # Object 1
    [500, 200, 700, 500],  # Object 2
]

results = predictor(bboxes=positive_bboxes)

# %% [markdown]
# ### üí° ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ó‡∏µ‡πà 2: Interactive Refinement
#
# **‡πÇ‡∏à‡∏ó‡∏¢‡πå:** ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÅ‡∏ö‡∏ö Iterative
# 1. ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å Text Prompt
# 2. ‡∏ñ‡πâ‡∏≤‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÑ‡∏°‡πà‡∏î‡∏µ ‡πÉ‡∏´‡πâ‡πÄ‡∏û‡∏¥‡πà‡∏° Positive/Negative Exemplars
# 3. ‡∏ß‡∏±‡∏î‡∏ú‡∏•‡∏ß‡πà‡∏≤ Accuracy ‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏´‡∏£‡πà

# %%
# TODO: Implement Interactive Refinement Pipeline
# Hint: ‡πÉ‡∏ä‡πâ set_image() ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß ‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏£‡∏µ‡∏¢‡∏Å predictor() ‡∏´‡∏•‡∏≤‡∏¢‡∏Ñ‡∏£‡∏±‡πâ‡∏á

# Iteration 1: Text only
# results_v1 = predictor(text=["cat"])

# Iteration 2: Text + 1 Exemplar
# results_v2 = predictor(text=["cat"], bboxes=[...])

# Iteration 3: Text + 2 Exemplars
# results_v3 = predictor(text=["cat"], bboxes=[..., ...])

# ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö CGF1 Score ‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞ Iteration

# %% [markdown]
# ---
#
# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 4: Feature Reuse for Efficiency
#
# ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û ‡πÄ‡∏£‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ **Extract Features ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß** ‡πÅ‡∏•‡πâ‡∏ß‡πÉ‡∏ä‡πâ‡∏ã‡πâ‡∏≥‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏´‡∏•‡∏≤‡∏¢ Queries

# %% [markdown]
# ### 4.1 Feature Extraction and Reuse

# %%
# ‡∏™‡∏£‡πâ‡∏≤‡∏á Predictors 2 ‡∏ï‡∏±‡∏ß
predictor1 = SAM3SemanticPredictor(overrides=overrides)
predictor2 = SAM3SemanticPredictor(overrides=overrides)

# Predictor 1: Extract Features
source = test_image
predictor1.set_image(source)
src_shape = cv2.imread(source).shape[:2]

# Predictor 2: Setup Model
predictor2.setup_model()

# %%
# ‡πÉ‡∏ä‡πâ Features ‡∏à‡∏≤‡∏Å Predictor 1 ‡∏Å‡∏±‡∏ö Predictor 2
# Query 1: Text Prompt
masks1, boxes1 = predictor2.inference_features(
    predictor1.features, 
    src_shape=src_shape, 
    text=["person"]
)

# Query 2: Bounding Box Prompt
masks2, boxes2 = predictor2.inference_features(
    predictor1.features, 
    src_shape=src_shape, 
    bboxes=[[100, 150, 300, 400]]
)

# %% [markdown]
# **‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå:**
# - ‡∏•‡∏î‡πÄ‡∏ß‡∏•‡∏≤ Inference ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ï‡πâ‡∏≠‡∏á Query ‡∏´‡∏•‡∏≤‡∏¢‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ö‡∏ô‡∏†‡∏≤‡∏û‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô
# - ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Interactive Applications

# %%
# Visualize Feature Reuse Results
def visualize_masks(image_path, masks, boxes, title):
    """Helper Function ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏™‡∏î‡∏á Segmentation Masks"""
    if masks is None or len(masks) == 0:
        print(f"No masks found for: {title}")
        return
    
    img = cv2.imread(image_path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    
    annotator = Annotator(img, pil=False)
    
    # ‡∏ß‡∏≤‡∏î Masks
    mask_colors = [colors(i, True) for i in range(len(masks))]
    annotator.masks(masks, mask_colors)
    
    # ‡∏ß‡∏≤‡∏î Bounding Boxes
    for i, box in enumerate(boxes):
        annotator.box_label(box, label=f"ID:{i}", color=mask_colors[i])
    
    plt.figure(figsize=(15, 10))
    plt.imshow(annotator.result())
    plt.title(title)
    plt.axis('off')
    plt.tight_layout()
    plt.show()

# Visualize Both Results
if masks1 is not None:
    visualize_masks(source, masks1.cpu().numpy(), boxes1.cpu().numpy(), 
                    "Query 1: Text Prompt 'person'")

if masks2 is not None:
    visualize_masks(source, masks2.cpu().numpy(), boxes2.cpu().numpy(), 
                    "Query 2: Bounding Box Prompt")

# %% [markdown]
# ### üí° ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ó‡∏µ‡πà 3: Batch Processing Efficiency
#
# **‡πÇ‡∏à‡∏ó‡∏¢‡πå:** ‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö Pipeline ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Process ‡∏†‡∏≤‡∏û‡∏´‡∏•‡∏≤‡∏¢‡πÜ ‡∏†‡∏≤‡∏û‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û
#
# **‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç:**
# - ‡∏°‡∏µ‡∏†‡∏≤‡∏û 100 ‡∏†‡∏≤‡∏û
# - ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏†‡∏≤‡∏û‡∏ï‡πâ‡∏≠‡∏á Query 5 Concepts
# - ‡∏ß‡∏±‡∏î‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á:
#   1. Extract Features ‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á
#   2. Reuse Features

# %%
# TODO: Implement Batch Processing Pipeline
import time

def process_without_reuse(image_paths, text_queries):
    """Process ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà Reuse Features"""
    start = time.time()
    # ... implementation ...
    end = time.time()
    return end - start

def process_with_reuse(image_paths, text_queries):
    """Process ‡πÇ‡∏î‡∏¢ Reuse Features"""
    start = time.time()
    # ... implementation ...
    end = time.time()
    return end - start

# ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÄ‡∏ß‡∏•‡∏≤
# time_without = process_without_reuse(...)
# time_with = process_with_reuse(...)
# speedup = time_without / time_with

# %% [markdown]
# ---
#
# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 5: Video Concept Tracking
#
# SAM 3 ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ Track Objects ‡πÉ‡∏ô Video ‡πÑ‡∏î‡πâ‡∏ó‡∏±‡πâ‡∏á Text Prompts ‡πÅ‡∏•‡∏∞ Visual Prompts

# %% [markdown]
# ### 5.1 Text-based Video Tracking

# %%
# ‡∏™‡∏£‡πâ‡∏≤‡∏á Video Predictor
video_overrides = dict(
    conf=0.25,
    task="segment",
    mode="predict",
    imgsz=640,              # ‡∏•‡∏î Resolution ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß
    model=model_path,
    half=True,
    save=True,
    verbose=True
)

video_predictor = SAM3VideoSemanticPredictor(overrides=video_overrides)

# %%
# Track Concepts ‡πÉ‡∏ô Video ‡∏î‡πâ‡∏ß‡∏¢ Text Prompts
video_path = "path/to/your/video.mp4"  # TODO: ‡πÉ‡∏™‡πà Path ‡∏Ç‡∏≠‡∏á Video

# Track "person" ‡πÅ‡∏•‡∏∞ "car" ‡πÉ‡∏ô Video
results = video_predictor(
    source=video_path,
    text=["person", "car"],
    stream=True  # Process Frame-by-Frame
)

# Process ‡πÅ‡∏•‡∏∞‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•
for r in results:
    # r.show()  # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡πÅ‡∏ö‡∏ö Real-time
    
    # ‡∏´‡∏£‡∏∑‡∏≠ Save Frame
    # r.save(f"output_frame_{r.frame_id}.jpg")
    
    # ‡∏´‡∏£‡∏∑‡∏≠ Extract Information
    if r.masks is not None:
        num_objects = len(r.masks)
        print(f"Frame {r.frame_id}: Found {num_objects} objects")

# %% [markdown]
# ### 5.2 Bounding Box-based Video Tracking

# %%
# Track Objects ‡πÇ‡∏î‡∏¢‡∏£‡∏∞‡∏ö‡∏∏ Initial Bounding Boxes ‡πÉ‡∏ô Frame ‡πÅ‡∏£‡∏Å
results = video_predictor(
    source=video_path,
    bboxes=[[100, 150, 300, 400], [500, 200, 700, 500]],  # TODO: ‡∏õ‡∏£‡∏±‡∏ö‡∏ï‡∏≤‡∏° Video
    labels=[1, 1],  # Positive Labels
    stream=True
)

for r in results:
    r.show()

# %% [markdown]
# ### üí° ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ó‡∏µ‡πà 4: Multi-Object Tracking with Occlusion
#
# **‡πÇ‡∏à‡∏ó‡∏¢‡πå:** Track ‡∏´‡∏•‡∏≤‡∏¢ Objects ‡∏ó‡∏µ‡πà‡∏°‡∏µ Occlusion (‡∏ö‡∏±‡∏á‡∏Å‡∏±‡∏ô)
#
# **‡∏™‡∏ñ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ì‡πå:**
# - Video ‡∏°‡∏µ‡∏Ñ‡∏ô 5 ‡∏Ñ‡∏ô ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏î‡∏¥‡∏ô
# - ‡∏ö‡∏≤‡∏á‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏Ñ‡∏ô‡πÄ‡∏î‡∏¥‡∏ô‡∏ö‡∏±‡∏á‡∏Å‡∏±‡∏ô
# - ‡∏ï‡πâ‡∏≠‡∏á Maintain Object Identity ‡∏ï‡∏•‡∏≠‡∏î Video
#
# **‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡πâ‡∏≤‡∏ó‡∏≤‡∏¢:**
# 1. SAM 3 ‡∏à‡∏∞ Re-detect Objects ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£‡πÄ‡∏°‡∏∑‡πà‡∏≠ Un-occluded?
# 2. Temporal Consistency ‡∏à‡∏∞‡∏î‡∏µ‡πÅ‡∏Ñ‡πà‡πÑ‡∏´‡∏ô?
# 3. ‡πÄ‡∏Å‡∏¥‡∏î ID Switch ‡∏ö‡πà‡∏≠‡∏¢‡πÑ‡∏´‡∏°?

# %%
# TODO: Implement Occlusion Handling
# Hint: SAM 3 ‡πÉ‡∏ä‡πâ Memory Bank ‡πÅ‡∏•‡∏∞ Temporal Disambiguation

# 1. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Video ‡∏ó‡∏µ‡πà‡∏°‡∏µ Occlusion
# 2. Track ‡∏î‡πâ‡∏ß‡∏¢ Text Prompt "person"
# 3. ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ß‡πà‡∏≤‡∏°‡∏µ ID Switch ‡∏Å‡∏µ‡πà‡∏Ñ‡∏£‡∏±‡πâ‡∏á
# 4. ‡∏•‡∏≠‡∏á‡πÉ‡∏ä‡πâ Periodic Re-prompting (‡∏ó‡∏∏‡∏Å‡πÜ N Frames)

# %% [markdown]
# ---
#
# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 6: Performance Evaluation
#
# ‡∏ß‡∏±‡∏î‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏Ç‡∏≠‡∏á SAM 3 ‡∏î‡πâ‡∏ß‡∏¢ Metrics ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°

# %% [markdown]
# ### 6.1 Classification-Gated F1 (CGF1)
#
# CGF1 = 100 √ó pmF1 √ó IL_MCC
#
# Where:
# - **pmF1**: Positive Macro F1 (Localization Quality)
# - **IL_MCC**: Image-Level Matthews Correlation Coefficient (Classification Accuracy)

# %%
def calculate_cgf1(predictions, ground_truth):
    """
    ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì CGF1 Score
    
    Parameters:
    -----------
    predictions : list of dict
        [{'masks': np.array, 'labels': list, 'scores': list}]
    ground_truth : list of dict
        [{'masks': np.array, 'labels': list}]
    
    Returns:
    --------
    cgf1 : float
        CGF1 Score (0-100)
    """
    # TODO: Implement CGF1 Calculation
    # 1. ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì pmF1 ‡∏à‡∏≤‡∏Å IoU ‡∏Ç‡∏≠‡∏á Masks
    # 2. ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì IL_MCC ‡∏à‡∏≤‡∏Å Presence Prediction
    # 3. CGF1 = pmF1 * IL_MCC * 100
    
    pass

# %% [markdown]
# ### 6.2 Benchmark Testing

# %%
# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö SAM 3 ‡∏ö‡∏ô Standard Benchmark
# ‡πÄ‡∏ä‡πà‡∏ô LVIS, COCO, ‡∏´‡∏£‡∏∑‡∏≠ Dataset ‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏≠‡∏á

def evaluate_on_dataset(predictor, dataset_path, text_queries):
    """
    Evaluate SAM 3 ‡∏ö‡∏ô Dataset
    
    Returns:
    --------
    metrics : dict
        {'CGF1': float, 'pmF1': float, 'IL_MCC': float, 'inference_time': float}
    """
    results = []
    
    for image_path in Path(dataset_path).glob("*.jpg"):
        # Load Image
        predictor.set_image(str(image_path))
        
        # Predict
        start = time.time()
        pred = predictor(text=text_queries)
        end = time.time()
        
        # Store Results
        results.append({
            'image': image_path.name,
            'prediction': pred,
            'time': end - start
        })
    
    # Calculate Metrics
    # ... implementation ...
    
    return results

# %%
# TODO: Run Evaluation
# dataset_path = "path/to/test/dataset"
# text_queries = ["person", "car", "bicycle"]
# results = evaluate_on_dataset(predictor, dataset_path, text_queries)

# %% [markdown]
# ### üí° ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ó‡∏µ‡πà 5: SAM 3 vs YOLO11 Comparison
#
# **‡πÇ‡∏à‡∏ó‡∏¢‡πå:** ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á SAM 3 ‡πÅ‡∏•‡∏∞ YOLO11-seg
#
# **Metrics ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏ß‡∏±‡∏î:**
# 1. Inference Speed (FPS)
# 2. Memory Usage (MB)
# 3. Accuracy (mAP, F1)
# 4. Zero-shot Capability
#
# **Dataset:** COCO Validation Set (‡∏´‡∏£‡∏∑‡∏≠ Subset ‡∏Ç‡∏≠‡∏á‡∏°‡∏±‡∏ô)

# %%
# TODO: Implement Comparison Pipeline
from ultralytics import YOLO

# 1. Load Models
# sam3 = SAM(model_path)
# yolo11 = YOLO("yolo11n-seg.pt")

# 2. Run Inference on Same Dataset
# 3. Compare Metrics
# 4. Plot Comparison Chart

# %% [markdown]
# ---
#
# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 7: Advanced Applications
#
# ‡∏õ‡∏£‡∏∞‡∏¢‡∏∏‡∏Å‡∏ï‡πå‡πÉ‡∏ä‡πâ SAM 3 ‡πÉ‡∏ô‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏à‡∏£‡∏¥‡∏á

# %% [markdown]
# ### 7.1 Object Counting

# %%
def count_objects(predictor, image_path, concept):
    """
    ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô Objects ‡∏Ç‡∏≠‡∏á Concept ‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î
    
    Parameters:
    -----------
    predictor : SAM3SemanticPredictor
        SAM 3 Predictor
    image_path : str
        Path ‡∏Ç‡∏≠‡∏á‡∏†‡∏≤‡∏û
    concept : str
        Concept ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ô‡∏±‡∏ö (e.g., "person", "car")
    
    Returns:
    --------
    count : int
        ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô Objects
    """
    predictor.set_image(image_path)
    results = predictor(text=[concept])
    
    if results and results[0].masks is not None:
        return len(results[0].masks)
    else:
        return 0

# %%
# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö Object Counting
test_concepts = ["person", "car", "bicycle", "traffic light"]

for concept in test_concepts:
    count = count_objects(predictor, test_image, concept)
    print(f"{concept}: {count} instances")

# %% [markdown]
# ### üí° ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ó‡∏µ‡πà 6: Crowd Counting Application
#
# **‡πÇ‡∏à‡∏ó‡∏¢‡πå:** ‡∏™‡∏£‡πâ‡∏≤‡∏á Application ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ô‡∏±‡∏ö‡∏Ñ‡∏ô‡πÉ‡∏ô‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏≠‡∏≠‡∏±‡∏î
#
# **‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡πâ‡∏≤‡∏ó‡∏≤‡∏¢:**
# 1. ‡∏Ñ‡∏ô‡∏≠‡∏¢‡∏π‡πà‡∏ä‡∏¥‡∏î‡∏Å‡∏±‡∏ô‡∏°‡∏≤‡∏Å (Occlusion)
# 2. ‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏ô‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô‡∏°‡∏≤‡∏Å (Scale Variation)
# 3. ‡∏°‡∏∏‡∏°‡∏Å‡∏•‡πâ‡∏≠‡∏á‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô (Viewpoint)
#
# **‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢:**
# - Accuracy > 90% (‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö Ground Truth)
# - Speed > 1 FPS ‡∏ö‡∏ô CPU

# %%
# TODO: Implement Crowd Counting Pipeline
# 1. Load Crowd Images
# 2. Use SAM 3 with text="person"
# 3. Apply Post-processing (NMS, Size Filtering, etc.)
# 4. Evaluate Accuracy

# %% [markdown]
# ### 7.2 Video Analytics: People Flow Analysis

# %%
def analyze_people_flow(video_path, entry_line, exit_line):
    """
    ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡πâ‡∏≤-‡∏≠‡∏≠‡∏Å‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏ô
    
    Parameters:
    -----------
    video_path : str
        Path ‡∏Ç‡∏≠‡∏á Video
    entry_line : tuple
        (x1, y1, x2, y2) ‡∏Ç‡∏≠‡∏á‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡πÄ‡∏Ç‡πâ‡∏≤
    exit_line : tuple
        (x1, y1, x2, y2) ‡∏Ç‡∏≠‡∏á‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡∏≠‡∏≠‡∏Å
    
    Returns:
    --------
    analytics : dict
        {'total_entered': int, 'total_exited': int, 'current_count': int}
    """
    predictor = SAM3VideoSemanticPredictor(overrides=video_overrides)
    
    entered = 0
    exited = 0
    tracked_ids = set()
    
    results = predictor(source=video_path, text=["person"], stream=True)
    
    for r in results:
        if r.masks is None:
            continue
        
        # Check ‡πÅ‡∏ï‡πà‡∏•‡∏∞ Object ‡∏ß‡πà‡∏≤‡∏Ç‡πâ‡∏≤‡∏°‡πÄ‡∏™‡πâ‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        # ... implementation ...
        
        pass
    
    return {
        'total_entered': entered,
        'total_exited': exited,
        'current_count': entered - exited
    }

# %% [markdown]
# ### üí° ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ó‡∏µ‡πà 7: Retail Analytics System
#
# **‡πÇ‡∏à‡∏ó‡∏¢‡πå:** ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏∞‡∏ö‡∏ö‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏û‡∏§‡∏ï‡∏¥‡∏Å‡∏£‡∏£‡∏°‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡πÉ‡∏ô‡∏£‡πâ‡∏≤‡∏ô‡∏Ñ‡πâ‡∏≤
#
# **Requirements:**
# 1. ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡πâ‡∏≤-‡∏≠‡∏≠‡∏Å‡∏£‡πâ‡∏≤‡∏ô
# 2. Track ‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏î‡∏¥‡∏ô‡∏Ç‡∏≠‡∏á‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤ (Heatmap)
# 3. ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Dwell Time (‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÇ‡∏ã‡∏ô)
# 4. ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö Queue (‡∏Ñ‡∏¥‡∏ß) ‡∏´‡∏ô‡πâ‡∏≤ Checkout
#
# **Output:**
# - Dashboard ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Real-time
# - Report ‡∏£‡∏≤‡∏¢‡∏ß‡∏±‡∏ô/‡∏£‡∏≤‡∏¢‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå

# %%
# TODO: Implement Retail Analytics System
# Hint: ‡πÉ‡∏ä‡πâ SAM 3 Video Tracking + Kalman Filter + Zone Detection

# %% [markdown]
# ---
#
# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 8: SAM 3 Agent (MLLM Integration)
#
# ‡πÉ‡∏ä‡πâ SAM 3 ‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ö Multimodal LLM ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏±‡∏ö Complex Queries

# %% [markdown]
# ### 8.1 Complex Query Examples
#
# SAM 3 ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö Simple Noun Phrases ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Complex Queries ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ö MLLM:
#
# **Simple (SAM 3 Native):**
# - "yellow school bus"
# - "person wearing red hat"
#
# **Complex (SAM 3 Agent):**
# - "People sitting down but not holding a gift box"
# - "The dog closest to the camera without a collar"
# - "Red objects larger than the person's hand"

# %% [markdown]
# ### üí° ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ó‡∏µ‡πà 8: Visual Reasoning Pipeline
#
# **‡πÇ‡∏à‡∏ó‡∏¢‡πå:** ‡∏™‡∏£‡πâ‡∏≤‡∏á Pipeline ‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏ö Complex Natural Language Query
#
# **‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á Query:**
# - "Find all chairs that are empty"
# - "Segment people who are looking at their phones"
# - "Highlight fruits that appear ripe"
#
# **Architecture:**
# 1. MLLM ‡πÅ‡∏õ‡∏•‡∏á Complex Query ‚Üí Simple Queries
# 2. SAM 3 ‡∏ó‡∏≥ Segmentation
# 3. MLLM ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Masks ‡πÅ‡∏•‡∏∞ Filter ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå

# %%
# TODO: Implement SAM 3 Agent Pipeline
# Pseudo-code:
# 
# def sam3_agent(image, complex_query):
#     # 1. MLLM: Break down query
#     simple_queries = mllm.decompose(complex_query)
#     
#     # 2. SAM 3: Segment
#     all_masks = []
#     for query in simple_queries:
#         masks = sam3.predict(image, text=query)
#         all_masks.append(masks)
#     
#     # 3. MLLM: Analyze and filter
#     final_masks = mllm.filter(all_masks, complex_query)
#     
#     return final_masks

# %% [markdown]
# ---
#
# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 9: Limitations and Troubleshooting
#
# ### 9.1 Known Limitations
#
# 1. **Phrase Complexity**: ‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏Å‡∏±‡∏ö Simple Noun Phrases
# 2. **Ambiguity**: Concepts ‡∏ó‡∏µ‡πà‡∏Ñ‡∏•‡∏∏‡∏°‡πÄ‡∏Ñ‡∏£‡∏∑‡∏≠ (e.g., "small window", "cozy room")
# 3. **Computational Cost**: ‡πÉ‡∏´‡∏ç‡πà‡πÅ‡∏•‡∏∞‡∏ä‡πâ‡∏≤‡∏Å‡∏ß‡πà‡∏≤ YOLO
# 4. **Rare Concepts**: ‡∏≠‡∏≤‡∏à‡πÑ‡∏°‡πà‡∏î‡∏µ‡∏Å‡∏±‡∏ö Concepts ‡∏ó‡∏µ‡πà‡∏´‡∏≤‡∏¢‡∏≤‡∏Å

# %% [markdown]
# ### 9.2 Common Errors and Solutions

# %%
# Error 1: TypeError: 'SimpleTokenizer' object is not callable
# Solution:
# !pip uninstall clip -y
# !pip install git+https://github.com/ultralytics/CLIP.git

# Error 2: Model weights not found
# Solution: ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏à‡∏≤‡∏Å HuggingFace ‡πÅ‡∏•‡∏∞‡∏ß‡∏≤‡∏á‡πÉ‡∏ô Correct Path

# Error 3: Out of Memory
# Solution: 
# - ‡∏•‡∏î imgsz (e.g., 640 ‚Üí 480)
# - ‡πÉ‡∏ä‡πâ half=True (FP16)
# - ‡∏•‡∏î Batch Size

# Error 4: Slow Inference
# Solutions:
# - ‡πÉ‡∏ä‡πâ Feature Reuse (inference_features)
# - ‡∏•‡∏î imgsz
# - ‡πÉ‡∏ä‡πâ GPU ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡πá‡∏ß‡∏Å‡∏ß‡πà‡∏≤ (H100, A100)

# %% [markdown]
# ### üí° ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ó‡∏µ‡πà 9: Optimization Challenge
#
# **‡πÇ‡∏à‡∏ó‡∏¢‡πå:** Optimize SAM 3 Pipeline ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Production
#
# **Constraints:**
# - Real-time (> 10 FPS) ‡∏ö‡∏ô GPU T4
# - Memory < 8 GB
# - Accuracy > 90% ‡∏Ç‡∏≠‡∏á Original
#
# **Techniques:**
# 1. Model Quantization (INT8)
# 2. Feature Caching
# 3. Dynamic Resolution
# 4. Batch Processing

# %%
# TODO: Implement Optimization Pipeline
# 1. Profile Current Performance
# 2. Apply Optimizations
# 3. Benchmark ‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö

# %% [markdown]
# ---
#
# ## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 10: Integration with MLOps Pipeline
#
# ‡∏ô‡∏≥ SAM 3 ‡∏°‡∏≤‡πÉ‡∏ä‡πâ‡πÉ‡∏ô Production Environment

# %% [markdown]
# ### 10.1 Model Serving with MLflow

# %%
import mlflow
from mlflow.models import infer_signature

# Log SAM 3 Model to MLflow
with mlflow.start_run(run_name="SAM3-Experiment"):
    # Log Parameters
    mlflow.log_params({
        "model": "sam3.pt",
        "conf_threshold": 0.25,
        "half_precision": True
    })
    
    # Log Metrics
    mlflow.log_metrics({
        "cgf1": 65.0,  # Example
        "inference_time_ms": 30
    })
    
    # Log Model
    # mlflow.pytorch.log_model(model, "sam3-model")

# %% [markdown]
# ### 10.2 Docker Deployment

# %%
# Dockerfile Example
dockerfile_content = """
FROM ultralytics/ultralytics:latest

# Install Dependencies
RUN pip install mlflow

# Copy Model Weights
COPY sam3.pt /app/sam3.pt

# Copy Inference Script
COPY inference.py /app/inference.py

# Expose API Port
EXPOSE 8000

# Run API Server
CMD ["python", "/app/inference.py"]
"""

# %%
# inference.py Example
inference_script = """
from fastapi import FastAPI, File, UploadFile
from ultralytics.models.sam import SAM3SemanticPredictor
import cv2
import numpy as np

app = FastAPI()
predictor = SAM3SemanticPredictor(overrides={"model": "sam3.pt"})

@app.post("/predict")
async def predict(file: UploadFile, text: str):
    # Read Image
    contents = await file.read()
    nparr = np.frombuffer(contents, np.uint8)
    img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
    
    # Predict
    predictor.set_image(img)
    results = predictor(text=[text])
    
    # Return Results
    return {
        "num_objects": len(results[0].masks) if results[0].masks else 0,
        "masks": results[0].masks.tolist() if results[0].masks else []
    }
"""

# %% [markdown]
# ### üí° ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ó‡∏µ‡πà 10: End-to-End MLOps Pipeline
#
# **‡πÇ‡∏à‡∏ó‡∏¢‡πå:** ‡∏™‡∏£‡πâ‡∏≤‡∏á Complete MLOps Pipeline ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö SAM 3
#
# **Components:**
# 1. **Data Pipeline**: Collect ‡πÅ‡∏•‡∏∞ Annotate Dataset
# 2. **Training/Fine-tuning**: (‡∏ñ‡πâ‡∏≤‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô)
# 3. **Experiment Tracking**: MLflow
# 4. **Model Registry**: Store Model Versions
# 5. **Deployment**: Docker + Kubernetes
# 6. **Monitoring**: Track Performance Metrics
# 7. **CI/CD**: Automated Testing ‡πÅ‡∏•‡∏∞ Deployment
#
# **Deliverables:**
# - Architecture Diagram
# - Code Implementation
# - Deployment Scripts
# - Monitoring Dashboard

# %%
# TODO: Design ‡πÅ‡∏•‡∏∞ Implement MLOps Pipeline

# %% [markdown]
# ---
#
# ## ‡∏™‡∏£‡∏∏‡∏õ (Summary)
#
# ‡πÉ‡∏ô Lab ‡∏ô‡∏µ‡πâ ‡πÄ‡∏£‡∏≤‡πÑ‡∏î‡πâ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ:
#
# 1. ‚úÖ ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á SAM 2 ‡πÅ‡∏•‡∏∞ SAM 3
# 2. ‚úÖ Text-based Concept Segmentation
# 3. ‚úÖ Image Exemplar Prompts ‡πÅ‡∏•‡∏∞ Interactive Refinement
# 4. ‚úÖ Feature Reuse ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û
# 5. ‚úÖ Video Tracking ‡∏î‡πâ‡∏ß‡∏¢ Text ‡πÅ‡∏•‡∏∞ Visual Prompts
# 6. ‚úÖ Performance Metrics (CGF1, pmF1, IL_MCC)
# 7. ‚úÖ Advanced Applications (Counting, Analytics, Reasoning)
# 8. ‚úÖ Optimization ‡πÅ‡∏•‡∏∞ Production Deployment
# 9. ‚úÖ MLOps Integration
#
# ### Key Takeaways:
#
# - **SAM 3** ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Open-vocabulary Tasks ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ Flexibility
# - **YOLO11** ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Production ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ Speed ‡πÅ‡∏•‡∏∞ Efficiency
# - **Feature Reuse** ‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡πÑ‡∏î‡πâ‡∏°‡∏≤‡∏Å
# - **Interactive Refinement** ‡∏ó‡∏≥‡πÉ‡∏´‡πâ Accuracy ‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏°‡∏µ‡∏ô‡∏±‡∏¢‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
# - **SAM 3 Agent** (MLLM) ‡∏Ç‡∏¢‡∏≤‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏π‡πà Complex Reasoning
#
# ---
#
# ## ‡πÅ‡∏ö‡∏ö‡∏ù‡∏∂‡∏Å‡∏´‡∏±‡∏î‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏° (Additional Exercises)
#
# 1. **Exercise 1**: Fine-tune SAM 3 ‡∏ö‡∏ô Domain-specific Dataset
# 2. **Exercise 2**: ‡∏™‡∏£‡πâ‡∏≤‡∏á Real-time Dashboard ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Video Analytics
# 3. **Exercise 3**: ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö SAM 3 ‡∏Å‡∏±‡∏ö YOLO-World
# 4. **Exercise 4**: Implement Active Learning Pipeline
# 5. **Exercise 5**: Deploy ‡∏ö‡∏ô Edge Device (Jetson, Raspberry Pi)
#
# ---
#
# ## ‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á (References)
#
# 1. SAM 3 Paper: https://openreview.net/forum?id=r35clVtGzw
# 2. Ultralytics Documentation: https://docs.ultralytics.com/models/sam-3/
# 3. SAM 2 Documentation: https://docs.ultralytics.com/models/sam-2/
# 4. YOLO11 Documentation: https://docs.ultralytics.com/models/yolo11/
#
# ---
#
# **‡∏à‡∏±‡∏î‡∏ó‡∏≥‡πÇ‡∏î‡∏¢:** [‡∏ä‡∏∑‡πà‡∏≠‡∏≠‡∏≤‡∏à‡∏≤‡∏£‡∏¢‡πå]  
# **Course:** Advanced Computer Vision with MLOps  
# **‡∏õ‡∏µ‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤:** 2567
