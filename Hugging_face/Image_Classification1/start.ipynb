{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA A100-SXM4-80GB (UUID: GPU-6620e1a6-0d7f-6cfe-1290-c3b795ee422b)\n",
      "  MIG 1g.10gb     Device  0: (UUID: MIG-23c4e088-ce35-5324-afdf-34f3960bcdb0)\n",
      "  MIG 1g.10gb     Device  1: (UUID: MIG-a40ffb87-88e6-5f6f-bde7-1af6ad01adf2)\n",
      "  MIG 1g.10gb     Device  2: (UUID: MIG-fccb8438-5870-52c3-aafa-42fddedd6b8e)\n",
      "  MIG 1g.10gb     Device  3: (UUID: MIG-8e6a7989-84f7-53a8-ac83-762afe1fbd36)\n",
      "  MIG 1g.10gb     Device  4: (UUID: MIG-b71e4b1a-a7c6-55b2-bdee-ddfc11df3f60)\n",
      "  MIG 1g.10gb     Device  5: (UUID: MIG-1f8b2754-cb9d-509f-b2dc-c4ff99a35290)\n",
      "  MIG 1g.10gb     Device  6: (UUID: MIG-f7abf2c0-a5f2-5ca3-b7cb-215c92d6e9c9)\n",
      "GPU 1: NVIDIA A100-SXM4-80GB (UUID: GPU-e84e0c4d-caeb-bff4-5bc2-79000e81f2c0)\n",
      "  MIG 1g.10gb     Device  0: (UUID: MIG-65568ef3-874c-554e-8033-47d7eda53db5)\n",
      "  MIG 1g.10gb     Device  1: (UUID: MIG-f31fdd03-d04b-53cb-b889-3c53ba7e4c29)\n",
      "  MIG 1g.10gb     Device  2: (UUID: MIG-e76da236-a0d8-513c-9b81-253e7b9bda25)\n",
      "  MIG 1g.10gb     Device  3: (UUID: MIG-248dba60-a9de-5218-81ae-8787f039a19f)\n",
      "  MIG 1g.10gb     Device  4: (UUID: MIG-9eb10c2d-a711-5778-8eee-d77f55852112)\n",
      "  MIG 1g.10gb     Device  5: (UUID: MIG-48d7653a-2162-5d48-8208-93a5c366f69e)\n",
      "  MIG 1g.10gb     Device  6: (UUID: MIG-cae5d3cd-d61e-5c4c-94f6-f3e9401120e6)\n",
      "GPU 2: NVIDIA A100-SXM4-80GB (UUID: GPU-dc94ed04-30d2-926e-e48b-c5871da3a409)\n",
      "  MIG 1g.10gb     Device  0: (UUID: MIG-05a30703-b842-5a2b-9c11-02cc84a8a7e4)\n",
      "  MIG 1g.10gb     Device  1: (UUID: MIG-6c84e900-5b55-56ec-ab27-73d5da3a60d5)\n",
      "  MIG 1g.10gb     Device  2: (UUID: MIG-2e7e822e-7db2-58fc-babe-abc44722f73c)\n",
      "  MIG 1g.10gb     Device  3: (UUID: MIG-51c6eba7-967b-554c-ab9f-f25f6841018c)\n",
      "  MIG 1g.10gb     Device  4: (UUID: MIG-026592c6-e800-546e-bf6f-14b36e4a1e20)\n",
      "  MIG 1g.10gb     Device  5: (UUID: MIG-6aa76b88-5532-5f0e-9898-368fa3d004a7)\n",
      "  MIG 1g.10gb     Device  6: (UUID: MIG-99312388-8281-551a-8eac-b59debef8d34)\n",
      "GPU 3: NVIDIA A100-SXM4-80GB (UUID: GPU-6b944573-903d-4082-ca47-887921f8e7ca)\n",
      "  MIG 1g.10gb     Device  0: (UUID: MIG-c65e10b9-6854-5af7-b015-f5f0c73639d1)\n",
      "  MIG 1g.10gb     Device  1: (UUID: MIG-ec6d8669-6189-5703-a3a7-06704163520f)\n",
      "  MIG 1g.10gb     Device  2: (UUID: MIG-888ff001-a6e1-5057-a356-813bc88c478a)\n",
      "  MIG 1g.10gb     Device  3: (UUID: MIG-78285856-e541-568d-9d7b-98df65c275a1)\n",
      "  MIG 1g.10gb     Device  4: (UUID: MIG-a2fa5640-eb68-50cb-aa52-1f543d95c1f9)\n",
      "  MIG 1g.10gb     Device  5: (UUID: MIG-968dc5c9-090b-5a86-a349-450233dceac3)\n",
      "  MIG 1g.10gb     Device  6: (UUID: MIG-3758c4bf-12f3-5772-8e8c-56a5e2aeeb1b)\n",
      "GPU 4: NVIDIA A100-SXM4-80GB (UUID: GPU-e7845ea0-c487-e96f-5fda-ebb1f6f84bc5)\n",
      "  MIG 7g.80gb     Device  0: (UUID: MIG-96f30d00-fc64-5fd0-88b0-6332787d91c5)\n",
      "GPU 5: NVIDIA A100-SXM4-80GB (UUID: GPU-85f6cf92-7790-c646-0e14-f10401fe214b)\n",
      "  MIG 7g.80gb     Device  0: (UUID: MIG-3aa38205-aad1-51d9-b28b-dca3df1758a3)\n",
      "GPU 6: NVIDIA A100-SXM4-80GB (UUID: GPU-e0c3d35e-579b-c1d6-eea9-c7691d42f275)\n",
      "  MIG 7g.80gb     Device  0: (UUID: MIG-2cb0acb1-ab08-5532-9c15-25b9625665b2)\n",
      "GPU 7: NVIDIA A100-SXM4-80GB (UUID: GPU-901a28b5-9684-0cc5-cec6-d78b14cfe130)\n",
      "  MIG 7g.80gb     Device  0: (UUID: MIG-b1700e4f-1b5c-53c6-b75e-b52536ff7060)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Apr  6 00:01:01 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.161.07             Driver Version: 535.161.07   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          On  | 00000000:07:00.0 Off |                   On |\n",
      "| N/A   29C    P0              56W / 400W |     87MiB / 81920MiB |     N/A      Default |\n",
      "|                                         |                      |              Enabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100-SXM4-80GB          On  | 00000000:0F:00.0 Off |                   On |\n",
      "| N/A   29C    P0              55W / 400W |     87MiB / 81920MiB |     N/A      Default |\n",
      "|                                         |                      |              Enabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100-SXM4-80GB          On  | 00000000:47:00.0 Off |                   On |\n",
      "| N/A   30C    P0              54W / 400W |     87MiB / 81920MiB |     N/A      Default |\n",
      "|                                         |                      |              Enabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100-SXM4-80GB          On  | 00000000:4E:00.0 Off |                   On |\n",
      "| N/A   31C    P0              55W / 400W |     87MiB / 81920MiB |     N/A      Default |\n",
      "|                                         |                      |              Enabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA A100-SXM4-80GB          On  | 00000000:87:00.0 Off |                   On |\n",
      "| N/A   33C    P0              53W / 400W |      2MiB / 81920MiB |     N/A      Default |\n",
      "|                                         |                      |              Enabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   5  NVIDIA A100-SXM4-80GB          On  | 00000000:90:00.0 Off |                   On |\n",
      "| N/A   31C    P0              54W / 400W |      2MiB / 81920MiB |     N/A      Default |\n",
      "|                                         |                      |              Enabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   6  NVIDIA A100-SXM4-80GB          On  | 00000000:B7:00.0 Off |                   On |\n",
      "| N/A   33C    P0              55W / 400W |      1MiB / 81920MiB |     N/A      Default |\n",
      "|                                         |                      |              Enabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   7  NVIDIA A100-SXM4-80GB          On  | 00000000:BD:00.0 Off |                   On |\n",
      "| N/A   34C    P0              56W / 400W |      1MiB / 81920MiB |     N/A      Default |\n",
      "|                                         |                      |              Enabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "\n",
      "+---------------------------------------------------------------------------------------+\n",
      "| MIG devices:                                                                          |\n",
      "+------------------+--------------------------------+-----------+-----------------------+\n",
      "| GPU  GI  CI  MIG |                   Memory-Usage |        Vol|      Shared           |\n",
      "|      ID  ID  Dev |                     BAR1-Usage | SM     Unc| CE ENC DEC OFA JPG    |\n",
      "|                  |                                |        ECC|                       |\n",
      "|==================+================================+===========+=======================|\n",
      "|  0    7   0   0  |              12MiB /  9728MiB  | 14      0 |  1   0    0    0    0 |\n",
      "|                  |               0MiB / 16383MiB  |           |                       |\n",
      "+------------------+--------------------------------+-----------+-----------------------+\n",
      "|  0    8   0   1  |              12MiB /  9728MiB  | 14      0 |  1   0    0    0    0 |\n",
      "|                  |               0MiB / 16383MiB  |           |                       |\n",
      "+------------------+--------------------------------+-----------+-----------------------+\n",
      "|  0    9   0   2  |              12MiB /  9728MiB  | 14      0 |  1   0    0    0    0 |\n",
      "|                  |               0MiB / 16383MiB  |           |                       |\n",
      "+------------------+--------------------------------+-----------+-----------------------+\n",
      "|  0   10   0   3  |              12MiB /  9728MiB  | 14      0 |  1   0    0    0    0 |\n",
      "|                  |               0MiB / 16383MiB  |           |                       |\n",
      "+------------------+--------------------------------+-----------+-----------------------+\n",
      "|  0   11   0   4  |              12MiB /  9728MiB  | 14      0 |  1   0    0    0    0 |\n",
      "|                  |               0MiB / 16383MiB  |           |                       |\n",
      "+------------------+--------------------------------+-----------+-----------------------+\n",
      "|  0   12   0   5  |              12MiB /  9728MiB  | 14      0 |  1   0    0    0    0 |\n",
      "|                  |               0MiB / 16383MiB  |           |                       |\n",
      "+------------------+--------------------------------+-----------+-----------------------+\n",
      "|  0   13   0   6  |              12MiB /  9728MiB  | 14      0 |  1   0    0    0    0 |\n",
      "|                  |               0MiB / 16383MiB  |           |                       |\n",
      "+------------------+--------------------------------+-----------+-----------------------+\n",
      "|  1    7   0   0  |              12MiB /  9728MiB  | 14      0 |  1   0    0    0    0 |\n",
      "|                  |               0MiB / 16383MiB  |           |                       |\n",
      "+------------------+--------------------------------+-----------+-----------------------+\n",
      "|  1    8   0   1  |              12MiB /  9728MiB  | 14      0 |  1   0    0    0    0 |\n",
      "|                  |               0MiB / 16383MiB  |           |                       |\n",
      "+------------------+--------------------------------+-----------+-----------------------+\n",
      "|  1    9   0   2  |              12MiB /  9728MiB  | 14      0 |  1   0    0    0    0 |\n",
      "|                  |               0MiB / 16383MiB  |           |                       |\n",
      "+------------------+--------------------------------+-----------+-----------------------+\n",
      "|  1   10   0   3  |              12MiB /  9728MiB  | 14      0 |  1   0    0    0    0 |\n",
      "|                  |               0MiB / 16383MiB  |           |                       |\n",
      "+------------------+--------------------------------+-----------+-----------------------+\n",
      "|  1   11   0   4  |              12MiB /  9728MiB  | 14      0 |  1   0    0    0    0 |\n",
      "|                  |               0MiB / 16383MiB  |           |                       |\n",
      "+------------------+--------------------------------+-----------+-----------------------+\n",
      "|  1   12   0   5  |              12MiB /  9728MiB  | 14      0 |  1   0    0    0    0 |\n",
      "|                  |               0MiB / 16383MiB  |           |                       |\n",
      "+------------------+--------------------------------+-----------+-----------------------+\n",
      "|  1   13   0   6  |              12MiB /  9728MiB  | 14      0 |  1   0    0    0    0 |\n",
      "|                  |               0MiB / 16383MiB  |           |                       |\n",
      "+------------------+--------------------------------+-----------+-----------------------+\n",
      "|  2    7   0   0  |              12MiB /  9728MiB  | 14      0 |  1   0    0    0    0 |\n",
      "|                  |               0MiB / 16383MiB  |           |                       |\n",
      "+------------------+--------------------------------+-----------+-----------------------+\n",
      "|  2    8   0   1  |              12MiB /  9728MiB  | 14      0 |  1   0    0    0    0 |\n",
      "|                  |               0MiB / 16383MiB  |           |                       |\n",
      "+------------------+--------------------------------+-----------+-----------------------+\n",
      "|  2    9   0   2  |              12MiB /  9728MiB  | 14      0 |  1   0    0    0    0 |\n",
      "|                  |               0MiB / 16383MiB  |           |                       |\n",
      "+------------------+--------------------------------+-----------+-----------------------+\n",
      "|  2   10   0   3  |              12MiB /  9728MiB  | 14      0 |  1   0    0    0    0 |\n",
      "|                  |               0MiB / 16383MiB  |           |                       |\n",
      "+------------------+--------------------------------+-----------+-----------------------+\n",
      "|  2   11   0   4  |              12MiB /  9728MiB  | 14      0 |  1   0    0    0    0 |\n",
      "|                  |               0MiB / 16383MiB  |           |                       |\n",
      "+------------------+--------------------------------+-----------+-----------------------+\n",
      "|  2   12   0   5  |              12MiB /  9728MiB  | 14      0 |  1   0    0    0    0 |\n",
      "|                  |               0MiB / 16383MiB  |           |                       |\n",
      "+------------------+--------------------------------+-----------+-----------------------+\n",
      "|  2   13   0   6  |              12MiB /  9728MiB  | 14      0 |  1   0    0    0    0 |\n",
      "|                  |               0MiB / 16383MiB  |           |                       |\n",
      "+------------------+--------------------------------+-----------+-----------------------+\n",
      "|  3    7   0   0  |              12MiB /  9728MiB  | 14      0 |  1   0    0    0    0 |\n",
      "|                  |               0MiB / 16383MiB  |           |                       |\n",
      "+------------------+--------------------------------+-----------+-----------------------+\n",
      "|  3    8   0   1  |              12MiB /  9728MiB  | 14      0 |  1   0    0    0    0 |\n",
      "|                  |               0MiB / 16383MiB  |           |                       |\n",
      "+------------------+--------------------------------+-----------+-----------------------+\n",
      "|  3    9   0   2  |              12MiB /  9728MiB  | 14      0 |  1   0    0    0    0 |\n",
      "|                  |               0MiB / 16383MiB  |           |                       |\n",
      "+------------------+--------------------------------+-----------+-----------------------+\n",
      "|  3   10   0   3  |              12MiB /  9728MiB  | 14      0 |  1   0    0    0    0 |\n",
      "|                  |               0MiB / 16383MiB  |           |                       |\n",
      "+------------------+--------------------------------+-----------+-----------------------+\n",
      "|  3   11   0   4  |              12MiB /  9728MiB  | 14      0 |  1   0    0    0    0 |\n",
      "|                  |               0MiB / 16383MiB  |           |                       |\n",
      "+------------------+--------------------------------+-----------+-----------------------+\n",
      "|  3   12   0   5  |              12MiB /  9728MiB  | 14      0 |  1   0    0    0    0 |\n",
      "|                  |               0MiB / 16383MiB  |           |                       |\n",
      "+------------------+--------------------------------+-----------+-----------------------+\n",
      "|  3   13   0   6  |              12MiB /  9728MiB  | 14      0 |  1   0    0    0    0 |\n",
      "|                  |               0MiB / 16383MiB  |           |                       |\n",
      "+------------------+--------------------------------+-----------+-----------------------+\n",
      "|  4    0   0   0  |               2MiB / 81050MiB  | 98      0 |  7   0    5    1    1 |\n",
      "|                  |               1MiB / 131072MiB |           |                       |\n",
      "+------------------+--------------------------------+-----------+-----------------------+\n",
      "|  5    0   0   0  |               2MiB / 81050MiB  | 98      0 |  7   0    5    1    1 |\n",
      "|                  |               1MiB / 131072MiB |           |                       |\n",
      "+------------------+--------------------------------+-----------+-----------------------+\n",
      "|  6    0   0   0  |               1MiB / 81050MiB  | 98      0 |  7   0    5    1    1 |\n",
      "|                  |               1MiB / 131072MiB |           |                       |\n",
      "+------------------+--------------------------------+-----------+-----------------------+\n",
      "|  7    0   0   0  |               1MiB / 81050MiB  | 98      0 |  7   0    5    1    1 |\n",
      "|                  |               1MiB / 131072MiB |           |                       |\n",
      "+------------------+--------------------------------+-----------+-----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available GPU devices: 1\n",
      "Device 0: cuda:0\n",
      "Device 0: NVIDIA A100-SXM4-80GB MIG 7g.80gb\n",
      "  Total GPU Memory: 81050.625 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# Set CUDA_VISIBLE_DEVICES to a specific 7g.80gb MIG device\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1:0\"  # Use MIG 7g.80gb Device 0 under GPU 6\n",
    "\n",
    "# Check the available GPU devices\n",
    "if torch.cuda.is_available():\n",
    "    num_devices = torch.cuda.device_count()\n",
    "    print(f\"Number of available GPU devices: {num_devices}\")\n",
    "    for i in range(num_devices):\n",
    "        device_name = torch.cuda.get_device_name(i)\n",
    "        device_properties = torch.cuda.get_device_properties(i)\n",
    "        total_memory = device_properties.total_memory / (1024 * 1024)  # Convert bytes to MB\n",
    "        device = torch.device(f\"cuda:{i}\")\n",
    "        print(f\"Device {i}: {device}\")\n",
    "        print(f\"Device {i}: {device_name}\")\n",
    "        print(f\"  Total GPU Memory: {total_memory} MB\")\n",
    "else:\n",
    "    print(\"CUDA is not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of visible GPU devices: 1\n",
      "Number of available GPUs: 1\n",
      "\n",
      "GPU 0 specifications:\n",
      "Device name: NVIDIA A100-SXM4-80GB MIG 7g.80gb\n",
      "Total memory: 81050.62 MB\n",
      "Compute capability: 8.0\n",
      "Number of multiprocessors: 98\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set CUDA_VISIBLE_DEVICES\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0:0\"  # Set visible devices to GPU 0 and 1\n",
    "\n",
    "# Rest of your code\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    num_devices = torch.cuda.device_count()\n",
    "    print(f\"Number of visible GPU devices: {num_devices}\")\n",
    "    # Additional code to utilize the GPU(s)\n",
    "else:\n",
    "    print(\"CUDA is not available\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    # Get the number of available GPUs\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of available GPUs: {num_gpus}\")\n",
    "\n",
    "    # Iterate over each GPU and display its specifications\n",
    "    for gpu_id in range(num_gpus):\n",
    "        print(f\"\\nGPU {gpu_id} specifications:\")\n",
    "        \n",
    "        # Set the current device\n",
    "        torch.cuda.set_device(gpu_id)\n",
    "        \n",
    "        # Get the device properties\n",
    "        device_properties = torch.cuda.get_device_properties(gpu_id)\n",
    "        \n",
    "        # Display the device name\n",
    "        print(f\"Device name: {device_properties.name}\")\n",
    "        \n",
    "        # Display the total memory\n",
    "        total_memory = device_properties.total_memory / (1024 * 1024)  # Convert bytes to MB\n",
    "        print(f\"Total memory: {total_memory:.2f} MB\")\n",
    "        \n",
    "        # Display the major and minor compute capability\n",
    "        major, minor = device_properties.major, device_properties.minor\n",
    "        print(f\"Compute capability: {major}.{minor}\")\n",
    "        \n",
    "        # Display the number of multiprocessors\n",
    "        num_multiprocessors = device_properties.multi_processor_count\n",
    "        print(f\"Number of multiprocessors: {num_multiprocessors}\")\n",
    "        \n",
    "      \n",
    "else:\n",
    "    print(\"CUDA is not available. Please check your PyTorch installation and GPU setup.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA_VISIBLE_DEVICES: 0:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "cuda_visible_devices = os.environ.get(\"CUDA_VISIBLE_DEVICES\")\n",
    "if cuda_visible_devices:\n",
    "    print(f\"CUDA_VISIBLE_DEVICES: {cuda_visible_devices}\")\n",
    "else:\n",
    "    print(\"CUDA_VISIBLE_DEVICES is not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of CPU cores: 256\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "num_cores = os.cpu_count()\n",
    "print(f\"Total number of CPU cores: {num_cores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([200, 100])\n",
      "Output shape: torch.Size([200, 10])\n",
      "Model running on: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Specify the GPU to use (e.g., 0 for the first GPU)\n",
    "gpu_id = 0\n",
    "\n",
    "# Set the device to use\n",
    "device = torch.device(f\"cuda:{gpu_id}\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define a simple neural network\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(100, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Create an instance of the model\n",
    "model = SimpleNet()\n",
    "\n",
    "# Move the model to the specified device\n",
    "model.to(device)\n",
    "\n",
    "# Generate dummy input data\n",
    "batch_size = 200\n",
    "input_data = torch.randn(batch_size, 100).to(device)\n",
    "\n",
    "# Perform a forward pass through the model\n",
    "output = model(input_data)\n",
    "\n",
    "print(\"Input shape:\", input_data.shape)\n",
    "print(\"Output shape:\", output.shape)\n",
    "print(\"Model running on:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([200, 100])\n",
      "Output shape: torch.Size([200, 10])\n",
      "Model running on: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Specify the GPU to use (e.g., 0 for the first GPU)\n",
    "gpu_id = 0\n",
    "\n",
    "# Set the device to use\n",
    "device = torch.device(f\"cuda:{gpu_id}\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define a simple neural network\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(100, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Create an instance of the model\n",
    "model = SimpleNet()\n",
    "\n",
    "# Move the model to the specified device\n",
    "model.to(device)\n",
    "\n",
    "# Generate dummy input data\n",
    "batch_size = 200\n",
    "input_data = torch.randn(batch_size, 100).to(device)\n",
    "\n",
    "# Perform a forward pass through the model\n",
    "output = model(input_data)\n",
    "\n",
    "print(\"Input shape:\", input_data.shape)\n",
    "print(\"Output shape:\", output.shape)\n",
    "print(\"Model running on:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Check if GPU is available and set PyTorch device\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Sample neural network model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 50)  # 10 input features, 50 outputs\n",
    "        self.fc2 = nn.Linear(50, 1)   # 50 input features, 1 output feature\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = Net().to(device)  # Transfer model to GPU\n",
    "\n",
    "# Create a simple dataset and dataloader\n",
    "x = torch.randn(100, 10)  # 100 samples, 10 features\n",
    "y = torch.randn(100, 1)   # 100 samples, 1 target variable\n",
    "dataset = TensorDataset(x, y)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):  # Loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)  # Transfer data to GPU\n",
    "\n",
    "        optimizer.zero_grad()   # Zero the parameter gradients\n",
    "\n",
    "        # Forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}, Loss: {running_loss / len(dataloader)}\")\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "# GPU UUID you want to use\n",
    "target_uuid = \"MIG-b1700e4f-1b5c-53c6-b75e-b52536ff7060\"\n",
    "\n",
    "# Get the list of GPUs and their UUIDs\n",
    "output = subprocess.check_output(\"nvidia-smi -L\", shell=True).decode()\n",
    "\n",
    "# Find the logical ID for the desired GPU UUID\n",
    "for line in output.split('\\n'):\n",
    "    if target_uuid in line:\n",
    "        gpu_id = line.split(':')[0].split(' ')[1]\n",
    "        break\n",
    "\n",
    "# Set the environment variable\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu_id\n",
    "\n",
    "# Now import PyTorch and continue with your script\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def get_gpu_memory_by_uuid(target_uuid):\n",
    "    # Execute the nvidia-smi command to get UUID and memory info\n",
    "    cmd = \"nvidia-smi --query-gpu=uuid,memory.total --format=csv\"\n",
    "    output = subprocess.check_output(cmd, shell=True).decode()\n",
    "\n",
    "    # Parse the output to find the memory for the target UUID\n",
    "    for line in output.split('\\n'):\n",
    "        if target_uuid in line:\n",
    "            # The memory info is expected to be in the second column\n",
    "            memory_info = line.split(',')[1].strip()\n",
    "            return memory_info\n",
    "    return \"GPU not found\"\n",
    "\n",
    "# Replace this with your GPU's UUID\n",
    "target_uuid = \"GPU-901a28b5-9684-0cc5-cec6-d78b14cfe130\"\n",
    "memory_info = get_gpu_memory_by_uuid(target_uuid)\n",
    "print(memory_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/tuchsanai/.conda/envs/torch/lib/python3.12/site-packages/torch']\n",
      "PyTorch version: 2.2.2+cu121\n",
      "CUDA version: 12.1\n",
      "CUDNN version: 8902\n",
      "Number of available GPUs: 1\n",
      "CUDA memory allocated: 0.01 GB\n",
      "CUDA memory cached: 0.02 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__path__)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"CUDNN version: {torch.backends.cudnn.version()}\")\n",
    "print(f\"Number of available GPUs: {torch.cuda.device_count()}\")\n",
    "# print(f\"Current GPU: {torch.cuda.current_device(1)}\")\n",
    "# print(f\"CUDA device name: {torch.cuda.get_device_name(0)}\")\n",
    "# print(f\"CUDA capability: {torch.cuda.get_device_capability()}\")\n",
    "print(f\"CUDA memory allocated: {torch.cuda.memory_allocated() / (1024**3):.2f} GB\")\n",
    "print(f\"CUDA memory cached: {torch.cuda.memory_reserved() / (1024**3):.2f} GB\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets evaluate\n",
    "\n",
    "# clean up the output\n",
    "from IPython.display import clear_output\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:7\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoFeatureExtractor, AutoModelForImageClassification, TrainingArguments, Trainer\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"beans\")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = dataset[\"train\"].features[\"labels\"].names\n",
    "\n",
    "\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = str(i)\n",
    "    id2label[str(i)] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load the pre-trained model and feature extractor\n",
    "model_name = \"google/vit-base-patch16-224-in21k\"\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(model_name)\n",
    "model = AutoModelForImageClassification.from_pretrained(model_name,  num_labels=len(labels))\n",
    "\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'][0]['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'][0]['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the image augmentation transforms\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std),\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std),\n",
    "])\n",
    "\n",
    "# Preprocess the dataset with image augmentation\n",
    "def preprocess_function(examples):\n",
    "    inputs = {}\n",
    "    inputs[\"pixel_values\"] = [train_transforms(image.convert(\"RGB\")) for image in examples[\"image\"]]\n",
    "    inputs[\"labels\"] = examples[\"labels\"]\n",
    "    return inputs\n",
    "\n",
    "def preprocess_function_val(examples):\n",
    "    inputs = {}\n",
    "    inputs[\"pixel_values\"] = [val_transforms(image.convert(\"RGB\")) for image in examples[\"image\"]]\n",
    "    inputs[\"labels\"] = examples[\"labels\"]\n",
    "    return inputs\n",
    "\n",
    "processed_dataset = dataset.map(preprocess_function, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "processed_dataset[\"validation\"] = dataset[\"validation\"].map(preprocess_function_val, batched=True, remove_columns=dataset[\"validation\"].column_names)\n",
    "\n",
    "# Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some augmented images\n",
    "fig, axs = plt.subplots(2, 4, figsize=(12, 6))\n",
    "for i in range(2):\n",
    "    for j in range(4):\n",
    "        image = processed_dataset[\"train\"][i*4+j][\"pixel_values\"]\n",
    "        image = torch.tensor(image)  # Convert the list to a tensor\n",
    "        image = image.permute(1, 2, 0).numpy()\n",
    "        image = (image - image.min()) / (image.max() - image.min())  # Normalize pixel values to [0, 1]\n",
    "        axs[i, j].imshow(image)\n",
    "        axs[i, j].axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "import torch\n",
    "from transformers import Trainer\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "\n",
    "def get_cpu_gpu_usage():\n",
    "    cpu_usage = psutil.cpu_percent()\n",
    "    gpu_usage = torch.cuda.utilization() if torch.cuda.is_available() else None\n",
    "    return cpu_usage, gpu_usage\n",
    "\n",
    "\n",
    "class MonitoringCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.cpu_gpu_usage = []\n",
    "\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        cpu_usage, gpu_usage = get_cpu_gpu_usage()\n",
    "        self.cpu_gpu_usage.append((cpu_usage, gpu_usage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the data collator\n",
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([torch.tensor(example[\"pixel_values\"]) for example in examples])\n",
    "    labels = torch.tensor([example[\"labels\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=96,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    num_train_epochs=5,\n",
    "    #fp16=True,\n",
    "    \n",
    "    save_steps=100,\n",
    "    eval_steps=100,\n",
    "    logging_steps=10,\n",
    "    learning_rate=2e-5,\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    ")\n",
    "\n",
    "# Create the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=processed_dataset[\"train\"],\n",
    "    eval_dataset=processed_dataset[\"validation\"],\n",
    "    tokenizer=feature_extractor,\n",
    "    data_collator=collate_fn,\n",
    "   \n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.8551386669278145\n",
      "Epoch 2, Loss: 0.8150906972587109\n",
      "Epoch 3, Loss: 1.0423409044742584\n",
      "Epoch 4, Loss: 1.350922852754593\n",
      "Epoch 5, Loss: 0.8309062048792839\n",
      "Epoch 6, Loss: 0.8482799157500267\n",
      "Epoch 7, Loss: 1.0287215113639832\n",
      "Epoch 8, Loss: 0.9033944606781006\n",
      "Epoch 9, Loss: 0.8291004002094269\n",
      "Epoch 10, Loss: 0.8488811552524567\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Check if GPU is available and set PyTorch device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Sample neural network model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 50)  # 10 input features, 50 outputs\n",
    "        self.fc2 = nn.Linear(50, 1)   # 50 input features, 1 output feature\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = Net().to(device)  # Transfer model to GPU\n",
    "\n",
    "# Create a simple dataset and dataloader\n",
    "x = torch.randn(100, 10)  # 100 samples, 10 features\n",
    "y = torch.randn(100, 1)   # 100 samples, 1 target variable\n",
    "dataset = TensorDataset(x, y)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):  # Loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)  # Transfer data to GPU\n",
    "\n",
    "        optimizer.zero_grad()   # Zero the parameter gradients\n",
    "\n",
    "        # Forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}, Loss: {running_loss / len(dataloader)}\")\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
