{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "https://huggingface.co/docs/transformers/en/tasks/image_classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "9oxewI3QypR1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /home/tuchsanai/.conda/envs/torch/lib/python3.12/site-packages (4.39.1)\n",
            "Requirement already satisfied: datasets in /home/tuchsanai/.conda/envs/torch/lib/python3.12/site-packages (2.18.0)\n",
            "Requirement already satisfied: evaluate in /home/tuchsanai/.conda/envs/torch/lib/python3.12/site-packages (0.4.1)\n",
            "Requirement already satisfied: filelock in /home/tuchsanai/.conda/envs/torch/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/tuchsanai/.conda/envs/torch/lib/python3.12/site-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/tuchsanai/.conda/envs/torch/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/tuchsanai/.conda/envs/torch/lib/python3.12/site-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/tuchsanai/.conda/envs/torch/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/tuchsanai/.conda/envs/torch/lib/python3.12/site-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /home/tuchsanai/.conda/envs/torch/lib/python3.12/site-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/tuchsanai/.conda/envs/torch/lib/python3.12/site-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /home/tuchsanai/.conda/envs/torch/lib/python3.12/site-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /home/tuchsanai/.conda/envs/torch/lib/python3.12/site-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /home/tuchsanai/.conda/envs/torch/lib/python3.12/site-packages (from datasets) (15.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /home/tuchsanai/.conda/envs/torch/lib/python3.12/site-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/tuchsanai/.conda/envs/torch/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /home/tuchsanai/.conda/envs/torch/lib/python3.12/site-packages (from datasets) (2.2.1)\n",
            "Requirement already satisfied: xxhash in /home/tuchsanai/.conda/envs/torch/lib/python3.12/site-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /home/tuchsanai/.conda/envs/torch/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /home/tuchsanai/.conda/envs/torch/lib/python3.12/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets) (2024.2.0)\n",
            "Requirement already satisfied: aiohttp in /home/tuchsanai/.conda/envs/torch/lib/python3.12/site-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: responses<0.19 in /home/tuchsanai/.conda/envs/torch/lib/python3.12/site-packages (from evaluate) (0.18.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /home/tuchsanai/.conda/envs/torch/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/tuchsanai/.conda/envs/torch/lib/python3.12/site-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/tuchsanai/.conda/envs/torch/lib/python3.12/site-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/tuchsanai/.conda/envs/torch/lib/python3.12/site-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /home/tuchsanai/.conda/envs/torch/lib/python3.12/site-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/tuchsanai/.conda/envs/torch/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/tuchsanai/.conda/envs/torch/lib/python3.12/site-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/tuchsanai/.conda/envs/torch/lib/python3.12/site-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/tuchsanai/.conda/envs/torch/lib/python3.12/site-packages (from requests->transformers) (2.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/tuchsanai/.conda/envs/torch/lib/python3.12/site-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /home/tuchsanai/.conda/envs/torch/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/tuchsanai/.conda/envs/torch/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /home/tuchsanai/.conda/envs/torch/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /home/tuchsanai/.conda/envs/torch/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "# Transformers installation\n",
        "!pip install transformers datasets evaluate\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htlkV-1qypR3"
      },
      "source": [
        "# Image classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "8b6krJUTypR5"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d9eea2bbbaad47679c50b9047dca6b88",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOocsqbJypR6"
      },
      "source": [
        "## Load Food-101 dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qv9zKSdcypR6"
      },
      "source": [
        "Start by loading a smaller subset of the Food-101 dataset from the ðŸ¤— Datasets library. This will give you a chance to\n",
        "experiment and make sure everything works before spending more time training on the full dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "0_UpVfUuypR6"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "food = load_dataset(\"food101\", split=\"train[:5000]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['image', 'label'],\n",
              "    num_rows: 5000\n",
              "})"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "food"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiBKRW2AypR7"
      },
      "source": [
        "Split the dataset's `train` split into a train and test set with the [train_test_split](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.train_test_split) method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "LDeoX-F3ypR7"
      },
      "outputs": [],
      "source": [
        "food = food.train_test_split(test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['image', 'label'],\n",
              "        num_rows: 4000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['image', 'label'],\n",
              "        num_rows: 1000\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "food"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQ-32tjcypR7"
      },
      "source": [
        "Then take a look at an example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "0W2LN_CuypR7",
        "outputId": "20fb146c-cd64-4ddd-d065-b96405390401"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=512x512>,\n",
              " 'label': 53}"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "food[\"train\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPIt8bG7ypR7"
      },
      "source": [
        "Each example in the dataset has two fields:\n",
        "\n",
        "- `image`: a PIL image of the food item\n",
        "- `label`: the label class of the food item\n",
        "\n",
        "To make it easier for the model to get the label name from the label id, create a dictionary that maps the label name\n",
        "to an integer and vice versa:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "ylTUWvqgypR7"
      },
      "outputs": [],
      "source": [
        "labels = food[\"train\"].features[\"label\"].names\n",
        "label2id, id2label = dict(), dict()\n",
        "for i, label in enumerate(labels):\n",
        "    label2id[label] = str(i)\n",
        "    id2label[str(i)] = label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFaC5h4hypR8"
      },
      "source": [
        "Now you can convert the label id to a label name:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "JrwOMCcMypR8",
        "outputId": "d156dc8c-2c25-401e-865f-8d016a6e1650"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'prime_rib'"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "id2label[str(79)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAFuFvIRypR8"
      },
      "source": [
        "## Preprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJfuhh7ZypR8"
      },
      "source": [
        "The next step is to load a ViT image processor to process the image into a tensor:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "oGvBuevuypR8"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoImageProcessor\n",
        "\n",
        "checkpoint = \"google/vit-base-patch16-224-in21k\"\n",
        "image_processor = AutoImageProcessor.from_pretrained(checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ViTImageProcessor {\n",
              "  \"_valid_processor_keys\": [\n",
              "    \"images\",\n",
              "    \"do_resize\",\n",
              "    \"size\",\n",
              "    \"resample\",\n",
              "    \"do_rescale\",\n",
              "    \"rescale_factor\",\n",
              "    \"do_normalize\",\n",
              "    \"image_mean\",\n",
              "    \"image_std\",\n",
              "    \"return_tensors\",\n",
              "    \"data_format\",\n",
              "    \"input_data_format\"\n",
              "  ],\n",
              "  \"do_normalize\": true,\n",
              "  \"do_rescale\": true,\n",
              "  \"do_resize\": true,\n",
              "  \"image_mean\": [\n",
              "    0.5,\n",
              "    0.5,\n",
              "    0.5\n",
              "  ],\n",
              "  \"image_processor_type\": \"ViTImageProcessor\",\n",
              "  \"image_std\": [\n",
              "    0.5,\n",
              "    0.5,\n",
              "    0.5\n",
              "  ],\n",
              "  \"resample\": 2,\n",
              "  \"rescale_factor\": 0.00392156862745098,\n",
              "  \"size\": {\n",
              "    \"height\": 224,\n",
              "    \"width\": 224\n",
              "  }\n",
              "}"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "image_processor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tk1KRcuYypR8"
      },
      "source": [
        "Apply some image transformations to the images to make the model more robust against overfitting. Here you'll use torchvision's [`transforms`](https://pytorch.org/vision/stable/transforms.html) module, but you can also use any image library you like.\n",
        "\n",
        "Crop a random part of the image, resize it, and normalize it with the image mean and standard deviation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "3yxbf-9cypR8"
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms import RandomResizedCrop, Compose, Normalize, ToTensor\n",
        "\n",
        "normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
        "size = (\n",
        "    image_processor.size[\"shortest_edge\"]\n",
        "    if \"shortest_edge\" in image_processor.size\n",
        "    else (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
        ")\n",
        "_transforms = Compose([RandomResizedCrop(size), ToTensor(), normalize])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ij2z2fLIypR8"
      },
      "source": [
        "Then create a preprocessing function to apply the transforms and return the `pixel_values` - the inputs to the model - of the image:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "4SyFonEqypR8"
      },
      "outputs": [],
      "source": [
        "def transforms(examples):\n",
        "    examples[\"pixel_values\"] = [_transforms(img.convert(\"RGB\")) for img in examples[\"image\"]]\n",
        "    del examples[\"image\"]\n",
        "    return examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53UljrzyypR8"
      },
      "source": [
        "To apply the preprocessing function over the entire dataset, use ðŸ¤— Datasets [with_transform](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.with_transform) method. The transforms are applied on the fly when you load an element of the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "_CbuN3lyypR8"
      },
      "outputs": [],
      "source": [
        "food = food.with_transform(transforms)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUY7LYnrypR8"
      },
      "source": [
        "Now create a batch of examples using [DefaultDataCollator](https://huggingface.co/docs/transformers/main/en/main_classes/data_collator#transformers.DefaultDataCollator). Unlike other data collators in ðŸ¤— Transformers, the `DefaultDataCollator` does not apply additional preprocessing such as padding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "88cUcV3zypR9"
      },
      "outputs": [],
      "source": [
        "from transformers import DefaultDataCollator\n",
        "\n",
        "data_collator = DefaultDataCollator()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UAO4at8ypR9"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Avh1ldlwypR9"
      },
      "source": [
        "Including a metric during training is often helpful for evaluating your model's performance. You can quickly load an\n",
        "evaluation method with the ðŸ¤— [Evaluate](https://huggingface.co/docs/evaluate/index) library. For this task, load\n",
        "the [accuracy](https://huggingface.co/spaces/evaluate-metric/accuracy) metric (see the ðŸ¤— Evaluate [quick tour](https://huggingface.co/docs/evaluate/a_quick_tour) to learn more about how to load and compute a metric):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "SF_71hHUypR9"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "87cd72d70bdd467491f8be615c45cf1f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import evaluate\n",
        "\n",
        "accuracy = evaluate.load(\"accuracy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fV9XmkdvypR9"
      },
      "source": [
        "Then create a function that passes your predictions and labels to [compute](https://huggingface.co/docs/evaluate/main/en/package_reference/main_classes#evaluate.EvaluationModule.compute) to calculate the accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "QgOG4XFxypR9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return accuracy.compute(predictions=predictions, references=labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODjgRALjypR9"
      },
      "source": [
        "Your `compute_metrics` function is ready to go now, and you'll return to it when you set up your training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQjq76GhypR9"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7moDhC7ypR9"
      },
      "source": [
        "<Tip>\n",
        "\n",
        "If you aren't familiar with finetuning a model with the [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer), take a look at the basic tutorial [here](https://huggingface.co/docs/transformers/main/en/tasks/../training#train-with-pytorch-trainer)!\n",
        "\n",
        "</Tip>\n",
        "\n",
        "You're ready to start training your model now! Load ViT with [AutoModelForImageClassification](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoModelForImageClassification). Specify the number of labels along with the number of expected labels, and the label mappings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "85_6E9_LypR9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForImageClassification, TrainingArguments, Trainer\n",
        "\n",
        "model = AutoModelForImageClassification.from_pretrained(\n",
        "    checkpoint,\n",
        "    num_labels=len(labels),\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkcPSgZiypR9"
      },
      "source": [
        "At this point, only three steps remain:\n",
        "\n",
        "1. Define your training hyperparameters in [TrainingArguments](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments). It is important you don't remove unused columns because that'll drop the `image` column. Without the `image` column, you can't create `pixel_values`. Set `remove_unused_columns=False` to prevent this behavior! The only other required parameter is `output_dir` which specifies where to save your model. You'll push this model to the Hub by setting `push_to_hub=True` (you need to be signed in to Hugging Face to upload your model). At the end of each epoch, the [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) will evaluate the accuracy and save the training checkpoint.\n",
        "2. Pass the training arguments to [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) along with the model, dataset, tokenizer, data collator, and `compute_metrics` function.\n",
        "3. Call [train()](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.train) to finetune your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "sWAmDHg1ypR9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/tuchsanai/.conda/envs/torch/lib/python3.12/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
            "  warnings.warn(\n",
            "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='186' max='186' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [186/186 03:28, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>2.672000</td>\n",
              "      <td>2.494720</td>\n",
              "      <td>0.825000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.551600</td>\n",
              "      <td>1.593781</td>\n",
              "      <td>0.886000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=186, training_loss=2.4093076746950866, metrics={'train_runtime': 210.6823, 'train_samples_per_second': 56.958, 'train_steps_per_second': 0.883, 'total_flos': 9.232831524962304e+17, 'train_loss': 2.4093076746950866, 'epoch': 2.98})"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"my_awesome_food_model\",\n",
        "    remove_unused_columns=False,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    gradient_accumulation_steps=4,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    warmup_ratio=0.1,\n",
        "    logging_steps=10,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    push_to_hub=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=food[\"train\"],\n",
        "    eval_dataset=food[\"test\"],\n",
        "    tokenizer=image_processor,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TorLCS3sypR9"
      },
      "source": [
        "Once training is completed, share your model to the Hub with the [push_to_hub()](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.push_to_hub) method so everyone can use your model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "MAsQPIj5ypSH"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/Tuch/my_awesome_food_model/commit/006a2446fa1d8fb7a726f88d922f9441c8e5ab62', commit_message='End of training', commit_description='', oid='006a2446fa1d8fb7a726f88d922f9441c8e5ab62', pr_url=None, pr_revision=None, pr_num=None)"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.push_to_hub()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vj-uznmIypSH"
      },
      "source": [
        "<Tip>\n",
        "\n",
        "For a more in-depth example of how to finetune a model for image classification, take a look at the corresponding [PyTorch notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb).\n",
        "\n",
        "</Tip>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snPa0qRVypSH"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cTDhKBKypSI"
      },
      "source": [
        "Great, now that you've fine-tuned a model, you can use it for inference!\n",
        "\n",
        "Load an image you'd like to run inference on:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "l3FFXNhlypSI"
      },
      "outputs": [],
      "source": [
        "ds = load_dataset(\"food101\", split=\"validation[:10]\")\n",
        "image = ds[\"image\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26b7PG_HypSI"
      },
      "source": [
        "<div class=\"flex justify-center\">\n",
        "    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png\" alt=\"image of beignets\"/>\n",
        "</div>\n",
        "\n",
        "The simplest way to try out your finetuned model for inference is to use it in a [pipeline()](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline). Instantiate a `pipeline` for image classification with your model, and pass your image to it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "ESspWY_sypSI",
        "outputId": "73d118f7-ed92-4aba-c319-5bd4c96c9c5b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'label': 'beignets', 'score': 0.2904573082923889},\n",
              " {'label': 'chicken_wings', 'score': 0.016236888244748116},\n",
              " {'label': 'ramen', 'score': 0.013662672601640224},\n",
              " {'label': 'bruschetta', 'score': 0.012738403864204884},\n",
              " {'label': 'hamburger', 'score': 0.011438599787652493}]"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"image-classification\", model=\"my_awesome_food_model\")\n",
        "classifier(image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myZDhmtsypSI"
      },
      "source": [
        "You can also manually replicate the results of the `pipeline` if you'd like:\n",
        "\n",
        "Load an image processor to preprocess the image and return the `input` as PyTorch tensors:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "3qbfUDTMypSI"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoImageProcessor\n",
        "import torch\n",
        "\n",
        "image_processor = AutoImageProcessor.from_pretrained(\"my_awesome_food_model\")\n",
        "inputs = image_processor(image, return_tensors=\"pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LsRZrNkypSI"
      },
      "source": [
        "Pass your inputs to the model and return the logits:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "XK53vxMJypSI"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForImageClassification\n",
        "\n",
        "model = AutoModelForImageClassification.from_pretrained(\"my_awesome_food_model\")\n",
        "with torch.no_grad():\n",
        "    logits = model(**inputs).logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQYfLIunypSI"
      },
      "source": [
        "Get the predicted label with the highest probability, and use the model's `id2label` mapping to convert it to a label:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "ttg7YjzbypSI",
        "outputId": "937b37b4-eb26-46c4-ab22-10795187010e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'beignets'"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predicted_label = logits.argmax(-1).item()\n",
        "model.config.id2label[predicted_label]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
