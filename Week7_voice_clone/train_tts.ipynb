{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dceb31d8",
   "metadata": {},
   "source": [
    "# ðŸŽ¯ Qwen3-TTS Fine-tuning Lab: Train Your Own Voice Model\n",
    "\n",
    "**Course:** Machine Learning / Deep Learning  \n",
    "**Topic:** Fine-tuning Text-to-Speech Models  \n",
    "**Model:** Qwen3-TTS (Alibaba, January 2025)\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "1. Understand the Qwen3-TTS fine-tuning architecture\n",
    "2. Prepare audio datasets for TTS training\n",
    "3. Configure training hyperparameters\n",
    "4. Fine-tune Qwen3-TTS on custom voice data\n",
    "5. Evaluate and test the fine-tuned model\n",
    "6. Export and deploy your trained model\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- GPU with at least 24GB VRAM (A100/A10/RTX 4090 recommended)\n",
    "- Basic understanding of PyTorch and Transformers\n",
    "- Familiarity with audio processing concepts\n",
    "\n",
    "---\n",
    "\n",
    "## Fine-tuning Overview\n",
    "\n",
    "### Why Fine-tune?\n",
    "\n",
    "| Use Case | Benefit |\n",
    "|----------|---------|\n",
    "| **Custom Voice** | Train on specific speaker's voice |\n",
    "| **Domain Adaptation** | Improve on technical/medical terms |\n",
    "| **Language Expansion** | Better support for unsupported languages |\n",
    "| **Style Transfer** | Train specific speaking styles |\n",
    "\n",
    "### Training Approaches\n",
    "\n",
    "| Approach | Data Required | VRAM | Quality |\n",
    "|----------|---------------|------|---------|\n",
    "| **Full Fine-tune** | 10+ hours | 80GB | Best |\n",
    "| **LoRA** | 1-5 hours | 24GB | Good |\n",
    "| **Prompt Tuning** | 10-30 mins | 16GB | Moderate |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897ae672",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf2e82e1",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Environment Setup\n",
    "\n",
    "à¸•à¸´à¸”à¸•à¸±à¹‰à¸‡ packages à¸—à¸µà¹ˆà¸ˆà¸³à¹€à¸›à¹‡à¸™à¸ªà¸³à¸«à¸£à¸±à¸šà¸à¸²à¸£ train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d295a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1.1: Install Required Packages\n",
    "# Uncomment to install:\n",
    "\n",
    "!pip install -U transformers==4.57.3\n",
    "!pip install -U accelerate\n",
    "!pip install -U datasets\n",
    "!pip install -U peft  # For LoRA\n",
    "!pip install -U qwen-tts\n",
    "!pip install -U soundfile librosa\n",
    "!pip install -U tensorboard\n",
    "!pip install -U bitsandbytes  # For 8-bit training\n",
    "!pip install -U wandb  # Optional: for experiment tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadbf45b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Cell 1.2: Import Libraries\n",
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "\n",
    "# Transformers & Training\n",
    "from transformers import (\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "from datasets import Dataset, DatasetDict, Audio\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# Qwen TTS\n",
    "from qwen_tts import Qwen3TTSModel, Qwen3TTSTokenizer\n",
    "\n",
    "# Visualization\n",
    "from IPython.display import Audio as IPAudio, display\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6392eeb2",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Cell 1.3: Utility Functions\n",
    "\n",
    "def print_section(title: str):\n",
    "    \"\"\"Print formatted section header\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"  {title}\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "def print_gpu_memory():\n",
    "    \"\"\"Print current GPU memory usage\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1e9\n",
    "        reserved = torch.cuda.memory_reserved() / 1e9\n",
    "        total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        print(f\"GPU Memory: {allocated:.2f}GB allocated / {reserved:.2f}GB reserved / {total:.2f}GB total\")\n",
    "    else:\n",
    "        print(\"No GPU available\")\n",
    "\n",
    "def format_duration(seconds: float) -> str:\n",
    "    \"\"\"Format seconds to HH:MM:SS\"\"\"\n",
    "    hours = int(seconds // 3600)\n",
    "    minutes = int((seconds % 3600) // 60)\n",
    "    secs = int(seconds % 60)\n",
    "    return f\"{hours:02d}:{minutes:02d}:{secs:02d}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cb7d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1.4: Check System Configuration\n",
    "\n",
    "print_section(\"System Configuration\")\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"GPU Memory: {gpu_memory:.2f} GB\")\n",
    "    \n",
    "    if gpu_memory >= 24:\n",
    "        print(\"âœ… Sufficient GPU memory for LoRA fine-tuning\")\n",
    "    elif gpu_memory >= 16:\n",
    "        print(\"âš ï¸ Limited memory - use smaller batch size or 8-bit training\")\n",
    "    else:\n",
    "        print(\"âŒ Insufficient GPU memory for fine-tuning\")\n",
    "else:\n",
    "    print(\"âŒ No CUDA GPU detected - fine-tuning requires GPU\")\n",
    "\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16ab95f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Cell 1.5: Create Directory Structure\n",
    "\n",
    "# Project directories\n",
    "PROJECT_DIR = Path(\"./qwen3_tts_training\")\n",
    "DATA_DIR = PROJECT_DIR / \"data\"\n",
    "RAW_AUDIO_DIR = DATA_DIR / \"raw_audio\"\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "CHECKPOINT_DIR = PROJECT_DIR / \"checkpoints\"\n",
    "OUTPUT_DIR = PROJECT_DIR / \"outputs\"\n",
    "LOG_DIR = PROJECT_DIR / \"logs\"\n",
    "\n",
    "# Create all directories\n",
    "for dir_path in [RAW_AUDIO_DIR, PROCESSED_DIR, CHECKPOINT_DIR, OUTPUT_DIR, LOG_DIR]:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"âœ… Created: {dir_path}\")\n",
    "\n",
    "print(f\"\\nðŸ“ Project Structure:\")\n",
    "print(f\"   {PROJECT_DIR}/\")\n",
    "print(f\"   â”œâ”€â”€ data/\")\n",
    "print(f\"   â”‚   â”œâ”€â”€ raw_audio/     # Original audio files\")\n",
    "print(f\"   â”‚   â””â”€â”€ processed/     # Processed dataset\")\n",
    "print(f\"   â”œâ”€â”€ checkpoints/       # Model checkpoints\")\n",
    "print(f\"   â”œâ”€â”€ outputs/           # Generated audio\")\n",
    "print(f\"   â””â”€â”€ logs/              # Training logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3515aa16",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Dataset Preparation\n",
    "\n",
    "à¸à¸²à¸£à¹€à¸•à¸£à¸µà¸¢à¸¡ dataset à¸ªà¸³à¸«à¸£à¸±à¸š TTS training à¸›à¸£à¸°à¸à¸­à¸šà¸”à¹‰à¸§à¸¢:\n",
    "1. Audio files (WAV format, 16kHz or 24kHz)\n",
    "2. Transcriptions (text matching audio)\n",
    "3. Speaker information (optional)\n",
    "\n",
    "### Dataset Format\n",
    "\n",
    "```\n",
    "data/\n",
    "â”œâ”€â”€ audio/\n",
    "â”‚   â”œâ”€â”€ 001.wav\n",
    "â”‚   â”œâ”€â”€ 002.wav\n",
    "â”‚   â””â”€â”€ ...\n",
    "â””â”€â”€ metadata.json\n",
    "```\n",
    "\n",
    "### metadata.json format:\n",
    "```json\n",
    "[\n",
    "    {\"audio\": \"001.wav\", \"text\": \"Hello world\", \"speaker\": \"speaker_1\"},\n",
    "    {\"audio\": \"002.wav\", \"text\": \"How are you\", \"speaker\": \"speaker_1\"}\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f278ff38",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Cell 2.1: Audio Processing Functions\n",
    "\n",
    "class AudioProcessor:\n",
    "    \"\"\"Audio processing utilities for TTS dataset preparation\"\"\"\n",
    "    \n",
    "    def __init__(self, target_sr: int = 24000):\n",
    "        self.target_sr = target_sr\n",
    "    \n",
    "    def load_audio(self, audio_path: str) -> Tuple[np.ndarray, int]:\n",
    "        \"\"\"Load audio file and return waveform and sample rate\"\"\"\n",
    "        wav, sr = librosa.load(audio_path, sr=None)\n",
    "        return wav, sr\n",
    "    \n",
    "    def resample(self, wav: np.ndarray, orig_sr: int) -> np.ndarray:\n",
    "        \"\"\"Resample audio to target sample rate\"\"\"\n",
    "        if orig_sr != self.target_sr:\n",
    "            wav = librosa.resample(wav, orig_sr=orig_sr, target_sr=self.target_sr)\n",
    "        return wav\n",
    "    \n",
    "    def normalize(self, wav: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Normalize audio to [-1, 1] range\"\"\"\n",
    "        max_val = np.abs(wav).max()\n",
    "        if max_val > 0:\n",
    "            wav = wav / max_val * 0.95  # Leave some headroom\n",
    "        return wav\n",
    "    \n",
    "    def trim_silence(self, wav: np.ndarray, top_db: int = 20) -> np.ndarray:\n",
    "        \"\"\"Trim silence from beginning and end\"\"\"\n",
    "        wav_trimmed, _ = librosa.effects.trim(wav, top_db=top_db)\n",
    "        return wav_trimmed\n",
    "    \n",
    "    def get_duration(self, wav: np.ndarray) -> float:\n",
    "        \"\"\"Get audio duration in seconds\"\"\"\n",
    "        return len(wav) / self.target_sr\n",
    "    \n",
    "    def process_audio(self, audio_path: str, \n",
    "                      normalize: bool = True,\n",
    "                      trim: bool = True) -> Tuple[np.ndarray, float]:\n",
    "        \"\"\"Full audio processing pipeline\"\"\"\n",
    "        # Load\n",
    "        wav, sr = self.load_audio(audio_path)\n",
    "        \n",
    "        # Resample\n",
    "        wav = self.resample(wav, sr)\n",
    "        \n",
    "        # Trim silence\n",
    "        if trim:\n",
    "            wav = self.trim_silence(wav)\n",
    "        \n",
    "        # Normalize\n",
    "        if normalize:\n",
    "            wav = self.normalize(wav)\n",
    "        \n",
    "        duration = self.get_duration(wav)\n",
    "        return wav, duration\n",
    "    \n",
    "    def save_audio(self, wav: np.ndarray, output_path: str):\n",
    "        \"\"\"Save audio to file\"\"\"\n",
    "        sf.write(output_path, wav, self.target_sr)\n",
    "\n",
    "# Initialize processor\n",
    "audio_processor = AudioProcessor(target_sr=24000)\n",
    "print(\"âœ… AudioProcessor initialized (target_sr=24000)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d17a99",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Cell 2.2: Create Sample Dataset (Demo)\n",
    "\n",
    "def create_sample_dataset():\n",
    "    \"\"\"\n",
    "    Create a sample dataset for demonstration\n",
    "    In real usage, replace with your own audio data\n",
    "    \"\"\"\n",
    "    print_section(\"Creating Sample Dataset\")\n",
    "    \n",
    "    # Sample texts for demonstration\n",
    "    sample_data = [\n",
    "        {\n",
    "            \"id\": \"001\",\n",
    "            \"text\": \"Welcome to the machine learning course.\",\n",
    "            \"speaker\": \"teacher_1\",\n",
    "            \"language\": \"English\"\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"002\", \n",
    "            \"text\": \"Today we will learn about neural networks.\",\n",
    "            \"speaker\": \"teacher_1\",\n",
    "            \"language\": \"English\"\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"003\",\n",
    "            \"text\": \"Deep learning has revolutionized artificial intelligence.\",\n",
    "            \"speaker\": \"teacher_1\",\n",
    "            \"language\": \"English\"\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"004\",\n",
    "            \"text\": \"Let's start with the basics of PyTorch.\",\n",
    "            \"speaker\": \"teacher_1\",\n",
    "            \"language\": \"English\"\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"005\",\n",
    "            \"text\": \"Practice is essential for mastering these concepts.\",\n",
    "            \"speaker\": \"teacher_1\",\n",
    "            \"language\": \"English\"\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata_path = DATA_DIR / \"sample_metadata.json\"\n",
    "    with open(metadata_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(sample_data, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"âœ… Created sample metadata: {metadata_path}\")\n",
    "    print(f\"   Total samples: {len(sample_data)}\")\n",
    "    \n",
    "    return sample_data\n",
    "\n",
    "# Create sample dataset\n",
    "sample_data = create_sample_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5eca544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2.3: Generate Training Audio with Base Model\n",
    "\n",
    "def generate_training_audio(metadata: List[Dict], \n",
    "                           output_dir: Path,\n",
    "                           model: Qwen3TTSModel,\n",
    "                           speaker_description: str):\n",
    "    \"\"\"\n",
    "    Generate audio files using Qwen3-TTS for training data\n",
    "    This creates synthetic data for demonstration\n",
    "    In real usage, use actual recorded audio\n",
    "    \"\"\"\n",
    "    print_section(\"Generating Training Audio\")\n",
    "    \n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    generated_files = []\n",
    "    total_duration = 0\n",
    "    \n",
    "    for i, item in enumerate(metadata):\n",
    "        print(f\"\\n[{i+1}/{len(metadata)}] Generating: {item['text'][:50]}...\")\n",
    "        \n",
    "        try:\n",
    "            # Generate audio\n",
    "            wavs, sr = model.generate_voice_design(\n",
    "                text=item['text'],\n",
    "                language=item.get('language', 'English'),\n",
    "                instruct=speaker_description,\n",
    "            )\n",
    "            \n",
    "            # Process audio\n",
    "            wav = wavs[0]\n",
    "            wav, duration = audio_processor.process_audio_array(wav)\n",
    "            \n",
    "            # Save\n",
    "            audio_filename = f\"{item['id']}.wav\"\n",
    "            audio_path = output_dir / audio_filename\n",
    "            audio_processor.save_audio(wav, str(audio_path))\n",
    "            \n",
    "            # Update metadata\n",
    "            item['audio_path'] = str(audio_path)\n",
    "            item['duration'] = duration\n",
    "            total_duration += duration\n",
    "            \n",
    "            generated_files.append(item)\n",
    "            print(f\"   âœ… Saved: {audio_filename} ({duration:.2f}s)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ Error: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Generation Summary:\")\n",
    "    print(f\"   Total files: {len(generated_files)}\")\n",
    "    print(f\"   Total duration: {format_duration(total_duration)}\")\n",
    "    \n",
    "    return generated_files\n",
    "\n",
    "# Add method to AudioProcessor for array processing\n",
    "def process_audio_array(self, wav: np.ndarray) -> Tuple[np.ndarray, float]:\n",
    "    \"\"\"Process audio array (already loaded)\"\"\"\n",
    "    wav = self.normalize(wav)\n",
    "    wav = self.trim_silence(wav)\n",
    "    duration = self.get_duration(wav)\n",
    "    return wav, duration\n",
    "\n",
    "AudioProcessor.process_audio_array = process_audio_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c978220d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Cell 2.4: Load or Generate Training Data\n",
    "\n",
    "print_section(\"Preparing Training Data\")\n",
    "\n",
    "# Check if we have existing audio or need to generate\n",
    "existing_audio_files = list(RAW_AUDIO_DIR.glob(\"*.wav\"))\n",
    "\n",
    "if len(existing_audio_files) > 0:\n",
    "    print(f\"Found {len(existing_audio_files)} existing audio files\")\n",
    "    print(\"Using existing data...\")\n",
    "    \n",
    "    # Load existing metadata if available\n",
    "    metadata_path = DATA_DIR / \"metadata.json\"\n",
    "    if metadata_path.exists():\n",
    "        with open(metadata_path, 'r') as f:\n",
    "            training_data = json.load(f)\n",
    "        print(f\"Loaded metadata: {len(training_data)} samples\")\n",
    "    else:\n",
    "        print(\"âš ï¸ No metadata.json found. Please create one.\")\n",
    "        training_data = []\n",
    "else:\n",
    "    print(\"No existing audio found.\")\n",
    "    print(\"For real training, add your audio files to:\")\n",
    "    print(f\"  {RAW_AUDIO_DIR}\")\n",
    "    print(\"\\nFor demo, we'll use the sample metadata (no audio generation)\")\n",
    "    \n",
    "    # Use sample metadata\n",
    "    training_data = sample_data\n",
    "    print(f\"\\nðŸ“‹ Sample training data: {len(training_data)} items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc9cc6f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Cell 2.5: Create HuggingFace Dataset\n",
    "\n",
    "def create_hf_dataset(metadata: List[Dict], \n",
    "                      audio_dir: Optional[Path] = None) -> Dataset:\n",
    "    \"\"\"\n",
    "    Convert metadata to HuggingFace Dataset format\n",
    "    \"\"\"\n",
    "    print_section(\"Creating HuggingFace Dataset\")\n",
    "    \n",
    "    # Prepare data\n",
    "    data_dict = {\n",
    "        \"id\": [],\n",
    "        \"text\": [],\n",
    "        \"speaker\": [],\n",
    "        \"language\": [],\n",
    "    }\n",
    "    \n",
    "    # Add audio paths if available\n",
    "    if audio_dir and audio_dir.exists():\n",
    "        data_dict[\"audio\"] = []\n",
    "        has_audio = True\n",
    "    else:\n",
    "        has_audio = False\n",
    "    \n",
    "    for item in metadata:\n",
    "        data_dict[\"id\"].append(item[\"id\"])\n",
    "        data_dict[\"text\"].append(item[\"text\"])\n",
    "        data_dict[\"speaker\"].append(item.get(\"speaker\", \"default\"))\n",
    "        data_dict[\"language\"].append(item.get(\"language\", \"English\"))\n",
    "        \n",
    "        if has_audio:\n",
    "            audio_path = audio_dir / f\"{item['id']}.wav\"\n",
    "            if audio_path.exists():\n",
    "                data_dict[\"audio\"].append(str(audio_path))\n",
    "            else:\n",
    "                data_dict[\"audio\"].append(None)\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = Dataset.from_dict(data_dict)\n",
    "    \n",
    "    # Cast audio column if available\n",
    "    if has_audio and any(data_dict[\"audio\"]):\n",
    "        dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=24000))\n",
    "    \n",
    "    print(f\"âœ… Dataset created:\")\n",
    "    print(f\"   Samples: {len(dataset)}\")\n",
    "    print(f\"   Columns: {dataset.column_names}\")\n",
    "    print(f\"   Has audio: {has_audio}\")\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# Create dataset (without audio for demo)\n",
    "# For real training, pass audio_dir=RAW_AUDIO_DIR\n",
    "demo_dataset = create_hf_dataset(training_data, audio_dir=None)\n",
    "print(demo_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14d63a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2.6: Split Dataset\n",
    "\n",
    "def split_dataset(dataset: Dataset, \n",
    "                  train_ratio: float = 0.9,\n",
    "                  seed: int = 42) -> DatasetDict:\n",
    "    \"\"\"Split dataset into train and validation sets\"\"\"\n",
    "    \n",
    "    # Shuffle and split\n",
    "    dataset = dataset.shuffle(seed=seed)\n",
    "    \n",
    "    split_idx = int(len(dataset) * train_ratio)\n",
    "    \n",
    "    train_dataset = dataset.select(range(split_idx))\n",
    "    val_dataset = dataset.select(range(split_idx, len(dataset)))\n",
    "    \n",
    "    dataset_dict = DatasetDict({\n",
    "        \"train\": train_dataset,\n",
    "        \"validation\": val_dataset\n",
    "    })\n",
    "    \n",
    "    print(f\"ðŸ“Š Dataset Split:\")\n",
    "    print(f\"   Train: {len(train_dataset)} samples\")\n",
    "    print(f\"   Validation: {len(val_dataset)} samples\")\n",
    "    \n",
    "    return dataset_dict\n",
    "\n",
    "# Split dataset\n",
    "dataset_splits = split_dataset(demo_dataset, train_ratio=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba3d672",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Model Configuration\n",
    "\n",
    "à¸à¸²à¸£à¸•à¸±à¹‰à¸‡à¸„à¹ˆà¸² model à¸ªà¸³à¸«à¸£à¸±à¸š fine-tuning\n",
    "\n",
    "### LoRA Configuration\n",
    "\n",
    "LoRA (Low-Rank Adaptation) à¸Šà¹ˆà¸§à¸¢à¸¥à¸” memory à¹à¸¥à¸°à¹€à¸§à¸¥à¸²à¹ƒà¸™à¸à¸²à¸£ train:\n",
    "\n",
    "| Parameter | Description | Recommended |\n",
    "|-----------|-------------|-------------|\n",
    "| `r` | Rank of update matrices | 8-64 |\n",
    "| `lora_alpha` | Scaling factor | 16-32 |\n",
    "| `lora_dropout` | Dropout probability | 0.05-0.1 |\n",
    "| `target_modules` | Layers to adapt | attention layers |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46286c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3.1: Load Base Model\n",
    "\n",
    "print_section(\"Loading Base Model\")\n",
    "\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "DTYPE = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Dtype: {DTYPE}\")\n",
    "print(\"\\nLoading model... (this may take a few minutes)\")\n",
    "\n",
    "# Load base model for fine-tuning\n",
    "base_model = Qwen3TTSModel.from_pretrained(\n",
    "    \"Qwen/Qwen3-TTS-12Hz-0.6B-Base\",  # Use smaller model for demo\n",
    "    device_map=DEVICE,\n",
    "    dtype=DTYPE,\n",
    "    attn_implementation=\"eager\",  # Use eager for training compatibility\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… Base model loaded!\")\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8e4073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3.2: Configure LoRA\n",
    "\n",
    "print_section(\"Configuring LoRA\")\n",
    "\n",
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=16,                          # Rank\n",
    "    lora_alpha=32,                 # Scaling\n",
    "    lora_dropout=0.05,             # Dropout\n",
    "    target_modules=[               # Target attention layers\n",
    "        \"q_proj\",\n",
    "        \"k_proj\", \n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    inference_mode=False,\n",
    ")\n",
    "\n",
    "print(\"ðŸ“‹ LoRA Configuration:\")\n",
    "print(f\"   Rank (r): {lora_config.r}\")\n",
    "print(f\"   Alpha: {lora_config.lora_alpha}\")\n",
    "print(f\"   Dropout: {lora_config.lora_dropout}\")\n",
    "print(f\"   Target modules: {lora_config.target_modules}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3158e440",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Cell 3.3: Apply LoRA to Model\n",
    "\n",
    "print_section(\"Applying LoRA\")\n",
    "\n",
    "# Note: This is a simplified example\n",
    "# Actual Qwen3-TTS may require specific adapter configuration\n",
    "\n",
    "# For demonstration, we'll show the structure\n",
    "print(\"âš ï¸ Note: Qwen3-TTS fine-tuning requires specific configuration\")\n",
    "print(\"   This demo shows the general approach\")\n",
    "print(\"\\nðŸ“‹ Steps for actual fine-tuning:\")\n",
    "print(\"   1. Extract the language model component\")\n",
    "print(\"   2. Apply LoRA adapters\")\n",
    "print(\"   3. Configure audio tokenizer training (if needed)\")\n",
    "print(\"   4. Set up the training loop\")\n",
    "\n",
    "# Count trainable parameters (example)\n",
    "def count_parameters(model):\n",
    "    \"\"\"Count trainable parameters\"\"\"\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    return trainable, total\n",
    "\n",
    "# For actual implementation:\n",
    "# peft_model = get_peft_model(base_model.language_model, lora_config)\n",
    "# trainable, total = count_parameters(peft_model)\n",
    "# print(f\"Trainable: {trainable:,} / {total:,} ({100*trainable/total:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827c15cd",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Training Configuration\n",
    "\n",
    "à¸•à¸±à¹‰à¸‡à¸„à¹ˆà¸² hyperparameters à¸ªà¸³à¸«à¸£à¸±à¸šà¸à¸²à¸£ train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e3e659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4.1: Training Arguments\n",
    "\n",
    "print_section(\"Training Arguments\")\n",
    "\n",
    "# Training configuration\n",
    "training_config = {\n",
    "    # Basic settings\n",
    "    \"output_dir\": str(CHECKPOINT_DIR),\n",
    "    \"num_train_epochs\": 3,\n",
    "    \"per_device_train_batch_size\": 2,\n",
    "    \"per_device_eval_batch_size\": 2,\n",
    "    \"gradient_accumulation_steps\": 4,\n",
    "    \n",
    "    # Learning rate\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"lr_scheduler_type\": \"cosine\",\n",
    "    \"warmup_ratio\": 0.1,\n",
    "    \n",
    "    # Optimization\n",
    "    \"optim\": \"adamw_torch\",\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"max_grad_norm\": 1.0,\n",
    "    \n",
    "    # Precision\n",
    "    \"bf16\": torch.cuda.is_available(),\n",
    "    \"fp16\": False,\n",
    "    \n",
    "    # Logging\n",
    "    \"logging_dir\": str(LOG_DIR),\n",
    "    \"logging_steps\": 10,\n",
    "    \"logging_first_step\": True,\n",
    "    \n",
    "    # Checkpointing\n",
    "    \"save_strategy\": \"steps\",\n",
    "    \"save_steps\": 100,\n",
    "    \"save_total_limit\": 3,\n",
    "    \n",
    "    # Evaluation\n",
    "    \"eval_strategy\": \"steps\",\n",
    "    \"eval_steps\": 100,\n",
    "    \n",
    "    # Other\n",
    "    \"seed\": 42,\n",
    "    \"dataloader_num_workers\": 2,\n",
    "    \"remove_unused_columns\": False,\n",
    "    \"report_to\": \"tensorboard\",\n",
    "}\n",
    "\n",
    "# Create TrainingArguments\n",
    "training_args = TrainingArguments(**training_config)\n",
    "\n",
    "print(\"ðŸ“‹ Training Configuration:\")\n",
    "for key, value in training_config.items():\n",
    "    print(f\"   {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6214b0e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Cell 4.2: Calculate Training Estimates\n",
    "\n",
    "print_section(\"Training Estimates\")\n",
    "\n",
    "# Estimates based on configuration\n",
    "num_samples = len(dataset_splits[\"train\"])\n",
    "batch_size = training_config[\"per_device_train_batch_size\"]\n",
    "grad_accum = training_config[\"gradient_accumulation_steps\"]\n",
    "epochs = training_config[\"num_train_epochs\"]\n",
    "\n",
    "effective_batch_size = batch_size * grad_accum\n",
    "steps_per_epoch = num_samples // effective_batch_size\n",
    "total_steps = steps_per_epoch * epochs\n",
    "\n",
    "print(f\"ðŸ“Š Training Estimates:\")\n",
    "print(f\"   Training samples: {num_samples}\")\n",
    "print(f\"   Effective batch size: {effective_batch_size}\")\n",
    "print(f\"   Steps per epoch: {steps_per_epoch}\")\n",
    "print(f\"   Total steps: {total_steps}\")\n",
    "print(f\"   Checkpoints: ~{total_steps // training_config['save_steps']}\")\n",
    "\n",
    "# Time estimate (rough)\n",
    "# Assuming ~1 second per step on A100\n",
    "estimated_time_seconds = total_steps * 1.0\n",
    "print(f\"\\nâ±ï¸ Estimated training time: {format_duration(estimated_time_seconds)}\")\n",
    "print(\"   (Actual time depends on hardware and data)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7154fe",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Custom Training Loop\n",
    "\n",
    "à¸ªà¸³à¸«à¸£à¸±à¸š TTS model à¸­à¸²à¸ˆà¸•à¹‰à¸­à¸‡à¹ƒà¸Šà¹‰ custom training loop\n",
    "à¹€à¸žà¸·à¹ˆà¸­à¸ˆà¸±à¸”à¸à¸²à¸£à¸à¸±à¸š audio encoding/decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff99d8eb",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Cell 5.1: Custom Data Collator\n",
    "\n",
    "class TTSDataCollator:\n",
    "    \"\"\"Custom data collator for TTS training\"\"\"\n",
    "    \n",
    "    def __init__(self, tokenizer, audio_tokenizer, max_length: int = 512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.audio_tokenizer = audio_tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __call__(self, features: List[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Collate batch of features\"\"\"\n",
    "        \n",
    "        texts = [f[\"text\"] for f in features]\n",
    "        \n",
    "        # Tokenize text\n",
    "        text_encodings = self.tokenizer(\n",
    "            texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        batch = {\n",
    "            \"input_ids\": text_encodings[\"input_ids\"],\n",
    "            \"attention_mask\": text_encodings[\"attention_mask\"],\n",
    "        }\n",
    "        \n",
    "        # Process audio if available\n",
    "        if \"audio\" in features[0] and features[0][\"audio\"] is not None:\n",
    "            audio_arrays = [f[\"audio\"][\"array\"] for f in features]\n",
    "            # Encode audio to tokens\n",
    "            # audio_tokens = self.audio_tokenizer.encode(audio_arrays)\n",
    "            # batch[\"labels\"] = audio_tokens\n",
    "        \n",
    "        return batch\n",
    "\n",
    "print(\"âœ… TTSDataCollator defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb5f060",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Cell 5.2: Custom Trainer Class\n",
    "\n",
    "class TTSTrainer(Trainer):\n",
    "    \"\"\"Custom trainer for TTS models\"\"\"\n",
    "    \n",
    "    def __init__(self, audio_tokenizer=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.audio_tokenizer = audio_tokenizer\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        \"\"\"\n",
    "        Custom loss computation for TTS\n",
    "        Combines text-to-audio prediction loss\n",
    "        \"\"\"\n",
    "        # Standard forward pass\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        # TTS specific loss computation\n",
    "        # This would include:\n",
    "        # 1. Audio token prediction loss\n",
    "        # 2. Duration prediction loss (optional)\n",
    "        # 3. Pitch prediction loss (optional)\n",
    "        \n",
    "        loss = outputs.loss if hasattr(outputs, 'loss') else outputs[0]\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "    \n",
    "    def evaluation_loop(self, *args, **kwargs):\n",
    "        \"\"\"Custom evaluation with audio generation\"\"\"\n",
    "        output = super().evaluation_loop(*args, **kwargs)\n",
    "        \n",
    "        # Add custom metrics\n",
    "        # - MOS (Mean Opinion Score) estimation\n",
    "        # - Speaker similarity\n",
    "        # - Intelligibility metrics\n",
    "        \n",
    "        return output\n",
    "\n",
    "print(\"âœ… TTSTrainer class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0add79b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5.3: Training Function\n",
    "\n",
    "def train_tts_model(\n",
    "    model,\n",
    "    train_dataset: Dataset,\n",
    "    eval_dataset: Dataset,\n",
    "    training_args: TrainingArguments,\n",
    "    audio_tokenizer=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Main training function for TTS model\n",
    "    \"\"\"\n",
    "    print_section(\"Starting Training\")\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = TTSTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        audio_tokenizer=audio_tokenizer,\n",
    "    )\n",
    "    \n",
    "    # Training info\n",
    "    print(f\"ðŸ“‹ Training Info:\")\n",
    "    print(f\"   Model: {model.__class__.__name__}\")\n",
    "    print(f\"   Train samples: {len(train_dataset)}\")\n",
    "    print(f\"   Eval samples: {len(eval_dataset)}\")\n",
    "    print(f\"   Output dir: {training_args.output_dir}\")\n",
    "    \n",
    "    # Start training\n",
    "    print(\"\\nðŸš€ Starting training...\")\n",
    "    train_result = trainer.train()\n",
    "    \n",
    "    # Save final model\n",
    "    print(\"\\nðŸ’¾ Saving final model...\")\n",
    "    trainer.save_model()\n",
    "    \n",
    "    # Log metrics\n",
    "    metrics = train_result.metrics\n",
    "    trainer.log_metrics(\"train\", metrics)\n",
    "    trainer.save_metrics(\"train\", metrics)\n",
    "    \n",
    "    print(\"\\nâœ… Training complete!\")\n",
    "    return trainer, metrics\n",
    "\n",
    "print(\"âœ… Training function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d242affb",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 6: Run Training (Demo)\n",
    "\n",
    "âš ï¸ **Note:** This section demonstrates the training workflow.\n",
    "For actual training, you need:\n",
    "1. Real audio data (several hours)\n",
    "2. Proper GPU resources (24GB+ VRAM)\n",
    "3. Qwen3-TTS specific training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52795bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6.1: Training Demo (Dry Run)\n",
    "\n",
    "print_section(\"Training Demo (Dry Run)\")\n",
    "\n",
    "print(\"âš ï¸ This is a demonstration of the training workflow\")\n",
    "print(\"   Actual training requires real audio data and sufficient GPU memory\")\n",
    "print(\"\\nðŸ“‹ What would happen in real training:\")\n",
    "print(\"   1. Load audio files and transcriptions\")\n",
    "print(\"   2. Encode audio to discrete tokens\")\n",
    "print(\"   3. Train model to predict audio tokens from text\")\n",
    "print(\"   4. Save checkpoints periodically\")\n",
    "print(\"   5. Evaluate on validation set\")\n",
    "\n",
    "# Simulate training steps\n",
    "print(\"\\nðŸ”„ Simulated Training Progress:\")\n",
    "for epoch in range(1, 4):\n",
    "    print(f\"\\n   Epoch {epoch}/3:\")\n",
    "    for step in range(0, 100, 25):\n",
    "        loss = 5.0 - (epoch * 0.5 + step * 0.01)  # Simulated decreasing loss\n",
    "        print(f\"      Step {step:3d} | Loss: {loss:.4f}\")\n",
    "\n",
    "print(\"\\nâœ… Training simulation complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c371b0",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Cell 6.2: Monitor Training (TensorBoard)\n",
    "\n",
    "print_section(\"Training Monitoring\")\n",
    "\n",
    "print(\"ðŸ“Š To monitor training with TensorBoard:\")\n",
    "print(f\"\\n   tensorboard --logdir={LOG_DIR}\")\n",
    "print(\"\\n   Then open: http://localhost:6006\")\n",
    "print(\"\\nðŸ“‹ Metrics to monitor:\")\n",
    "print(\"   - Training loss\")\n",
    "print(\"   - Validation loss\")\n",
    "print(\"   - Learning rate\")\n",
    "print(\"   - GPU memory usage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91267439",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 7: Model Evaluation\n",
    "\n",
    "à¸›à¸£à¸°à¹€à¸¡à¸´à¸™à¸œà¸¥ model à¸«à¸¥à¸±à¸‡à¸à¸²à¸£ train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de018b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7.1: Evaluation Metrics\n",
    "\n",
    "class TTSEvaluator:\n",
    "    \"\"\"Evaluation utilities for TTS models\"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def generate_samples(self, texts: List[str], **kwargs) -> List[np.ndarray]:\n",
    "        \"\"\"Generate audio samples from text\"\"\"\n",
    "        samples = []\n",
    "        for text in texts:\n",
    "            wav, sr = self.model.generate(text, **kwargs)\n",
    "            samples.append(wav)\n",
    "        return samples\n",
    "    \n",
    "    def compute_rtf(self, text: str, num_runs: int = 5) -> float:\n",
    "        \"\"\"\n",
    "        Compute Real-Time Factor (RTF)\n",
    "        RTF < 1 means faster than real-time\n",
    "        \"\"\"\n",
    "        import time\n",
    "        \n",
    "        total_time = 0\n",
    "        total_duration = 0\n",
    "        \n",
    "        for _ in range(num_runs):\n",
    "            start = time.time()\n",
    "            wav, sr = self.model.generate(text)\n",
    "            elapsed = time.time() - start\n",
    "            \n",
    "            audio_duration = len(wav) / sr\n",
    "            total_time += elapsed\n",
    "            total_duration += audio_duration\n",
    "        \n",
    "        rtf = total_time / total_duration\n",
    "        return rtf\n",
    "    \n",
    "    def speaker_similarity(self, ref_audio: np.ndarray, \n",
    "                          gen_audio: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Compute speaker similarity score\n",
    "        (Requires speaker embedding model)\n",
    "        \"\"\"\n",
    "        # Placeholder - would use speaker verification model\n",
    "        return 0.85\n",
    "    \n",
    "    def intelligibility_score(self, text: str, audio: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Compute intelligibility score using ASR\n",
    "        (Requires ASR model)\n",
    "        \"\"\"\n",
    "        # Placeholder - would use ASR model\n",
    "        return 0.92\n",
    "\n",
    "print(\"âœ… TTSEvaluator class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848f9928",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Cell 7.2: Run Evaluation\n",
    "\n",
    "print_section(\"Model Evaluation\")\n",
    "\n",
    "# Test texts for evaluation\n",
    "eval_texts = [\n",
    "    \"Hello, this is a test of the text to speech system.\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Machine learning enables computers to learn from data.\",\n",
    "]\n",
    "\n",
    "print(\"ðŸ“‹ Evaluation Tests:\")\n",
    "print(\"\\n1. Generation Quality:\")\n",
    "for i, text in enumerate(eval_texts, 1):\n",
    "    print(f\"   Test {i}: {text[:50]}...\")\n",
    "\n",
    "print(\"\\n2. Real-Time Factor (RTF):\")\n",
    "print(\"   RTF measures inference speed relative to audio duration\")\n",
    "print(\"   RTF < 1.0 = faster than real-time\")\n",
    "print(\"   Target: RTF < 0.5 for production\")\n",
    "\n",
    "print(\"\\n3. Speaker Consistency:\")\n",
    "print(\"   Measures how consistent the voice is across generations\")\n",
    "\n",
    "print(\"\\n4. Intelligibility:\")\n",
    "print(\"   Measures how well the speech can be understood (ASR-based)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6236f3b",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 8: Export and Deploy\n",
    "\n",
    "Export trained model à¸ªà¸³à¸«à¸£à¸±à¸š deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e17c0b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Cell 8.1: Save Trained Model\n",
    "\n",
    "def save_trained_model(model, output_path: Path, config: dict = None):\n",
    "    \"\"\"Save trained model and configuration\"\"\"\n",
    "    \n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save model weights\n",
    "    model_path = output_path / \"model.pt\"\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"âœ… Saved model weights: {model_path}\")\n",
    "    \n",
    "    # Save configuration\n",
    "    if config:\n",
    "        config_path = output_path / \"config.json\"\n",
    "        with open(config_path, 'w') as f:\n",
    "            json.dump(config, f, indent=2)\n",
    "        print(f\"âœ… Saved config: {config_path}\")\n",
    "    \n",
    "    # Save LoRA adapters (if applicable)\n",
    "    # adapter_path = output_path / \"adapter\"\n",
    "    # model.save_pretrained(adapter_path)\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "print(\"âœ… Save function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536b12ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8.2: Export for Inference\n",
    "\n",
    "def export_for_inference(model_path: Path, export_path: Path):\n",
    "    \"\"\"Export model optimized for inference\"\"\"\n",
    "    \n",
    "    export_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(\"ðŸ“‹ Export options:\")\n",
    "    print(\"\\n1. PyTorch (standard):\")\n",
    "    print(\"   - Full precision (FP32)\")\n",
    "    print(\"   - Half precision (FP16/BF16)\")\n",
    "    \n",
    "    print(\"\\n2. ONNX Export:\")\n",
    "    print(\"   - Cross-platform compatibility\")\n",
    "    print(\"   - Optimized inference\")\n",
    "    \n",
    "    print(\"\\n3. TensorRT (NVIDIA):\")\n",
    "    print(\"   - Maximum GPU performance\")\n",
    "    print(\"   - Requires NVIDIA GPU\")\n",
    "    \n",
    "    print(\"\\n4. Quantization:\")\n",
    "    print(\"   - INT8 for smaller size\")\n",
    "    print(\"   - Faster inference on CPU\")\n",
    "\n",
    "print(\"âœ… Export options defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa149d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8.3: Inference Script Template\n",
    "\n",
    "inference_script = '''\n",
    "\"\"\"\n",
    "Inference script for fine-tuned Qwen3-TTS model\n",
    "\"\"\"\n",
    "import torch\n",
    "from qwen_tts import Qwen3TTSModel\n",
    "import soundfile as sf\n",
    "\n",
    "class FineTunedTTS:\n",
    "    def __init__(self, model_path: str, device: str = \"cuda\"):\n",
    "        self.device = device\n",
    "        \n",
    "        # Load base model\n",
    "        self.model = Qwen3TTSModel.from_pretrained(\n",
    "            \"Qwen/Qwen3-TTS-12Hz-0.6B-Base\",\n",
    "            device_map=device,\n",
    "        )\n",
    "        \n",
    "        # Load fine-tuned weights\n",
    "        state_dict = torch.load(f\"{model_path}/model.pt\")\n",
    "        self.model.load_state_dict(state_dict, strict=False)\n",
    "        \n",
    "        self.model.eval()\n",
    "    \n",
    "    def generate(self, text: str, output_path: str = None):\n",
    "        \"\"\"Generate speech from text\"\"\"\n",
    "        with torch.no_grad():\n",
    "            wav, sr = self.model.generate(text)\n",
    "        \n",
    "        if output_path:\n",
    "            sf.write(output_path, wav, sr)\n",
    "        \n",
    "        return wav, sr\n",
    "\n",
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    tts = FineTunedTTS(\"./checkpoints/final\")\n",
    "    wav, sr = tts.generate(\n",
    "        \"Hello, this is my fine-tuned voice!\",\n",
    "        output_path=\"output.wav\"\n",
    "    )\n",
    "    print(f\"Generated {len(wav)/sr:.2f} seconds of audio\")\n",
    "'''\n",
    "\n",
    "# Save inference script\n",
    "script_path = OUTPUT_DIR / \"inference.py\"\n",
    "with open(script_path, 'w') as f:\n",
    "    f.write(inference_script)\n",
    "\n",
    "print(f\"âœ… Saved inference script: {script_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd01cf4",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 9: Summary and Best Practices\n",
    "\n",
    "### Training Checklist\n",
    "\n",
    "| Step | Description | Status |\n",
    "|------|-------------|--------|\n",
    "| 1 | Collect high-quality audio | â¬œ |\n",
    "| 2 | Transcribe accurately | â¬œ |\n",
    "| 3 | Preprocess and normalize | â¬œ |\n",
    "| 4 | Configure training | â¬œ |\n",
    "| 5 | Monitor training | â¬œ |\n",
    "| 6 | Evaluate results | â¬œ |\n",
    "| 7 | Export model | â¬œ |\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Data Quality**\n",
    "   - Use clean, noise-free recordings\n",
    "   - Consistent microphone and environment\n",
    "   - Accurate transcriptions\n",
    "\n",
    "2. **Training**\n",
    "   - Start with small learning rate\n",
    "   - Use gradient clipping\n",
    "   - Monitor for overfitting\n",
    "\n",
    "3. **Evaluation**\n",
    "   - Listen to generated samples\n",
    "   - Compare with reference audio\n",
    "   - Test on diverse texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa6ab7f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Cell 9.1: Final Summary\n",
    "\n",
    "print_section(\"Training Lab Summary\")\n",
    "\n",
    "print(\"ðŸ“š What we covered:\")\n",
    "print(\"   1. Dataset preparation for TTS\")\n",
    "print(\"   2. Audio processing pipeline\")\n",
    "print(\"   3. LoRA configuration for efficient fine-tuning\")\n",
    "print(\"   4. Training hyperparameters\")\n",
    "print(\"   5. Custom training loop for TTS\")\n",
    "print(\"   6. Model evaluation metrics\")\n",
    "print(\"   7. Export and deployment\")\n",
    "\n",
    "print(\"\\nðŸ“ Generated Files:\")\n",
    "for f in OUTPUT_DIR.glob(\"*\"):\n",
    "    print(f\"   {f.name}\")\n",
    "\n",
    "print(\"\\nðŸ”— Resources:\")\n",
    "print(\"   - Qwen3-TTS GitHub: https://github.com/QwenLM/Qwen3-TTS\")\n",
    "print(\"   - HuggingFace PEFT: https://huggingface.co/docs/peft\")\n",
    "print(\"   - Training Tips: https://huggingface.co/docs/transformers/training\")\n",
    "\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5646f5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9.2: Cleanup\n",
    "\n",
    "def cleanup():\n",
    "    \"\"\"Clean up resources\"\"\"\n",
    "    global base_model\n",
    "    \n",
    "    if 'base_model' in globals():\n",
    "        del base_model\n",
    "        print(\"âœ… Deleted base model\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "        print(\"âœ… Cleared GPU cache\")\n",
    "\n",
    "# Uncomment to cleanup\n",
    "# cleanup()\n",
    "\n",
    "print(\"To free GPU memory, run: cleanup()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcc1552",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ“ Lab Exercises\n",
    "\n",
    "### Exercise 1: Data Preparation\n",
    "à¸ªà¸£à¹‰à¸²à¸‡ dataset à¸ˆà¸²à¸à¹„à¸Ÿà¸¥à¹Œà¹€à¸ªà¸µà¸¢à¸‡à¸‚à¸­à¸‡à¸„à¸¸à¸“à¹€à¸­à¸‡:\n",
    "- à¸šà¸±à¸™à¸—à¸¶à¸à¹€à¸ªà¸µà¸¢à¸‡à¸­à¹ˆà¸²à¸™ 10-20 à¸›à¸£à¸°à¹‚à¸¢à¸„\n",
    "- à¸ªà¸£à¹‰à¸²à¸‡ metadata.json\n",
    "- Process audio files\n",
    "\n",
    "### Exercise 2: Hyperparameter Tuning\n",
    "à¸—à¸”à¸¥à¸­à¸‡à¸›à¸£à¸±à¸šà¸„à¹ˆà¸² hyperparameters:\n",
    "- à¹€à¸›à¸¥à¸µà¹ˆà¸¢à¸™ LoRA rank (r = 8, 16, 32, 64)\n",
    "- à¸›à¸£à¸±à¸š learning rate\n",
    "- à¹€à¸›à¸¥à¸µà¹ˆà¸¢à¸™ batch size\n",
    "\n",
    "### Exercise 3: Evaluation\n",
    "à¸ªà¸£à¹‰à¸²à¸‡ evaluation pipeline:\n",
    "- Generate samples à¸ˆà¸²à¸ test set\n",
    "- à¸„à¸³à¸™à¸§à¸“ RTF\n",
    "- à¹€à¸›à¸£à¸µà¸¢à¸šà¹€à¸—à¸µà¸¢à¸šà¸à¸±à¸š base model\n",
    "\n",
    "### Exercise 4: Deployment\n",
    "à¸ªà¸£à¹‰à¸²à¸‡ inference API:\n",
    "- FastAPI endpoint\n",
    "- Streaming audio response\n",
    "- Error handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bacec1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise Space\n",
    "\n",
    "print_section(\"Exercise Space\")\n",
    "\n",
    "# Your code here!\n",
    "print(\"Write your exercise solutions here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b277ae",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ“š Additional Resources\n",
    "\n",
    "- **Qwen3-TTS Paper**: https://arxiv.org/abs/2601.15621\n",
    "- **LoRA Paper**: https://arxiv.org/abs/2106.09685\n",
    "- **HuggingFace Transformers**: https://huggingface.co/docs/transformers\n",
    "- **PEFT Documentation**: https://huggingface.co/docs/peft\n",
    "- **Audio Processing**: https://librosa.org/doc/latest/\n",
    "\n",
    "---\n",
    "\n",
    "**End of Lab**"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
