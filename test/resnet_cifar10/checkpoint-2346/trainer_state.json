{
  "best_metric": 0.7437919974327087,
  "best_model_checkpoint": "./resnet_cifar10/checkpoint-2346",
  "epoch": 6.0,
  "eval_steps": 500,
  "global_step": 2346,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.02557544757033248,
      "grad_norm": 6.560461044311523,
      "learning_rate": 0.0009974424552429667,
      "loss": 2.317,
      "step": 10
    },
    {
      "epoch": 0.05115089514066496,
      "grad_norm": 4.858614444732666,
      "learning_rate": 0.0009948849104859335,
      "loss": 1.8972,
      "step": 20
    },
    {
      "epoch": 0.07672634271099744,
      "grad_norm": 3.7562742233276367,
      "learning_rate": 0.0009923273657289002,
      "loss": 1.8483,
      "step": 30
    },
    {
      "epoch": 0.10230179028132992,
      "grad_norm": 3.120326042175293,
      "learning_rate": 0.000989769820971867,
      "loss": 1.6826,
      "step": 40
    },
    {
      "epoch": 0.1278772378516624,
      "grad_norm": 3.0544543266296387,
      "learning_rate": 0.0009872122762148339,
      "loss": 1.6543,
      "step": 50
    },
    {
      "epoch": 0.1534526854219949,
      "grad_norm": 3.704089403152466,
      "learning_rate": 0.0009846547314578005,
      "loss": 1.5913,
      "step": 60
    },
    {
      "epoch": 0.17902813299232737,
      "grad_norm": 3.1817946434020996,
      "learning_rate": 0.0009820971867007674,
      "loss": 1.5538,
      "step": 70
    },
    {
      "epoch": 0.20460358056265984,
      "grad_norm": 3.757596969604492,
      "learning_rate": 0.000979539641943734,
      "loss": 1.5739,
      "step": 80
    },
    {
      "epoch": 0.23017902813299232,
      "grad_norm": 2.931636333465576,
      "learning_rate": 0.000976982097186701,
      "loss": 1.5326,
      "step": 90
    },
    {
      "epoch": 0.2557544757033248,
      "grad_norm": 2.9103219509124756,
      "learning_rate": 0.0009744245524296675,
      "loss": 1.5629,
      "step": 100
    },
    {
      "epoch": 0.2813299232736573,
      "grad_norm": 4.329400539398193,
      "learning_rate": 0.0009718670076726343,
      "loss": 1.4493,
      "step": 110
    },
    {
      "epoch": 0.3069053708439898,
      "grad_norm": 2.3314871788024902,
      "learning_rate": 0.000969309462915601,
      "loss": 1.4526,
      "step": 120
    },
    {
      "epoch": 0.33248081841432225,
      "grad_norm": 4.193652153015137,
      "learning_rate": 0.0009667519181585678,
      "loss": 1.3622,
      "step": 130
    },
    {
      "epoch": 0.35805626598465473,
      "grad_norm": 2.440829038619995,
      "learning_rate": 0.0009641943734015346,
      "loss": 1.4034,
      "step": 140
    },
    {
      "epoch": 0.3836317135549872,
      "grad_norm": 3.6103391647338867,
      "learning_rate": 0.0009616368286445013,
      "loss": 1.3942,
      "step": 150
    },
    {
      "epoch": 0.4092071611253197,
      "grad_norm": 3.0756587982177734,
      "learning_rate": 0.0009590792838874681,
      "loss": 1.3761,
      "step": 160
    },
    {
      "epoch": 0.43478260869565216,
      "grad_norm": 2.746140480041504,
      "learning_rate": 0.0009565217391304348,
      "loss": 1.3243,
      "step": 170
    },
    {
      "epoch": 0.46035805626598464,
      "grad_norm": 5.0564680099487305,
      "learning_rate": 0.0009539641943734016,
      "loss": 1.3155,
      "step": 180
    },
    {
      "epoch": 0.4859335038363171,
      "grad_norm": 3.365220785140991,
      "learning_rate": 0.0009514066496163683,
      "loss": 1.3803,
      "step": 190
    },
    {
      "epoch": 0.5115089514066496,
      "grad_norm": 3.6780784130096436,
      "learning_rate": 0.0009488491048593351,
      "loss": 1.3148,
      "step": 200
    },
    {
      "epoch": 0.5370843989769821,
      "grad_norm": 3.811976909637451,
      "learning_rate": 0.0009462915601023018,
      "loss": 1.2461,
      "step": 210
    },
    {
      "epoch": 0.5626598465473146,
      "grad_norm": 2.9431912899017334,
      "learning_rate": 0.0009437340153452686,
      "loss": 1.2728,
      "step": 220
    },
    {
      "epoch": 0.5882352941176471,
      "grad_norm": 3.7507107257843018,
      "learning_rate": 0.0009411764705882353,
      "loss": 1.2185,
      "step": 230
    },
    {
      "epoch": 0.6138107416879796,
      "grad_norm": 3.129167079925537,
      "learning_rate": 0.0009386189258312021,
      "loss": 1.235,
      "step": 240
    },
    {
      "epoch": 0.639386189258312,
      "grad_norm": 4.862919807434082,
      "learning_rate": 0.0009360613810741688,
      "loss": 1.2238,
      "step": 250
    },
    {
      "epoch": 0.6649616368286445,
      "grad_norm": 4.46756649017334,
      "learning_rate": 0.0009335038363171356,
      "loss": 1.2457,
      "step": 260
    },
    {
      "epoch": 0.690537084398977,
      "grad_norm": 2.6079392433166504,
      "learning_rate": 0.0009309462915601023,
      "loss": 1.268,
      "step": 270
    },
    {
      "epoch": 0.7161125319693095,
      "grad_norm": 2.9999873638153076,
      "learning_rate": 0.0009283887468030691,
      "loss": 1.2259,
      "step": 280
    },
    {
      "epoch": 0.7416879795396419,
      "grad_norm": 4.223467826843262,
      "learning_rate": 0.0009258312020460357,
      "loss": 1.2283,
      "step": 290
    },
    {
      "epoch": 0.7672634271099744,
      "grad_norm": 3.529348850250244,
      "learning_rate": 0.0009232736572890026,
      "loss": 1.1312,
      "step": 300
    },
    {
      "epoch": 0.7928388746803069,
      "grad_norm": 3.623095989227295,
      "learning_rate": 0.0009207161125319693,
      "loss": 1.1911,
      "step": 310
    },
    {
      "epoch": 0.8184143222506394,
      "grad_norm": 2.668268918991089,
      "learning_rate": 0.0009181585677749361,
      "loss": 1.1505,
      "step": 320
    },
    {
      "epoch": 0.8439897698209718,
      "grad_norm": 3.982362985610962,
      "learning_rate": 0.0009156010230179028,
      "loss": 1.1461,
      "step": 330
    },
    {
      "epoch": 0.8695652173913043,
      "grad_norm": 3.5054357051849365,
      "learning_rate": 0.0009130434782608695,
      "loss": 1.0647,
      "step": 340
    },
    {
      "epoch": 0.8951406649616368,
      "grad_norm": 2.5563175678253174,
      "learning_rate": 0.0009104859335038363,
      "loss": 1.1417,
      "step": 350
    },
    {
      "epoch": 0.9207161125319693,
      "grad_norm": 3.4673984050750732,
      "learning_rate": 0.0009079283887468031,
      "loss": 1.1295,
      "step": 360
    },
    {
      "epoch": 0.9462915601023018,
      "grad_norm": 2.978433609008789,
      "learning_rate": 0.0009053708439897699,
      "loss": 1.1051,
      "step": 370
    },
    {
      "epoch": 0.9718670076726342,
      "grad_norm": 2.489053249359131,
      "learning_rate": 0.0009028132992327366,
      "loss": 1.0949,
      "step": 380
    },
    {
      "epoch": 0.9974424552429667,
      "grad_norm": 3.1431591510772705,
      "learning_rate": 0.0009002557544757032,
      "loss": 1.121,
      "step": 390
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.6007,
      "eval_loss": 1.1146992444992065,
      "eval_runtime": 5.4314,
      "eval_samples_per_second": 1841.149,
      "eval_steps_per_second": 14.545,
      "step": 391
    },
    {
      "epoch": 1.0230179028132993,
      "grad_norm": 4.448899745941162,
      "learning_rate": 0.0008976982097186701,
      "loss": 1.0651,
      "step": 400
    },
    {
      "epoch": 1.0485933503836318,
      "grad_norm": 3.4813833236694336,
      "learning_rate": 0.0008951406649616369,
      "loss": 1.0313,
      "step": 410
    },
    {
      "epoch": 1.0741687979539642,
      "grad_norm": 2.946770429611206,
      "learning_rate": 0.0008925831202046036,
      "loss": 1.0673,
      "step": 420
    },
    {
      "epoch": 1.0997442455242967,
      "grad_norm": 3.261096477508545,
      "learning_rate": 0.0008900255754475704,
      "loss": 0.9839,
      "step": 430
    },
    {
      "epoch": 1.1253196930946292,
      "grad_norm": 3.3818960189819336,
      "learning_rate": 0.000887468030690537,
      "loss": 1.0602,
      "step": 440
    },
    {
      "epoch": 1.1508951406649617,
      "grad_norm": 2.843533754348755,
      "learning_rate": 0.0008849104859335039,
      "loss": 1.0689,
      "step": 450
    },
    {
      "epoch": 1.1764705882352942,
      "grad_norm": 2.587015151977539,
      "learning_rate": 0.0008823529411764706,
      "loss": 1.0226,
      "step": 460
    },
    {
      "epoch": 1.2020460358056266,
      "grad_norm": 3.0906779766082764,
      "learning_rate": 0.0008797953964194374,
      "loss": 0.9723,
      "step": 470
    },
    {
      "epoch": 1.227621483375959,
      "grad_norm": 4.502353668212891,
      "learning_rate": 0.0008772378516624041,
      "loss": 1.0007,
      "step": 480
    },
    {
      "epoch": 1.2531969309462916,
      "grad_norm": 4.9666242599487305,
      "learning_rate": 0.0008746803069053708,
      "loss": 0.9993,
      "step": 490
    },
    {
      "epoch": 1.278772378516624,
      "grad_norm": 3.6311213970184326,
      "learning_rate": 0.0008721227621483376,
      "loss": 0.9705,
      "step": 500
    },
    {
      "epoch": 1.3043478260869565,
      "grad_norm": 2.974996566772461,
      "learning_rate": 0.0008695652173913044,
      "loss": 0.9672,
      "step": 510
    },
    {
      "epoch": 1.329923273657289,
      "grad_norm": 3.7065961360931396,
      "learning_rate": 0.0008670076726342711,
      "loss": 0.9438,
      "step": 520
    },
    {
      "epoch": 1.3554987212276215,
      "grad_norm": 3.0875167846679688,
      "learning_rate": 0.0008644501278772379,
      "loss": 0.9346,
      "step": 530
    },
    {
      "epoch": 1.381074168797954,
      "grad_norm": 4.697050094604492,
      "learning_rate": 0.0008618925831202045,
      "loss": 0.9311,
      "step": 540
    },
    {
      "epoch": 1.4066496163682864,
      "grad_norm": 3.054164171218872,
      "learning_rate": 0.0008593350383631714,
      "loss": 0.9316,
      "step": 550
    },
    {
      "epoch": 1.432225063938619,
      "grad_norm": 3.433302164077759,
      "learning_rate": 0.0008567774936061381,
      "loss": 0.9933,
      "step": 560
    },
    {
      "epoch": 1.4578005115089514,
      "grad_norm": 3.011810064315796,
      "learning_rate": 0.0008542199488491049,
      "loss": 0.9616,
      "step": 570
    },
    {
      "epoch": 1.4833759590792839,
      "grad_norm": 2.430619955062866,
      "learning_rate": 0.0008516624040920716,
      "loss": 0.9683,
      "step": 580
    },
    {
      "epoch": 1.5089514066496164,
      "grad_norm": 2.5590744018554688,
      "learning_rate": 0.0008491048593350383,
      "loss": 0.9683,
      "step": 590
    },
    {
      "epoch": 1.5345268542199488,
      "grad_norm": 2.934986114501953,
      "learning_rate": 0.0008465473145780051,
      "loss": 0.9654,
      "step": 600
    },
    {
      "epoch": 1.5601023017902813,
      "grad_norm": 2.549093246459961,
      "learning_rate": 0.0008439897698209719,
      "loss": 0.8943,
      "step": 610
    },
    {
      "epoch": 1.5856777493606138,
      "grad_norm": 2.877986431121826,
      "learning_rate": 0.0008414322250639387,
      "loss": 0.8854,
      "step": 620
    },
    {
      "epoch": 1.6112531969309463,
      "grad_norm": 2.7243399620056152,
      "learning_rate": 0.0008388746803069054,
      "loss": 1.0112,
      "step": 630
    },
    {
      "epoch": 1.6368286445012787,
      "grad_norm": 2.4620625972747803,
      "learning_rate": 0.000836317135549872,
      "loss": 0.9385,
      "step": 640
    },
    {
      "epoch": 1.6624040920716112,
      "grad_norm": 2.871335029602051,
      "learning_rate": 0.0008337595907928389,
      "loss": 0.9123,
      "step": 650
    },
    {
      "epoch": 1.6879795396419437,
      "grad_norm": 2.810047149658203,
      "learning_rate": 0.0008312020460358057,
      "loss": 0.9387,
      "step": 660
    },
    {
      "epoch": 1.7135549872122762,
      "grad_norm": 2.6671440601348877,
      "learning_rate": 0.0008286445012787724,
      "loss": 0.9715,
      "step": 670
    },
    {
      "epoch": 1.7391304347826086,
      "grad_norm": 2.8162333965301514,
      "learning_rate": 0.0008260869565217392,
      "loss": 0.8909,
      "step": 680
    },
    {
      "epoch": 1.7647058823529411,
      "grad_norm": 3.6977391242980957,
      "learning_rate": 0.0008235294117647058,
      "loss": 0.9094,
      "step": 690
    },
    {
      "epoch": 1.7902813299232738,
      "grad_norm": 1.9946739673614502,
      "learning_rate": 0.0008209718670076727,
      "loss": 0.8202,
      "step": 700
    },
    {
      "epoch": 1.815856777493606,
      "grad_norm": 5.790404796600342,
      "learning_rate": 0.0008184143222506394,
      "loss": 0.9057,
      "step": 710
    },
    {
      "epoch": 1.8414322250639388,
      "grad_norm": 2.515369176864624,
      "learning_rate": 0.0008158567774936062,
      "loss": 0.8924,
      "step": 720
    },
    {
      "epoch": 1.867007672634271,
      "grad_norm": 3.3826849460601807,
      "learning_rate": 0.0008132992327365729,
      "loss": 0.8858,
      "step": 730
    },
    {
      "epoch": 1.8925831202046037,
      "grad_norm": 3.340932846069336,
      "learning_rate": 0.0008107416879795396,
      "loss": 0.8784,
      "step": 740
    },
    {
      "epoch": 1.918158567774936,
      "grad_norm": 3.1425461769104004,
      "learning_rate": 0.0008081841432225064,
      "loss": 0.8729,
      "step": 750
    },
    {
      "epoch": 1.9437340153452687,
      "grad_norm": 4.056018829345703,
      "learning_rate": 0.0008056265984654732,
      "loss": 0.8608,
      "step": 760
    },
    {
      "epoch": 1.969309462915601,
      "grad_norm": 2.536066770553589,
      "learning_rate": 0.0008030690537084399,
      "loss": 0.9198,
      "step": 770
    },
    {
      "epoch": 1.9948849104859336,
      "grad_norm": 3.501835584640503,
      "learning_rate": 0.0008005115089514067,
      "loss": 0.8837,
      "step": 780
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.6684,
      "eval_loss": 0.9555377960205078,
      "eval_runtime": 5.426,
      "eval_samples_per_second": 1842.977,
      "eval_steps_per_second": 14.56,
      "step": 782
    },
    {
      "epoch": 2.020460358056266,
      "grad_norm": 3.667816400527954,
      "learning_rate": 0.0007979539641943733,
      "loss": 0.8098,
      "step": 790
    },
    {
      "epoch": 2.0460358056265986,
      "grad_norm": 2.995882272720337,
      "learning_rate": 0.0007953964194373402,
      "loss": 0.831,
      "step": 800
    },
    {
      "epoch": 2.071611253196931,
      "grad_norm": 2.929013967514038,
      "learning_rate": 0.0007928388746803069,
      "loss": 0.7365,
      "step": 810
    },
    {
      "epoch": 2.0971867007672635,
      "grad_norm": 3.3931901454925537,
      "learning_rate": 0.0007902813299232737,
      "loss": 0.8036,
      "step": 820
    },
    {
      "epoch": 2.122762148337596,
      "grad_norm": 3.471372604370117,
      "learning_rate": 0.0007877237851662404,
      "loss": 0.8213,
      "step": 830
    },
    {
      "epoch": 2.1483375959079285,
      "grad_norm": 3.4239537715911865,
      "learning_rate": 0.0007851662404092071,
      "loss": 0.7979,
      "step": 840
    },
    {
      "epoch": 2.1739130434782608,
      "grad_norm": 2.256803512573242,
      "learning_rate": 0.000782608695652174,
      "loss": 0.7745,
      "step": 850
    },
    {
      "epoch": 2.1994884910485935,
      "grad_norm": 3.219498872756958,
      "learning_rate": 0.0007800511508951407,
      "loss": 0.8004,
      "step": 860
    },
    {
      "epoch": 2.2250639386189257,
      "grad_norm": 2.5375983715057373,
      "learning_rate": 0.0007774936061381073,
      "loss": 0.7752,
      "step": 870
    },
    {
      "epoch": 2.2506393861892584,
      "grad_norm": 2.6708076000213623,
      "learning_rate": 0.0007749360613810742,
      "loss": 0.737,
      "step": 880
    },
    {
      "epoch": 2.2762148337595907,
      "grad_norm": 2.455359935760498,
      "learning_rate": 0.0007723785166240409,
      "loss": 0.6984,
      "step": 890
    },
    {
      "epoch": 2.3017902813299234,
      "grad_norm": 3.8465638160705566,
      "learning_rate": 0.0007698209718670077,
      "loss": 0.7728,
      "step": 900
    },
    {
      "epoch": 2.3273657289002556,
      "grad_norm": 3.5830602645874023,
      "learning_rate": 0.0007672634271099745,
      "loss": 0.7044,
      "step": 910
    },
    {
      "epoch": 2.3529411764705883,
      "grad_norm": 2.8445119857788086,
      "learning_rate": 0.0007647058823529411,
      "loss": 0.7991,
      "step": 920
    },
    {
      "epoch": 2.3785166240409206,
      "grad_norm": 2.5809779167175293,
      "learning_rate": 0.000762148337595908,
      "loss": 0.7518,
      "step": 930
    },
    {
      "epoch": 2.4040920716112533,
      "grad_norm": 2.935708999633789,
      "learning_rate": 0.0007595907928388746,
      "loss": 0.8062,
      "step": 940
    },
    {
      "epoch": 2.4296675191815855,
      "grad_norm": 2.6519179344177246,
      "learning_rate": 0.0007570332480818415,
      "loss": 0.7651,
      "step": 950
    },
    {
      "epoch": 2.455242966751918,
      "grad_norm": 3.164443016052246,
      "learning_rate": 0.0007544757033248082,
      "loss": 0.7522,
      "step": 960
    },
    {
      "epoch": 2.4808184143222505,
      "grad_norm": 2.26080322265625,
      "learning_rate": 0.0007519181585677749,
      "loss": 0.6898,
      "step": 970
    },
    {
      "epoch": 2.506393861892583,
      "grad_norm": 2.3277060985565186,
      "learning_rate": 0.0007493606138107417,
      "loss": 0.7897,
      "step": 980
    },
    {
      "epoch": 2.531969309462916,
      "grad_norm": 3.304030179977417,
      "learning_rate": 0.0007468030690537084,
      "loss": 0.7553,
      "step": 990
    },
    {
      "epoch": 2.557544757033248,
      "grad_norm": 3.5405731201171875,
      "learning_rate": 0.0007442455242966752,
      "loss": 0.7904,
      "step": 1000
    },
    {
      "epoch": 2.5831202046035804,
      "grad_norm": 3.2186787128448486,
      "learning_rate": 0.000741687979539642,
      "loss": 0.7498,
      "step": 1010
    },
    {
      "epoch": 2.608695652173913,
      "grad_norm": 4.007926940917969,
      "learning_rate": 0.0007391304347826086,
      "loss": 0.7168,
      "step": 1020
    },
    {
      "epoch": 2.634271099744246,
      "grad_norm": 3.2454631328582764,
      "learning_rate": 0.0007365728900255755,
      "loss": 0.8082,
      "step": 1030
    },
    {
      "epoch": 2.659846547314578,
      "grad_norm": 3.2967355251312256,
      "learning_rate": 0.0007340153452685422,
      "loss": 0.8149,
      "step": 1040
    },
    {
      "epoch": 2.6854219948849103,
      "grad_norm": 2.959609270095825,
      "learning_rate": 0.000731457800511509,
      "loss": 0.7527,
      "step": 1050
    },
    {
      "epoch": 2.710997442455243,
      "grad_norm": 3.074732542037964,
      "learning_rate": 0.0007289002557544757,
      "loss": 0.7847,
      "step": 1060
    },
    {
      "epoch": 2.7365728900255757,
      "grad_norm": 3.048021078109741,
      "learning_rate": 0.0007263427109974424,
      "loss": 0.7496,
      "step": 1070
    },
    {
      "epoch": 2.762148337595908,
      "grad_norm": 3.4020895957946777,
      "learning_rate": 0.0007237851662404093,
      "loss": 0.7552,
      "step": 1080
    },
    {
      "epoch": 2.78772378516624,
      "grad_norm": 3.2676000595092773,
      "learning_rate": 0.000721227621483376,
      "loss": 0.8071,
      "step": 1090
    },
    {
      "epoch": 2.813299232736573,
      "grad_norm": 2.122976779937744,
      "learning_rate": 0.0007186700767263428,
      "loss": 0.7191,
      "step": 1100
    },
    {
      "epoch": 2.8388746803069056,
      "grad_norm": 2.500875949859619,
      "learning_rate": 0.0007161125319693095,
      "loss": 0.7231,
      "step": 1110
    },
    {
      "epoch": 2.864450127877238,
      "grad_norm": 2.7149910926818848,
      "learning_rate": 0.0007135549872122762,
      "loss": 0.7552,
      "step": 1120
    },
    {
      "epoch": 2.89002557544757,
      "grad_norm": 3.253019094467163,
      "learning_rate": 0.000710997442455243,
      "loss": 0.7377,
      "step": 1130
    },
    {
      "epoch": 2.915601023017903,
      "grad_norm": 4.490123271942139,
      "learning_rate": 0.0007084398976982098,
      "loss": 0.7108,
      "step": 1140
    },
    {
      "epoch": 2.9411764705882355,
      "grad_norm": 3.6895670890808105,
      "learning_rate": 0.0007058823529411765,
      "loss": 0.7647,
      "step": 1150
    },
    {
      "epoch": 2.9667519181585678,
      "grad_norm": 3.415786027908325,
      "learning_rate": 0.0007033248081841433,
      "loss": 0.7193,
      "step": 1160
    },
    {
      "epoch": 2.9923273657289,
      "grad_norm": 2.314748764038086,
      "learning_rate": 0.0007007672634271099,
      "loss": 0.7222,
      "step": 1170
    },
    {
      "epoch": 3.0,
      "eval_accuracy": 0.7092,
      "eval_loss": 0.8540481328964233,
      "eval_runtime": 5.5515,
      "eval_samples_per_second": 1801.312,
      "eval_steps_per_second": 14.23,
      "step": 1173
    },
    {
      "epoch": 3.0179028132992327,
      "grad_norm": 2.7944998741149902,
      "learning_rate": 0.0006982097186700768,
      "loss": 0.6487,
      "step": 1180
    },
    {
      "epoch": 3.0434782608695654,
      "grad_norm": 3.645268440246582,
      "learning_rate": 0.0006956521739130435,
      "loss": 0.674,
      "step": 1190
    },
    {
      "epoch": 3.0690537084398977,
      "grad_norm": 1.9344900846481323,
      "learning_rate": 0.0006930946291560103,
      "loss": 0.6251,
      "step": 1200
    },
    {
      "epoch": 3.0946291560102304,
      "grad_norm": 2.4395675659179688,
      "learning_rate": 0.000690537084398977,
      "loss": 0.6354,
      "step": 1210
    },
    {
      "epoch": 3.1202046035805626,
      "grad_norm": 3.500967025756836,
      "learning_rate": 0.0006879795396419437,
      "loss": 0.5455,
      "step": 1220
    },
    {
      "epoch": 3.1457800511508953,
      "grad_norm": 2.577641248703003,
      "learning_rate": 0.0006854219948849105,
      "loss": 0.6164,
      "step": 1230
    },
    {
      "epoch": 3.1713554987212276,
      "grad_norm": 2.541400909423828,
      "learning_rate": 0.0006828644501278773,
      "loss": 0.6183,
      "step": 1240
    },
    {
      "epoch": 3.1969309462915603,
      "grad_norm": 3.3994874954223633,
      "learning_rate": 0.000680306905370844,
      "loss": 0.68,
      "step": 1250
    },
    {
      "epoch": 3.2225063938618925,
      "grad_norm": 3.2761268615722656,
      "learning_rate": 0.0006777493606138108,
      "loss": 0.6222,
      "step": 1260
    },
    {
      "epoch": 3.2480818414322252,
      "grad_norm": 2.943943977355957,
      "learning_rate": 0.0006751918158567774,
      "loss": 0.5614,
      "step": 1270
    },
    {
      "epoch": 3.2736572890025575,
      "grad_norm": 2.812709093093872,
      "learning_rate": 0.0006726342710997443,
      "loss": 0.6413,
      "step": 1280
    },
    {
      "epoch": 3.29923273657289,
      "grad_norm": 3.419133424758911,
      "learning_rate": 0.000670076726342711,
      "loss": 0.6878,
      "step": 1290
    },
    {
      "epoch": 3.3248081841432224,
      "grad_norm": 2.607937812805176,
      "learning_rate": 0.0006675191815856778,
      "loss": 0.6291,
      "step": 1300
    },
    {
      "epoch": 3.350383631713555,
      "grad_norm": 2.4234325885772705,
      "learning_rate": 0.0006649616368286446,
      "loss": 0.5805,
      "step": 1310
    },
    {
      "epoch": 3.3759590792838874,
      "grad_norm": 2.5889556407928467,
      "learning_rate": 0.0006624040920716112,
      "loss": 0.6176,
      "step": 1320
    },
    {
      "epoch": 3.40153452685422,
      "grad_norm": 3.451478958129883,
      "learning_rate": 0.0006598465473145781,
      "loss": 0.6016,
      "step": 1330
    },
    {
      "epoch": 3.4271099744245523,
      "grad_norm": 2.823808193206787,
      "learning_rate": 0.0006572890025575448,
      "loss": 0.6003,
      "step": 1340
    },
    {
      "epoch": 3.452685421994885,
      "grad_norm": 3.814810037612915,
      "learning_rate": 0.0006547314578005116,
      "loss": 0.67,
      "step": 1350
    },
    {
      "epoch": 3.4782608695652173,
      "grad_norm": 2.2832412719726562,
      "learning_rate": 0.0006521739130434783,
      "loss": 0.6493,
      "step": 1360
    },
    {
      "epoch": 3.50383631713555,
      "grad_norm": 2.4036266803741455,
      "learning_rate": 0.000649616368286445,
      "loss": 0.634,
      "step": 1370
    },
    {
      "epoch": 3.5294117647058822,
      "grad_norm": 2.3175525665283203,
      "learning_rate": 0.0006470588235294118,
      "loss": 0.6162,
      "step": 1380
    },
    {
      "epoch": 3.554987212276215,
      "grad_norm": 2.804642677307129,
      "learning_rate": 0.0006445012787723786,
      "loss": 0.5812,
      "step": 1390
    },
    {
      "epoch": 3.580562659846547,
      "grad_norm": 2.732877254486084,
      "learning_rate": 0.0006419437340153452,
      "loss": 0.566,
      "step": 1400
    },
    {
      "epoch": 3.60613810741688,
      "grad_norm": 3.0977158546447754,
      "learning_rate": 0.0006393861892583121,
      "loss": 0.646,
      "step": 1410
    },
    {
      "epoch": 3.631713554987212,
      "grad_norm": 2.758206605911255,
      "learning_rate": 0.0006368286445012787,
      "loss": 0.601,
      "step": 1420
    },
    {
      "epoch": 3.657289002557545,
      "grad_norm": 2.6316449642181396,
      "learning_rate": 0.0006342710997442456,
      "loss": 0.62,
      "step": 1430
    },
    {
      "epoch": 3.682864450127877,
      "grad_norm": 3.1904144287109375,
      "learning_rate": 0.0006317135549872123,
      "loss": 0.6403,
      "step": 1440
    },
    {
      "epoch": 3.70843989769821,
      "grad_norm": 3.540193796157837,
      "learning_rate": 0.000629156010230179,
      "loss": 0.6164,
      "step": 1450
    },
    {
      "epoch": 3.734015345268542,
      "grad_norm": 2.449739933013916,
      "learning_rate": 0.0006265984654731458,
      "loss": 0.6271,
      "step": 1460
    },
    {
      "epoch": 3.7595907928388748,
      "grad_norm": 2.289689540863037,
      "learning_rate": 0.0006240409207161125,
      "loss": 0.5775,
      "step": 1470
    },
    {
      "epoch": 3.785166240409207,
      "grad_norm": 2.0853137969970703,
      "learning_rate": 0.0006214833759590793,
      "loss": 0.6084,
      "step": 1480
    },
    {
      "epoch": 3.8107416879795397,
      "grad_norm": 2.6498939990997314,
      "learning_rate": 0.0006189258312020461,
      "loss": 0.5961,
      "step": 1490
    },
    {
      "epoch": 3.836317135549872,
      "grad_norm": 3.225137233734131,
      "learning_rate": 0.0006163682864450127,
      "loss": 0.6312,
      "step": 1500
    },
    {
      "epoch": 3.8618925831202047,
      "grad_norm": 3.1018309593200684,
      "learning_rate": 0.0006138107416879796,
      "loss": 0.6424,
      "step": 1510
    },
    {
      "epoch": 3.887468030690537,
      "grad_norm": 3.214012622833252,
      "learning_rate": 0.0006112531969309462,
      "loss": 0.6189,
      "step": 1520
    },
    {
      "epoch": 3.9130434782608696,
      "grad_norm": 3.1749038696289062,
      "learning_rate": 0.0006086956521739131,
      "loss": 0.5925,
      "step": 1530
    },
    {
      "epoch": 3.938618925831202,
      "grad_norm": 3.513479232788086,
      "learning_rate": 0.0006061381074168799,
      "loss": 0.5865,
      "step": 1540
    },
    {
      "epoch": 3.9641943734015346,
      "grad_norm": 2.7305870056152344,
      "learning_rate": 0.0006035805626598465,
      "loss": 0.6229,
      "step": 1550
    },
    {
      "epoch": 3.9897698209718673,
      "grad_norm": 2.7540552616119385,
      "learning_rate": 0.0006010230179028134,
      "loss": 0.6004,
      "step": 1560
    },
    {
      "epoch": 4.0,
      "eval_accuracy": 0.741,
      "eval_loss": 0.764710545539856,
      "eval_runtime": 5.2063,
      "eval_samples_per_second": 1920.766,
      "eval_steps_per_second": 15.174,
      "step": 1564
    },
    {
      "epoch": 4.015345268542199,
      "grad_norm": 2.4424779415130615,
      "learning_rate": 0.00059846547314578,
      "loss": 0.5476,
      "step": 1570
    },
    {
      "epoch": 4.040920716112532,
      "grad_norm": 2.7432146072387695,
      "learning_rate": 0.0005959079283887469,
      "loss": 0.473,
      "step": 1580
    },
    {
      "epoch": 4.0664961636828645,
      "grad_norm": 2.664935350418091,
      "learning_rate": 0.0005933503836317136,
      "loss": 0.4558,
      "step": 1590
    },
    {
      "epoch": 4.092071611253197,
      "grad_norm": 2.7306127548217773,
      "learning_rate": 0.0005907928388746803,
      "loss": 0.4683,
      "step": 1600
    },
    {
      "epoch": 4.117647058823529,
      "grad_norm": 2.502739906311035,
      "learning_rate": 0.0005882352941176471,
      "loss": 0.4687,
      "step": 1610
    },
    {
      "epoch": 4.143222506393862,
      "grad_norm": 2.326246976852417,
      "learning_rate": 0.0005856777493606138,
      "loss": 0.4967,
      "step": 1620
    },
    {
      "epoch": 4.168797953964194,
      "grad_norm": 3.030733823776245,
      "learning_rate": 0.0005831202046035806,
      "loss": 0.4992,
      "step": 1630
    },
    {
      "epoch": 4.194373401534527,
      "grad_norm": 2.7898337841033936,
      "learning_rate": 0.0005805626598465474,
      "loss": 0.463,
      "step": 1640
    },
    {
      "epoch": 4.21994884910486,
      "grad_norm": 3.333035945892334,
      "learning_rate": 0.000578005115089514,
      "loss": 0.4752,
      "step": 1650
    },
    {
      "epoch": 4.245524296675192,
      "grad_norm": 3.449336290359497,
      "learning_rate": 0.0005754475703324809,
      "loss": 0.4374,
      "step": 1660
    },
    {
      "epoch": 4.271099744245524,
      "grad_norm": 3.0063796043395996,
      "learning_rate": 0.0005728900255754475,
      "loss": 0.4729,
      "step": 1670
    },
    {
      "epoch": 4.296675191815857,
      "grad_norm": 3.228407621383667,
      "learning_rate": 0.0005703324808184144,
      "loss": 0.4631,
      "step": 1680
    },
    {
      "epoch": 4.322250639386189,
      "grad_norm": 4.984773635864258,
      "learning_rate": 0.0005677749360613811,
      "loss": 0.5134,
      "step": 1690
    },
    {
      "epoch": 4.3478260869565215,
      "grad_norm": 2.2980196475982666,
      "learning_rate": 0.0005652173913043478,
      "loss": 0.4709,
      "step": 1700
    },
    {
      "epoch": 4.373401534526854,
      "grad_norm": 3.167473793029785,
      "learning_rate": 0.0005626598465473146,
      "loss": 0.5095,
      "step": 1710
    },
    {
      "epoch": 4.398976982097187,
      "grad_norm": 2.9786345958709717,
      "learning_rate": 0.0005601023017902813,
      "loss": 0.487,
      "step": 1720
    },
    {
      "epoch": 4.42455242966752,
      "grad_norm": 2.7108163833618164,
      "learning_rate": 0.0005575447570332481,
      "loss": 0.4698,
      "step": 1730
    },
    {
      "epoch": 4.450127877237851,
      "grad_norm": 3.8536171913146973,
      "learning_rate": 0.0005549872122762149,
      "loss": 0.4961,
      "step": 1740
    },
    {
      "epoch": 4.475703324808184,
      "grad_norm": 3.3741374015808105,
      "learning_rate": 0.0005524296675191815,
      "loss": 0.4848,
      "step": 1750
    },
    {
      "epoch": 4.501278772378517,
      "grad_norm": 3.1886463165283203,
      "learning_rate": 0.0005498721227621484,
      "loss": 0.4698,
      "step": 1760
    },
    {
      "epoch": 4.526854219948849,
      "grad_norm": 2.2157187461853027,
      "learning_rate": 0.000547314578005115,
      "loss": 0.4845,
      "step": 1770
    },
    {
      "epoch": 4.552429667519181,
      "grad_norm": 3.1560168266296387,
      "learning_rate": 0.0005447570332480819,
      "loss": 0.5303,
      "step": 1780
    },
    {
      "epoch": 4.578005115089514,
      "grad_norm": 2.7105860710144043,
      "learning_rate": 0.0005421994884910487,
      "loss": 0.4813,
      "step": 1790
    },
    {
      "epoch": 4.603580562659847,
      "grad_norm": 3.074141263961792,
      "learning_rate": 0.0005396419437340153,
      "loss": 0.4897,
      "step": 1800
    },
    {
      "epoch": 4.629156010230179,
      "grad_norm": 3.04917573928833,
      "learning_rate": 0.0005370843989769822,
      "loss": 0.4772,
      "step": 1810
    },
    {
      "epoch": 4.654731457800511,
      "grad_norm": 2.6889312267303467,
      "learning_rate": 0.0005345268542199488,
      "loss": 0.4951,
      "step": 1820
    },
    {
      "epoch": 4.680306905370844,
      "grad_norm": 2.7175233364105225,
      "learning_rate": 0.0005319693094629157,
      "loss": 0.4494,
      "step": 1830
    },
    {
      "epoch": 4.705882352941177,
      "grad_norm": 3.478726625442505,
      "learning_rate": 0.0005294117647058824,
      "loss": 0.483,
      "step": 1840
    },
    {
      "epoch": 4.731457800511509,
      "grad_norm": 3.035256862640381,
      "learning_rate": 0.0005268542199488491,
      "loss": 0.4311,
      "step": 1850
    },
    {
      "epoch": 4.757033248081841,
      "grad_norm": 3.351569652557373,
      "learning_rate": 0.0005242966751918159,
      "loss": 0.5088,
      "step": 1860
    },
    {
      "epoch": 4.782608695652174,
      "grad_norm": 5.402308940887451,
      "learning_rate": 0.0005217391304347826,
      "loss": 0.5031,
      "step": 1870
    },
    {
      "epoch": 4.8081841432225065,
      "grad_norm": 3.070901870727539,
      "learning_rate": 0.0005191815856777494,
      "loss": 0.493,
      "step": 1880
    },
    {
      "epoch": 4.833759590792839,
      "grad_norm": 3.500988483428955,
      "learning_rate": 0.0005166240409207162,
      "loss": 0.4918,
      "step": 1890
    },
    {
      "epoch": 4.859335038363171,
      "grad_norm": 3.6459267139434814,
      "learning_rate": 0.0005140664961636828,
      "loss": 0.4907,
      "step": 1900
    },
    {
      "epoch": 4.884910485933504,
      "grad_norm": 3.0559608936309814,
      "learning_rate": 0.0005115089514066497,
      "loss": 0.5057,
      "step": 1910
    },
    {
      "epoch": 4.910485933503836,
      "grad_norm": 1.9356663227081299,
      "learning_rate": 0.0005089514066496163,
      "loss": 0.4848,
      "step": 1920
    },
    {
      "epoch": 4.936061381074169,
      "grad_norm": 2.79671049118042,
      "learning_rate": 0.0005063938618925832,
      "loss": 0.4838,
      "step": 1930
    },
    {
      "epoch": 4.961636828644501,
      "grad_norm": 2.4194259643554688,
      "learning_rate": 0.0005038363171355499,
      "loss": 0.5328,
      "step": 1940
    },
    {
      "epoch": 4.987212276214834,
      "grad_norm": 2.9354443550109863,
      "learning_rate": 0.0005012787723785166,
      "loss": 0.5142,
      "step": 1950
    },
    {
      "epoch": 5.0,
      "eval_accuracy": 0.7512,
      "eval_loss": 0.7458837628364563,
      "eval_runtime": 5.2947,
      "eval_samples_per_second": 1888.686,
      "eval_steps_per_second": 14.921,
      "step": 1955
    },
    {
      "epoch": 5.012787723785166,
      "grad_norm": 2.0400850772857666,
      "learning_rate": 0.0004987212276214833,
      "loss": 0.3838,
      "step": 1960
    },
    {
      "epoch": 5.038363171355499,
      "grad_norm": 2.200627088546753,
      "learning_rate": 0.0004961636828644501,
      "loss": 0.3084,
      "step": 1970
    },
    {
      "epoch": 5.063938618925831,
      "grad_norm": 2.4857890605926514,
      "learning_rate": 0.0004936061381074169,
      "loss": 0.3423,
      "step": 1980
    },
    {
      "epoch": 5.089514066496164,
      "grad_norm": 2.435049533843994,
      "learning_rate": 0.0004910485933503837,
      "loss": 0.3557,
      "step": 1990
    },
    {
      "epoch": 5.115089514066496,
      "grad_norm": 2.0620884895324707,
      "learning_rate": 0.0004884910485933504,
      "loss": 0.3134,
      "step": 2000
    },
    {
      "epoch": 5.140664961636829,
      "grad_norm": 2.906205654144287,
      "learning_rate": 0.00048593350383631715,
      "loss": 0.3516,
      "step": 2010
    },
    {
      "epoch": 5.166240409207161,
      "grad_norm": 1.8201605081558228,
      "learning_rate": 0.0004833759590792839,
      "loss": 0.3089,
      "step": 2020
    },
    {
      "epoch": 5.1918158567774935,
      "grad_norm": 3.0602452754974365,
      "learning_rate": 0.00048081841432225065,
      "loss": 0.3356,
      "step": 2030
    },
    {
      "epoch": 5.217391304347826,
      "grad_norm": 3.7379448413848877,
      "learning_rate": 0.0004782608695652174,
      "loss": 0.3736,
      "step": 2040
    },
    {
      "epoch": 5.242966751918159,
      "grad_norm": 2.7852628231048584,
      "learning_rate": 0.00047570332480818416,
      "loss": 0.3529,
      "step": 2050
    },
    {
      "epoch": 5.268542199488491,
      "grad_norm": 2.98350191116333,
      "learning_rate": 0.0004731457800511509,
      "loss": 0.3374,
      "step": 2060
    },
    {
      "epoch": 5.294117647058823,
      "grad_norm": 2.4773707389831543,
      "learning_rate": 0.00047058823529411766,
      "loss": 0.3666,
      "step": 2070
    },
    {
      "epoch": 5.319693094629156,
      "grad_norm": 3.3636295795440674,
      "learning_rate": 0.0004680306905370844,
      "loss": 0.3429,
      "step": 2080
    },
    {
      "epoch": 5.345268542199489,
      "grad_norm": 3.0117969512939453,
      "learning_rate": 0.00046547314578005116,
      "loss": 0.4135,
      "step": 2090
    },
    {
      "epoch": 5.370843989769821,
      "grad_norm": 3.057176113128662,
      "learning_rate": 0.00046291560102301786,
      "loss": 0.3415,
      "step": 2100
    },
    {
      "epoch": 5.396419437340153,
      "grad_norm": 3.554422378540039,
      "learning_rate": 0.00046035805626598467,
      "loss": 0.3992,
      "step": 2110
    },
    {
      "epoch": 5.421994884910486,
      "grad_norm": 3.90144681930542,
      "learning_rate": 0.0004578005115089514,
      "loss": 0.3727,
      "step": 2120
    },
    {
      "epoch": 5.447570332480819,
      "grad_norm": 2.7290210723876953,
      "learning_rate": 0.0004552429667519182,
      "loss": 0.3994,
      "step": 2130
    },
    {
      "epoch": 5.4731457800511505,
      "grad_norm": 2.040438413619995,
      "learning_rate": 0.0004526854219948849,
      "loss": 0.3568,
      "step": 2140
    },
    {
      "epoch": 5.498721227621483,
      "grad_norm": 2.3657846450805664,
      "learning_rate": 0.0004501278772378516,
      "loss": 0.3667,
      "step": 2150
    },
    {
      "epoch": 5.524296675191816,
      "grad_norm": 2.5565638542175293,
      "learning_rate": 0.00044757033248081843,
      "loss": 0.3761,
      "step": 2160
    },
    {
      "epoch": 5.549872122762149,
      "grad_norm": 2.586674690246582,
      "learning_rate": 0.0004450127877237852,
      "loss": 0.3648,
      "step": 2170
    },
    {
      "epoch": 5.57544757033248,
      "grad_norm": 2.0705809593200684,
      "learning_rate": 0.00044245524296675193,
      "loss": 0.3543,
      "step": 2180
    },
    {
      "epoch": 5.601023017902813,
      "grad_norm": 3.16296124458313,
      "learning_rate": 0.0004398976982097187,
      "loss": 0.3831,
      "step": 2190
    },
    {
      "epoch": 5.626598465473146,
      "grad_norm": 3.666292190551758,
      "learning_rate": 0.0004373401534526854,
      "loss": 0.3628,
      "step": 2200
    },
    {
      "epoch": 5.6521739130434785,
      "grad_norm": 3.40330171585083,
      "learning_rate": 0.0004347826086956522,
      "loss": 0.3791,
      "step": 2210
    },
    {
      "epoch": 5.677749360613811,
      "grad_norm": 3.0124151706695557,
      "learning_rate": 0.00043222506393861894,
      "loss": 0.3435,
      "step": 2220
    },
    {
      "epoch": 5.703324808184143,
      "grad_norm": 2.614565849304199,
      "learning_rate": 0.0004296675191815857,
      "loss": 0.3497,
      "step": 2230
    },
    {
      "epoch": 5.728900255754476,
      "grad_norm": 3.911065101623535,
      "learning_rate": 0.00042710997442455245,
      "loss": 0.425,
      "step": 2240
    },
    {
      "epoch": 5.754475703324808,
      "grad_norm": 2.77262282371521,
      "learning_rate": 0.00042455242966751914,
      "loss": 0.3756,
      "step": 2250
    },
    {
      "epoch": 5.78005115089514,
      "grad_norm": 2.476276397705078,
      "learning_rate": 0.00042199488491048595,
      "loss": 0.3138,
      "step": 2260
    },
    {
      "epoch": 5.805626598465473,
      "grad_norm": 2.332343578338623,
      "learning_rate": 0.0004194373401534527,
      "loss": 0.3745,
      "step": 2270
    },
    {
      "epoch": 5.831202046035806,
      "grad_norm": 2.6068968772888184,
      "learning_rate": 0.00041687979539641946,
      "loss": 0.3869,
      "step": 2280
    },
    {
      "epoch": 5.856777493606138,
      "grad_norm": 2.3609273433685303,
      "learning_rate": 0.0004143222506393862,
      "loss": 0.344,
      "step": 2290
    },
    {
      "epoch": 5.882352941176471,
      "grad_norm": 2.6048245429992676,
      "learning_rate": 0.0004117647058823529,
      "loss": 0.3337,
      "step": 2300
    },
    {
      "epoch": 5.907928388746803,
      "grad_norm": 5.043975830078125,
      "learning_rate": 0.0004092071611253197,
      "loss": 0.3621,
      "step": 2310
    },
    {
      "epoch": 5.9335038363171355,
      "grad_norm": 2.3489763736724854,
      "learning_rate": 0.00040664961636828646,
      "loss": 0.3691,
      "step": 2320
    },
    {
      "epoch": 5.959079283887468,
      "grad_norm": 2.7898764610290527,
      "learning_rate": 0.0004040920716112532,
      "loss": 0.3429,
      "step": 2330
    },
    {
      "epoch": 5.9846547314578,
      "grad_norm": 3.2942752838134766,
      "learning_rate": 0.00040153452685421997,
      "loss": 0.3211,
      "step": 2340
    },
    {
      "epoch": 6.0,
      "eval_accuracy": 0.7663,
      "eval_loss": 0.7437919974327087,
      "eval_runtime": 5.8556,
      "eval_samples_per_second": 1707.778,
      "eval_steps_per_second": 13.491,
      "step": 2346
    }
  ],
  "logging_steps": 10,
  "max_steps": 3910,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 0.0,
  "train_batch_size": 128,
  "trial_name": null,
  "trial_params": null
}
