{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# DeepSeek R1 Model Fine-Tuning (LORA) with GPT-4 Dataset [Unsloth and OLLAMA]","metadata":{}},{"cell_type":"markdown","source":"## **Unsloth**\n\nUnsloth appears to be a tool or framework designed for efficient fine-tuning of language models. From the context, it likely incorporates techniques like Low-Rank Adaptation (LoRA) and other efficiency optimizations to fine-tune large models, such as Llama derivatives, with minimal computational resources.\n\nThe key features of Unsloth as implied by your description might include:\n\n1. **Efficient Fine-Tuning**: Instead of updating the entire model's weights, it leverages methods like LoRA, which fine-tune a smaller subset of the parameters, making it resource-efficient.\n\n2. **Simplified Workflow**: The process of loading models, configuring parameters, and training appears streamlined, allowing developers to focus on specific customizations rather than managing complex infrastructure.\n\n3. **Integration with Local Runtimes**: After fine-tuning, exporting to tools like Ollama for local deployment demonstrates its support for practical application.\n\n## **DeepSeek-R1**\n\nThe DeepSeek-R1 is a versatile robot for exploration and inspection in tough environments. With AI, precise sensors, and multi-terrain mobility, it handles tasks like data collection, mapping, and monitoring. Customisable for search and rescue, inspections, or research, it ensures reliable performance in hazardous areas.\n\n![](https://bsmedia.business-standard.com/_media/bs/img/article/2025-01/28/full/1738047877-0145.JPG?im=FeatureCrop,size=(826,465))","metadata":{}},{"cell_type":"code","source":"!pip install unsloth","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Load the Model\nUsing Unsloth, you load the DeepSeek-R1 Distilled Llama-8B model, a smaller, faster version of the Llama model optimised for performance while retaining accuracy.\nAlong with the model, you also load its tokeniser. The tokeniser breaks down input text into smaller units (tokens) that the model can process.AttributeError\n\n## Importance?\nLoading the model and tokenizer is the foundation for fine-tuning since they define how text inputs are processed and predictions are generated.","metadata":{}},{"cell_type":"code","source":"from unsloth import FastLanguageModel\nimport torch\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/DeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit\",\n    max_seq_length = 2048,\n    dtype = None,\n    load_in_4bit = True,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T19:35:22.644139Z","iopub.execute_input":"2025-01-28T19:35:22.644485Z","iopub.status.idle":"2025-01-28T19:36:19.089957Z","shell.execute_reply.started":"2025-01-28T19:35:22.644457Z","shell.execute_reply":"2025-01-28T19:36:19.089148Z"}},"outputs":[{"name":"stdout","text":"ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\nü¶• Unsloth Zoo will now patch everything to make training faster!\n==((====))==  Unsloth 2025.1.7: Fast Llama patching. Transformers: 4.48.1.\n   \\\\   /|    GPU: Tesla T4. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 7.5. CUDA Toolkit: 12.1. Triton: 3.1.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post1. FA2 = False]\n \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/5.96G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4460f8ca77424460a5dfa6c5c7e6de3b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/231 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"458bfef5921f4434ac92d86eff7dfdca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/52.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8bc899fb5734e83a620733073c93b99"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b37d3b95b8b4023b6c40bb372a9b95f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ddb07aacdd3a4c9db170d52e0cbef1bc"}},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"## Setup Parameter-Efficient Fine-Tuning (PEFT) \nThe **FastLanguageModel.get_peft_model** function modifies a pre-trained language model to use PEFT techniques, which allow fine-tuning of the model using fewer resources and parameters.\n\nThe method introduces additional tunable parameters (like LoRA matrices) to specific layers of the model while freezing most of the original model weights.\n\n***In this Project, we use LORA to fine-tune the DeepSeek R1 LLM.***","metadata":{}},{"cell_type":"markdown","source":"First, it takes the existing model. Then, it applies a technique called PEFT (Parameter-Efficient Fine-Tuning).  Think of it as adding small, adjustable \"knobs\" instead of changing the whole engine.\n\nr=4 and lora_alpha=16 are just settings for how many \"knobs\" and how sensitive they are.  target_modules specifies where these knobs are attached ‚Äì specifically, parts of the model that handle questions, keys, values, and outputs.  lora_dropout=0 means no \"knobs\" are randomly turned off during training.\n\nbias=\"none\" means no extra adjustments to the model's biases. use_gradient_checkpointing=\"unsloth\" is a memory-saving trick for training. random_state=42 ensures we get the same results if we run this again.  use_rslora=False and loftq_config=None are more advanced settings that are turned off here.","metadata":{}},{"cell_type":"code","source":"model = FastLanguageModel.get_peft_model(\n    model,\n    r = 4,\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n    lora_alpha = 16,\n    lora_dropout = 0,\n    bias = \"none\",\n    use_gradient_checkpointing = \"unsloth\",\n    random_state = 42,\n    use_rslora = False,\n    loftq_config = None,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T19:37:14.223695Z","iopub.execute_input":"2025-01-28T19:37:14.224138Z","iopub.status.idle":"2025-01-28T19:37:20.051562Z","shell.execute_reply.started":"2025-01-28T19:37:14.224087Z","shell.execute_reply":"2025-01-28T19:37:20.050877Z"}},"outputs":[{"name":"stderr","text":"Not an error, but Unsloth cannot patch MLP layers with our manual autograd engine since either LoRA adapters\nare not enabled or a bias term (like in Qwen) is used.\nUnsloth 2025.1.7 patched 32 layers with 32 QKV layers, 32 O layers and 0 MLP layers.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## Load Dataset","metadata":{}},{"cell_type":"markdown","source":"The vicgalle/alpaca-gpt4 dataset is a collection of 52,000 instruction-following instances designed to fine-tune language models (LLMs). It was created by Vic Galie and is available on Hugging Face.","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\ndataset = load_dataset(\"vicgalle/alpaca-gpt4\", split = \"train\")\nprint(dataset.column_names)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T19:44:43.524338Z","iopub.execute_input":"2025-01-28T19:44:43.524760Z","iopub.status.idle":"2025-01-28T19:44:46.104782Z","shell.execute_reply.started":"2025-01-28T19:44:43.524698Z","shell.execute_reply":"2025-01-28T19:44:46.103935Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/3.38k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4e0e67b97b64e0aa6f56d7fbdcb244c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(‚Ä¶)-00000-of-00001-6ef3991c06080e14.parquet:   0%|          | 0.00/48.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a72a57519be4a38b20e2dd950562584"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/52002 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb0829b5b59446d3920f5ffb2ad4b8e1"}},"metadata":{}},{"name":"stdout","text":"['instruction', 'input', 'output', 'text']\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"dataset[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T19:44:47.830803Z","iopub.execute_input":"2025-01-28T19:44:47.831145Z","iopub.status.idle":"2025-01-28T19:44:47.837464Z","shell.execute_reply.started":"2025-01-28T19:44:47.831121Z","shell.execute_reply":"2025-01-28T19:44:47.836520Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"{'instruction': 'Give three tips for staying healthy.',\n 'input': '',\n 'output': '1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\\n\\n2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\\n\\n3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.',\n 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nGive three tips for staying healthy.\\n\\n### Response:\\n1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\\n\\n2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\\n\\n3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.'}"},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"Let's format the dataset in a way suitable for conversational AI training using the ShareGPT format, which is designed for multi-turn conversations.","metadata":{}},{"cell_type":"code","source":"from unsloth import to_sharegpt\n\ndataset = to_sharegpt(\n    dataset,\n    merged_prompt = \"{instruction}[[\\nYour input is:\\n{input}]]\",\n    output_column_name = \"output\",\n    conversation_extension = 3, # Select more to handle longer conversations\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T19:44:48.368559Z","iopub.execute_input":"2025-01-28T19:44:48.368943Z","iopub.status.idle":"2025-01-28T19:45:07.176530Z","shell.execute_reply.started":"2025-01-28T19:44:48.368914Z","shell.execute_reply":"2025-01-28T19:45:07.175532Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Merging columns:   0%|          | 0/52002 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b164a195125461ab4509d0fbb75e0ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Converting to ShareGPT:   0%|          | 0/52002 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"591337babebc4d11befa0f8ab6ff2020"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Flattening the indices:   0%|          | 0/52002 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"986e0a35cc2d44fa880461763329b7d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Flattening the indices:   0%|          | 0/52002 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ea2b1a830414e2ba6545d39308d8f99"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Flattening the indices:   0%|          | 0/52002 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08e3f52e9b9044aa98a05698aa308959"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extending conversations:   0%|          | 0/52002 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94f34f9193e34d08862762c27d9bcf24"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"from unsloth import standardize_sharegpt\ndataset = standardize_sharegpt(dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T19:45:07.178263Z","iopub.execute_input":"2025-01-28T19:45:07.178493Z","iopub.status.idle":"2025-01-28T19:45:10.130543Z","shell.execute_reply.started":"2025-01-28T19:45:07.178473Z","shell.execute_reply":"2025-01-28T19:45:10.129425Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Standardizing format:   0%|          | 0/52002 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1801307d57054f5c8d34c5f958c44335"}},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"dataset[0]['conversations']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T19:45:10.132675Z","iopub.execute_input":"2025-01-28T19:45:10.132983Z","iopub.status.idle":"2025-01-28T19:45:10.139400Z","shell.execute_reply.started":"2025-01-28T19:45:10.132960Z","shell.execute_reply":"2025-01-28T19:45:10.138464Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"[{'content': 'Give three tips for staying healthy.', 'role': 'user'},\n {'content': '1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\\n\\n2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\\n\\n3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.',\n  'role': 'assistant'},\n {'content': 'Describe what a monotheistic religion is.', 'role': 'user'},\n {'content': 'A monotheistic religion is a type of religion that believes in the existence of only one supreme and all-powerful deity, who is considered the creator and ruler of the universe. This deity is worshiped as the ultimate and only divine being, and followers of such religions often see their deity as omniscient, omnipotent, and omnibenevolent. Some of the most widely practiced monotheistic religions in the world today include Christianity, Islam, and Judaism, among others. The concept of monotheism differs from polytheism, which believes in the existence of multiple gods, and from atheism, which denies the existence of any deity.',\n  'role': 'assistant'},\n {'content': 'How does one add a chart to a document?', 'role': 'user'},\n {'content': 'To add a chart to a document, follow these steps:\\n\\n1. Open the document where you want to insert the chart.\\n2. Click the location where you want to insert the chart.\\n3. In most word processors, you can go to the **Insert** tab, where you can find a **Chart** option. Click on it.\\n4. A new window will appear, and you\\'ll be able to select the chart type that you want to use (column, pie, line, bar, area, scatter, etc.).\\n5. Once you\\'ve selected your chart type, you‚Äôll be prompted to enter your data into a spreadsheet. You can either type your data in manually or copy it from an existing data source.\\n6. Edit your chart data and customize its appearance to fit your document\\'s style or branding.\\n7. When you‚Äôre happy with how the chart looks, click \"OK\" or \"Insert\" to add the chart to your document.\\n8. Optionally, you can add a chart title or labels to the axes to make the chart easier to understand.\\n\\nThese instructions may vary depending on the type of word processor you are using.',\n  'role': 'assistant'}]"},"metadata":{}}],"execution_count":9},{"cell_type":"markdown","source":"## Customizable Chat Templates","metadata":{}},{"cell_type":"markdown","source":"This code takes a dataset, formats it into a chat-friendly format, and then applies a template so that the AI can understand the instructions and responses correctly.","metadata":{}},{"cell_type":"code","source":"chat_template = \"\"\"Below are some instructions that describe some tasks. Write responses that appropriately complete each request.\n\n### Instruction:\n{INPUT}\n\n### Response:\n{OUTPUT}\"\"\"\n\nfrom unsloth import apply_chat_template\ndataset = apply_chat_template(\n    dataset,\n    tokenizer = tokenizer,\n    chat_template = chat_template,\n    # default_system_message = \"You are a helpful assistant\", << [OPTIONAL]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T19:45:10.140594Z","iopub.execute_input":"2025-01-28T19:45:10.140900Z","iopub.status.idle":"2025-01-28T19:45:17.006478Z","shell.execute_reply.started":"2025-01-28T19:45:10.140876Z","shell.execute_reply":"2025-01-28T19:45:17.005447Z"}},"outputs":[{"name":"stderr","text":"Unsloth: We automatically added an EOS token to stop endless generations.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/52002 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aabfc629d1c042b486c72c0e8e8b1a99"}},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"## Train the model","metadata":{}},{"cell_type":"markdown","source":"Let's define and configure a fine-tuning trainer for a language model using the **SFTTrainer** class from the **trl** library (likely for fine-tuning language models) and transformers' **TrainingArguments**","metadata":{}},{"cell_type":"markdown","source":"It uses SFTTrainer from the trl library, which is specifically for Supervised Fine-Tuning.  This means the model will learn from the dataset of instructions and responses you prepared earlier.\n\nIt takes the model (the tuned language model), the tokenizer (for breaking down text), and the dataset. dataset_text_field=\"text\" tells it where the actual text is in the dataset.  max_seq_length=2048 limits the length of text sequences the model processes at once. dataset_num_proc=2 uses two processes to prepare the data, and packing=False disables a specific data packing technique.\n\nThen, it configures the training process with TrainingArguments.  per_device_train_batch_size=2 means each training device (like a GPU) will process 2 examples at a time. gradient_accumulation_steps=4 combines the gradients from 4 batches to simulate a larger batch size. warmup_steps=5 gradually increases the learning rate at the beginning. max_steps=20 limits the total training steps. learning_rate=2e-4 sets the learning rate.  fp16 and bf16 control the precision of calculations (using either half-precision or bfloat16 if supported, for faster training). logging_steps=1 logs training progress every step. optim=\"adamw_8bit\" specifies the optimizer. weight_decay=0.01 is a regularization technique. lr_scheduler_type=\"linear\" sets how the learning rate changes over time. seed=3407 ensures reproducibility. output_dir=\"outputs\" specifies where to save the trained model. report_to=\"none\" disables reporting to services like WandB (Weights & Biases).\n","metadata":{}},{"cell_type":"code","source":"from trl import SFTTrainer\nfrom transformers import TrainingArguments\nfrom unsloth import is_bfloat16_supported\n\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = dataset,\n    dataset_text_field = \"text\",\n    max_seq_length = 2048,\n    dataset_num_proc = 2,\n    packing = False,\n    args = TrainingArguments(\n        per_device_train_batch_size = 2,\n        gradient_accumulation_steps = 4,\n        warmup_steps = 5,\n        max_steps = 20,\n        learning_rate = 2e-4,\n        fp16 = not is_bfloat16_supported(),\n        bf16 = is_bfloat16_supported(),\n        logging_steps = 1,\n        optim = \"adamw_8bit\",\n        weight_decay = 0.01,\n        lr_scheduler_type = \"linear\",\n        seed = 3407,\n        output_dir = \"outputs\",\n        report_to = \"none\", # Use this for WandB etc\n    ),\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T19:45:17.007726Z","iopub.execute_input":"2025-01-28T19:45:17.008092Z","iopub.status.idle":"2025-01-28T19:46:03.412875Z","shell.execute_reply.started":"2025-01-28T19:45:17.008053Z","shell.execute_reply":"2025-01-28T19:46:03.411741Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map (num_proc=2):   0%|          | 0/52002 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"572c95b8b6fa4d789f93d9a096d7f7f4"}},"metadata":{}}],"execution_count":11},{"cell_type":"markdown","source":"The trainer orchestrates the fine-tuning process by combining the model, tokenizer, dataset, and hyperparameters. It ensures efficient training, taking advantage of mixed precision (fp16 or bf16) and memory-efficient optimizations (e.g., AdamW with 8-bit precision). Additionally, it handles sequence preprocessing, gradient accumulation, and learning rate scheduling.","metadata":{}},{"cell_type":"code","source":"trainer_stats = trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T19:46:03.413997Z","iopub.execute_input":"2025-01-28T19:46:03.414275Z","iopub.status.idle":"2025-01-28T19:52:29.397069Z","shell.execute_reply.started":"2025-01-28T19:46:03.414249Z","shell.execute_reply":"2025-01-28T19:52:29.396239Z"}},"outputs":[{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n   \\\\   /|    Num examples = 52,002 | Num Epochs = 1\nO^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n\\        /    Total batch size = 8 | Total steps = 20\n \"-____-\"     Number of trainable parameters = 3,407,872\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [20/20 05:49, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.903300</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.925000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.821300</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>2.015100</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>2.090600</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>1.789000</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>1.760200</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>1.675400</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>1.706200</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>1.535400</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>1.561000</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>1.596300</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>1.349200</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>1.455700</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>1.513300</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>1.451700</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>1.357800</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>1.541500</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>1.413600</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>1.399100</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":12},{"cell_type":"markdown","source":"## Ollama","metadata":{}},{"cell_type":"markdown","source":"OLAMA is a powerful tool that enables you to run large language models (LLMs) directly on your own computer (laptop or desktop).\n","metadata":{}},{"cell_type":"code","source":"!curl -fsSL https://ollama.com/install.sh | sh","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T20:22:33.733430Z","iopub.execute_input":"2025-01-28T20:22:33.733920Z","iopub.status.idle":"2025-01-28T20:23:25.235067Z","shell.execute_reply.started":"2025-01-28T20:22:33.733884Z","shell.execute_reply":"2025-01-28T20:23:25.233937Z"}},"outputs":[{"name":"stdout","text":">>> Installing ollama to /usr/local\n>>> Downloading Linux amd64 bundle\n############################################################################################# 100.0%##                                                                                     11.3%############################                                                         41.9%###################################                                               52.2%1.4%####################################################################             88.3%#########################################################################        94.0%\n>>> Creating ollama user...\n>>> Adding ollama user to video group...\n>>> Adding current user to ollama group...\n>>> Creating ollama systemd service...\n\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n>>> The Ollama API is now available at 127.0.0.1:11434.\n>>> Install complete. Run \"ollama\" from the command line.\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"Let's save this model.","metadata":{}},{"cell_type":"code","source":"model.save_pretrained_gguf(\"model\", tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T20:23:25.236588Z","iopub.execute_input":"2025-01-28T20:23:25.236859Z","iopub.status.idle":"2025-01-28T20:29:56.857808Z","shell.execute_reply.started":"2025-01-28T20:23:25.236835Z","shell.execute_reply":"2025-01-28T20:29:56.856335Z"}},"outputs":[{"name":"stderr","text":"Unsloth: ##### The current model auto adds a BOS token.\nUnsloth: ##### Your chat template has a BOS token. We shall remove it temporarily.\n","output_type":"stream"},{"name":"stdout","text":"Cloning into 'llama.cpp'...\nSubmodule 'kompute' (https://github.com/nomic-ai/kompute.git) registered for path 'ggml/src/ggml-kompute/kompute'\nCloning into '/kaggle/working/llama.cpp/ggml/src/ggml-kompute/kompute'...\nSubmodule path 'ggml/src/ggml-kompute/kompute': checked out '4565194ed7c32d1d2efa32ceab4d3c6cae006306'\nRequirement already satisfied: gguf in /usr/local/lib/python3.10/dist-packages (0.14.0)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (3.20.3)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from gguf) (1.26.4)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from gguf) (6.0.2)\nRequirement already satisfied: sentencepiece<=0.2.0,>=0.1.98 in /usr/local/lib/python3.10/dist-packages (from gguf) (0.2.0)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from gguf) (4.67.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->gguf) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->gguf) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->gguf) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->gguf) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->gguf) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->gguf) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->gguf) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->gguf) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->gguf) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->gguf) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->gguf) (2024.2.0)\nmake: Entering directory '/kaggle/working/llama.cpp'\n-- The C compiler identification is GNU 11.4.0\n-- The CXX compiler identification is GNU 11.4.0\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Check for working C compiler: /usr/bin/cc - skipped\n-- Detecting C compile features\n-- Detecting C compile features - done\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Check for working CXX compiler: /usr/bin/c++ - skipped\n-- Detecting CXX compile features\n-- Detecting CXX compile features - done\n-- Found Git: /usr/bin/git (found version \"2.34.1\")\n-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n-- Found Threads: TRUE\n-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\n-- CMAKE_SYSTEM_PROCESSOR: x86_64\n-- Including CPU backend\n-- Found OpenMP_C: -fopenmp (found version \"4.5\")\n-- Found OpenMP_CXX: -fopenmp (found version \"4.5\")\n-- Found OpenMP: TRUE (found version \"4.5\")\n-- x86 detected\n-- Adding CPU backend variant ggml-cpu: -march=native \n-- Found CURL: /usr/lib/x86_64-linux-gnu/libcurl.so (found version \"7.81.0\")\n-- Configuring done (1.7s)\n-- Generating done (0.3s)\n-- Build files have been written to: /kaggle/working/llama.cpp/build\n[  0%] Generating build details from Git\n[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o\n[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o\n[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o\n[  7%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o\n[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o\n[ 11%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o\n-- Found Git: /usr/bin/git (found version \"2.34.1\")\n[ 15%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o\n[ 19%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o\n[ 19%] Built target build_info\n[ 19%] Linking CXX static library libggml-base.a\n[ 19%] Built target ggml-base\n[ 19%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o\n[ 23%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o\n[ 26%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o\n[ 26%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o\n[ 30%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o\n[ 30%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o\n[ 34%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o\n[ 34%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o\n[ 38%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o\n[ 38%] Linking CXX static library libggml-cpu.a\n[ 38%] Built target ggml-cpu\n[ 42%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o\n[ 42%] Linking CXX static library libggml.a\n[ 42%] Built target ggml\n[ 42%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o\n[ 46%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o\n[ 50%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o\n[ 50%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o\n[ 53%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o\n[ 57%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o\n[ 57%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o\n[ 57%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o\n[ 61%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o\n[ 61%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o\n[ 65%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o\n[ 65%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o\n[ 69%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o\n[ 69%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o\n[ 73%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o\n[ 73%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o\n[ 76%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o\n[ 76%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o\n[ 80%] Linking CXX static library libllama.a\n[ 80%] Built target llama\n[ 84%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o\n[ 84%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o\n[ 84%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o\n[ 88%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o\n[ 92%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o\n[ 92%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o\n[ 92%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o\n[ 96%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o\n[ 96%] Linking CXX static library libcommon.a\n[ 96%] Built target common\n[ 96%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o\n[100%] Linking CXX executable ../../bin/llama-quantize\n[100%] Built target llama-quantize\n[  3%] Built target build_info\n[ 19%] Built target ggml-base\n[ 38%] Built target ggml-cpu\n[ 42%] Built target ggml\n[ 80%] Built target llama\n[ 96%] Built target common\n[ 96%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o\n[100%] Linking CXX executable ../../bin/llama-export-lora\n[100%] Built target llama-export-lora\n[  3%] Built target build_info\n[ 19%] Built target ggml-base\n[ 38%] Built target ggml-cpu\n[ 42%] Built target ggml\n[ 80%] Built target llama\n[ 96%] Built target common\n[ 96%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o\n[100%] Linking CXX executable ../../bin/llama-cli\n[100%] Built target llama-cli\n","output_type":"stream"},{"name":"stderr","text":"Unsloth: You have 2 CPUs. Using `safe_serialization` is 10x slower.\nWe shall switch to Pytorch saving, which might take 3 minutes and not 30 minutes.\nTo force `safe_serialization`, set it to `None` instead.\nUnsloth: Kaggle/Colab has limited disk space. We need to delete the downloaded\nmodel which will save 4-16GB of disk space, allowing you to save on Kaggle/Colab.\nUnsloth: Will remove a cached repo with size 6.0G\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Merging 4bit and LoRA weights to 16bit...\nUnsloth: Will use up to 21.16 out of 31.35 RAM for saving.\nUnsloth: Saving model... This might take 5 minutes ...\n","output_type":"stream"},{"name":"stderr","text":" 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 14/32 [00:00<00:00, 32.79it/s]\nWe will save to Disk and not RAM now.\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:15<00:00,  2.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Saving tokenizer... Done.\nUnsloth: Saving model/pytorch_model-00001-of-00004.bin...\nUnsloth: Saving model/pytorch_model-00002-of-00004.bin...\nUnsloth: Saving model/pytorch_model-00003-of-00004.bin...\nUnsloth: Saving model/pytorch_model-00004-of-00004.bin...\nDone.\n","output_type":"stream"},{"name":"stderr","text":"Unsloth: Converting llama model. Can use fast conversion = False.\n","output_type":"stream"},{"name":"stdout","text":"==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n   \\\\   /|    [0] Installing llama.cpp might take 3 minutes.\nO^O/ \\_/ \\    [1] Converting HF to GGUF 16bits might take 3 minutes.\n\\        /    [2] Converting GGUF 16bits to ['q8_0'] might take 10 minutes each.\n \"-____-\"     In total, you will have to wait at least 16 minutes.\n\nUnsloth: Installing llama.cpp. This might take 3 minutes...\nUnsloth: [1] Converting model at model into q8_0 GGUF format.\nThe output location will be /kaggle/working/model/unsloth.Q8_0.gguf\nThis might take 3 minutes...\n2025-01-28 20:27:27.181333: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-01-28 20:27:27.208793: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-01-28 20:27:27.215399: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nWriting:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 4.54G/8.53G [02:19<01:55, 34.4Mbyte/s]Traceback (most recent call last):\n  File \"/kaggle/working/llama.cpp/convert_hf_to_gguf.py\", line 5140, in <module>\n    main()\n  File \"/kaggle/working/llama.cpp/convert_hf_to_gguf.py\", line 5134, in main\n    model_instance.write()\n  File \"/kaggle/working/llama.cpp/convert_hf_to_gguf.py\", line 443, in write\n    self.gguf_writer.write_tensors_to_file(progress=True)\n  File \"/kaggle/working/llama.cpp/gguf-py/gguf/gguf_writer.py\", line 454, in write_tensors_to_file\n    ti.tensor.tofile(fout)\n  File \"/kaggle/working/llama.cpp/gguf-py/gguf/lazy.py\", line 211, in tofile\n    return eager.tofile(*args, **kwargs)\nOSError: Not enough free space to write 62390272 bytes\nWriting:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 4.54G/8.53G [02:21<02:04, 32.0Mbyte/s]\nUnsloth: Conversion completed! Output location: /kaggle/working/model/unsloth.Q8_0.gguf\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-7ea4c770b5c5>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained_gguf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/unsloth/save.py\u001b[0m in \u001b[0;36munsloth_save_pretrained_gguf\u001b[0;34m(self, save_directory, tokenizer, quantization_method, first_conversion, push_to_hub, token, private, is_main_process, state_dict, save_function, max_shard_size, safe_serialization, variant, save_peft_format, tags, temporary_location, maximum_memory_usage)\u001b[0m\n\u001b[1;32m   1743\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmodelfile\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1744\u001b[0m         \u001b[0mmodelfile_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_save_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Modelfile\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1745\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelfile_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1746\u001b[0m             \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1747\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device"],"ename":"OSError","evalue":"[Errno 28] No space left on device","output_type":"error"}],"execution_count":14},{"cell_type":"markdown","source":"# Start the OLLAMA Server","metadata":{}},{"cell_type":"code","source":"import subprocess\nsubprocess.Popen([\"ollama\", \"serve\"])\nimport time\ntime.sleep(3)\nprint(tokenizer._ollama_modelfile)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T20:29:56.858594Z","iopub.status.idle":"2025-01-28T20:29:56.859025Z","shell.execute_reply":"2025-01-28T20:29:56.858824Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This command registers the fine-tuned model (deepseek_finetuned_model) with Ollama. Once registered:\n\n- The model can be run locally using Ollama.\n- It becomes accessible for further tasks, such as querying, evaluating, or deploying in specific applications.\n- Ollama ensures the model is formatted and stored correctly for efficient usage","metadata":{}},{"cell_type":"code","source":"!ollama create deepseek_finetuned_model -f ./model/Modelfile","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install ollama","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import ollama\n\nresponse = ollama.chat(model=\"deepseek_finetuned_model\",\n            messages=[{ \"role\": \"user\", \"content\": \"Continue the Fibonacci sequence: 1, 1, 2, 3, 5, 8,\"\n            },\n                      ])\n\nprint(response.message.content)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"1, 1, 2, 3, 5, 8, 13, 21","metadata":{}},{"cell_type":"code","source":"from IPython.display import Markdown\nimport ollama\n\nresponse = ollama.chat(model=\"deepseek_finetuned_model\",\n                       messages=[{\"role\": \"user\",\n                                  \"content\": \"How to add chart to a document?\"},\n                      ])\n\nMarkdown(response.message.content)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"To add a chart to a document, follow these steps:\n\n- Insert a Table: Start by inserting a table into the document. You can do this using the 'Table' tool in most word processors.\n- Insert Data: Add data into the table. Ensure that your data is properly formatted and organised before adding the chart.\n- Choose a Chart Type: Select the type of chart you want to create from the available options (e.g., bar chart, pie chart, line graph, etc.).\n- Edit the Chart Data: Add the necessary data points and formatting to the chart using the chart editor that appears once the chart is selected.\n- Format the Table and Chart Together: Make sure the table and chart work well together by adjusting alignment, spacing, and other design elements as needed.\n\nFor more detailed instructions, you may want to consult a guide or use a tool such as Microsoft Word's chart features.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}