{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aKs1PM-O-VQa"
   },
   "source": [
    "# Models\n",
    "\n",
    "Looking at the lower level API of Transformers - the models that wrap PyTorch code for the transformers themselves.\n",
    "\n",
    "This notebook can run on a low-cost or free T4 runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "import sys\n",
    "\n",
    "def clean_notebook():\n",
    "    IPython.display.clear_output(wait=True)\n",
    "    print(\"Notebook cleaned.\")\n",
    "\n",
    "# Uninstall flash-attention (the PyPI package name may vary)\n",
    "!pip uninstall flash-attn -y\n",
    "!pip uninstall xformers -y\n",
    "\n",
    "!pip install flash-attn\n",
    "!pip install xformers\n",
    "\n",
    "\n",
    "# Clean up the notebook\n",
    "clean_notebook()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "C9zvDGWD5pKp"
   },
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, BitsAndBytesConfig\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xyKWKWSw7Iqp"
   },
   "source": [
    "# Sign in to Hugging Face\n",
    "\n",
    "1. If you haven't already done so, create a free HuggingFace account at https://huggingface.co and navigate to Settings, then Create a new API token, giving yourself write permissions\n",
    "\n",
    "2. Press the \"key\" icon on the side panel to the left, and add a new secret:\n",
    "`HF_TOKEN = your_token`\n",
    "\n",
    "3. Execute the cell below to log in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xd7cEDUC6Lkq"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f85d224d45d743b69c5cd9855ae115e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "hf_token = os.environ.get('HF_TOKEN')\n",
    "\n",
    "# Log into Hugging Face, adding credentials to git if needed\n",
    "login(hf_token, add_to_git_credential=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "UtN7OKILQato"
   },
   "outputs": [],
   "source": [
    "# instruct models\n",
    "\n",
    "LLAMA    = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "Deepseek =\"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "KgxCLBJIT5Hx"
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant answer in thai language\"},\n",
    "    {\"role\": \"user\", \"content\": \"เล่านิทานให้ที in thai language เกี่ยวกับ แมว และ สุนัข และ เต่า \"}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZSiYqPn87msu"
   },
   "source": [
    "# Accessing Llama 3.1 from Meta\n",
    "\n",
    "In order to use the fantastic Llama 3.1, Meta does require you to sign their terms of service.\n",
    "\n",
    "Visit their model instructions page in Hugging Face:\n",
    "https://huggingface.co/meta-llama/Meta-Llama-3.1-8B\n",
    "\n",
    "At the top of the page are instructions on how to agree to their terms. If possible, you should use the same email as your huggingface account.\n",
    "\n",
    "In my experience approval comes in a couple of minutes. Once you've been approved for any 3.1 model, it applies to the whole family of models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "hhOgL1p_T6-b"
   },
   "outputs": [],
   "source": [
    "# Quantization Config - this allows us to load the model into memory and use less memory\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7MtWZYZG920F"
   },
   "source": [
    "If the next cell gives you a 403 permissions error, then please check:\n",
    "1. Are you logged in to HuggingFace? Try running `login()` to check your key works\n",
    "2. Did you set up your API key with full read and write permissions?\n",
    "3. If you visit the Llama3.1 page at https://huggingface.co/meta-llama/Meta-Llama-3.1-8B, does it show that you have access to the model near the top?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Zi8YXiwJHF59"
   },
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(LLAMA)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "S5jly421tno3"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a585d0dc8f949e4b22eaef0eadfd0aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    LLAMA, \n",
    "    device_map=\"auto\", \n",
    "    quantization_config=quant_config,\n",
    "    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "bdbYaT8hWXWE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory footprint: 5,591.5 MB\n"
     ]
    }
   ],
   "source": [
    "memory = model.get_memory_footprint() / 1e6\n",
    "print(f\"Memory footprint: {memory:,.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "P0qmAD5ZtqWA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "SkYEXzbotcud"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "You are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "เล่านิทานให้ที<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "นิทานที่ฉันจะเล่าให้คุณฟังวันนี้คือเรื่อง \"หมูสามตัว\"\n",
      "\n",
      "มีสามตัวหมูอยู่ในสวนหมู พวกมันชอบเล่นกันและกินอาหารได้เยอะ แต่หมูหนึ่งตัวมีชีวิตที่ไม่เหมือนกับหมูที่เหลือ มันชอบไปเดินเล่นในสวนและดูพืชพันธุ์ต่างๆ มากกว่าที่จะกินอาหาร\n",
      "\n",
      "หนึ่งวัน หมูสามตัวนี้ไปพบกับหมูอื่นๆ ที่กำลังเล่นอยู่ในสวน \"ทำไมคุณไม่ไปกินอาหาร?\" หมูอื่นๆ ถาม \"อาหารมันเยอะมากที่นี่\" แต่หมูหนึ่งตัวนั้นไม่สนใจ \"ฉันอยากไปดูพืชพันธุ์ต่างๆ มากกว่าที่จะกินอาหาร\"\n",
      "\n",
      "หมูอื่นๆ ก็ไม่เข้าใจว่าทำไมหมูหนึ่งตัวจะไม่ชอบกินอาหาร แต่พวกมันก็ไม่ได้บังคับหมูหนึ่งตัวให้ทำตามพวกมัน\n",
      "\n",
      "วันเวลาผ่านไปและหมูสามตัวนี้ก็โตขึ้นและแข็งแรงขึ้น แต่หมูหนึ่งตัวนั้นไม่เคยเปลี่ยนแปลงความชอบของมัน \"ฉันชอบไปดูพืชพันธุ์ต่างๆ มากกว่าที่จะกินอาหาร\" มันพูด\n",
      "\n",
      "สุดท้าย หมูสามตัวนี้ก็อายุได้ 10 ปี และหมูหนึ่งตัวนั้นยังคงอยู่ในสวนและดูพืชพันธุ์ต่างๆ อย่างไม่เปลี่ยนแปลง\n",
      "\n",
      "เรื่องนี้บอกเราว่า \"แต่ละคนมีความชอบและความสามารถที่ไม่เหมือนกัน\" และไม่ควรบังคับคนอื่นๆ ให้ทำตามเรา<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(inputs, max_new_tokens=500)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "2oL0RWU2ttZf"
   },
   "outputs": [],
   "source": [
    "# Clean up\n",
    "\n",
    "del inputs, outputs, model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iDCeJ20e4Hxx"
   },
   "source": [
    "## A couple of quick notes on the next block of code:\n",
    "\n",
    "I'm using a HuggingFace utility called TextStreamer so that results stream back.\n",
    "To stream results, we simply replace:  \n",
    "`outputs = model.generate(inputs, max_new_tokens=80)`  \n",
    "With:  \n",
    "`streamer = TextStreamer(tokenizer)`  \n",
    "`outputs = model.generate(inputs, max_new_tokens=80, streamer=streamer)`\n",
    "\n",
    "Also I've added the argument `add_generation_prompt=True` to my call to create the Chat template. This ensures that Phi generates a response to the question, instead of just predicting how the user prompt continues. Try experimenting with setting this to False to see what happens. You can read about this argument here:\n",
    "\n",
    "https://huggingface.co/docs/transformers/main/en/chat_templating#what-are-generation-prompts\n",
    "\n",
    "Thank you to student Piotr B for raising the issue!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "RO_VYZ3DZ7cs"
   },
   "outputs": [],
   "source": [
    "# Wrapping everything in a function - and adding Streaming and generation prompts\n",
    "\n",
    "def generate(model, messages):\n",
    "  tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "  tokenizer.pad_token = tokenizer.eos_token\n",
    "  inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True).to(\"cuda\")\n",
    "  streamer = TextStreamer(tokenizer)\n",
    "  model = AutoModelForCausalLM.from_pretrained(model, device_map=\"auto\", quantization_config=quant_config)\n",
    "  outputs = model.generate(inputs, max_new_tokens=2500, streamer=streamer)\n",
    "  del tokenizer, streamer, model, inputs, outputs\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "RFjaY4Pdvbfy"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c8467d31d6342d691dac16d3af066a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "You are a helpful assistant answer in thai language<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "เล่านิทานให้ที เกี่ยวกับ แมว และ สุนัข และ เต่า<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "มีครั้งหนึ่ง 3 ตัวสัตว์อยู่ในสวน \n",
      "\n",
      "แมวหนึ่งตัว มีสีดำและขาว มีใจอยากเล่นและล่าเหยื่อ\n",
      "\n",
      "สุนัขหนึ่งตัว มีสีแดง มีใจอยากเล่นและใช้กำลัง\n",
      "\n",
      "เต่าหนึ่งตัว มีสีเหลือง มีใจอยากว่ายน้ำและพักผ่อน\n",
      "\n",
      "หนึ่งวัน แมวและสุนัขได้พบกันในสวน แมวพูดว่า \"ฉันสามารถล่าเหยื่อได้ไง\" สุนัขพูดว่า \"ฉันสามารถใช้กำลังได้ไง\" เต่าพูดว่า \"ฉันสามารถว่ายน้ำได้ไง\"\n",
      "\n",
      "แมวและสุนัขได้เล่นกันอยู่ในสวน แต่เต่าไม่กล้าเล่นเพราะกลัวจะถูกล่า\n",
      "\n",
      "จนกระทั่งแมวและสุนัขได้ทำร้ายตัวเองกันเอง แมวได้ถูกแมวอีกตัวหนึ่งทำร้าย สุนัขได้ถูกสุนัขอีกตัวหนึ่งทำร้าย\n",
      "\n",
      "เต่าได้เห็นเหตุการณ์นี้และพูดว่า \"ฉันสามารถพักผ่อนได้ไง\" แมวและสุนัขได้ฟังคำพูดของเต่าและได้เรียนรู้ว่า การพักผ่อนเป็นเรื่องสำคัญ\n",
      "\n",
      "จากนั้น แมวและสุนัขได้เล่นกันอยู่ในสวนพร้อมกับเต่า และได้เรียนรู้ว่า การเล่นและพักผ่อนเป็นเรื่องสำคัญ<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "generate(LLAMA, messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "0m8yjMB3ZTp3"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "141e91b2ea2c4f69ab25f6b3c2c4d419",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜>You are a helpful assistant answer in thai language<｜User｜>เล่านิทานให้ที in thai language เกี่ยวกับ แมว และ สุนัข และ เต่า <｜Assistant｜><think>\n",
      "Okay, the user wants a story in Thai about cats, dogs, and turtles. I should make it engaging and suitable for all ages. Let me think of a simple plot that includes these animals and some adventures.\n",
      "\n",
      "Maybe the animals can go on a journey together, facing some challenges. That way, I can include each of them and create a fun scenario. I'll set it in a village near a river with a temple, which is a common setting in Thai stories.\n",
      "\n",
      "I'll introduce each animal with their own traits: the cat as clever, the dog as loyal, and the turtle as wise. Their interactions can show teamwork and friendship. Perhaps they come together when something happens, like finding a lost item or helping someone in need.\n",
      "\n",
      "I should include some Thai vocabulary to make it authentic without making it too complex. The story should have a happy ending, showing how their teamwork leads to a positive outcome.\n",
      "\n",
      "Let me outline the story: The animals meet when the cat hears a noise near the temple. They explore and find a lost puppy. They work together to return it to its owner, learning about kindness and friendship along the way.\n",
      "\n",
      "I need to make sure the language is simple and flows naturally, suitable for reading aloud. I'll also add some descriptive elements to make the setting vivid.\n",
      "\n",
      "Now, I'll start writing the story, making sure each part transitions smoothly and keeps the reader interested. I'll include the interactions between the animals to highlight their unique traits.\n",
      "\n",
      "Finally, I'll conclude with a moral or a positive message about working together and the value of friendship. That should make the story both entertaining and educational.\n",
      "</think>\n",
      "\n",
      "นิทานในภาษาไทยเกี่ยวกับ แมว สุนัข และ เต่า\n",
      "\n",
      "---\n",
      "\n",
      "เรื่องนี้เกี่ยวกับสามคุณค่า: แมวชื่อ \"น้อย\" สุนัขชื่อ \"น้อยสุนัข\" และ เต่าชื่อ \"น้อยเต่า\" ที่住ในเมืองเล็กๆ near river และมีหินโบรฆาร์ที่เรียกว่า \"พระปราสิทธิ์\" (พระบรมสารีริก) หรือ \"เจ้าพระ\" ซึ่งชาวน้องเมื่อจะเรียกว่า \"เจ้าพระ\"\n",
      "\n",
      "เมื่อครั้งนั้น มีเหตุการณ์ง่ายๆเกี่ยวข้องกับเจ้าพระ ที่ทำให้สามคุณค่าต้องร่วมมือให้เรื่องดีขึ้น\n",
      "\n",
      "เริ่มแรก น้อย (แมว) ฟังเสียงที่ผิดปกติ near temple \"พระปราสิทธิ์\" แค่猫耳朵靈敏 she hears something abnormal near the temple \"พระปราสิทธิ์\" เพราะแมวเป็นน้องที่ชื่อความไว้ใจดี\n",
      "\n",
      "น้อยสุนัข (สุนัข) ได้ตามหาและพบว่า \"เจ้าพระ\" มีหินลอยนอก temple โดยไม่ทราบว่าเกี่ยวข้องกับใคร\n",
      "\n",
      "น้อยเต่า (เต่า) ที่ชื่อ \"น้อยเต่า\" เป็นเต่าผู้นอบน้อมและชื่อความปรัญชันสูง she thinks \"อาจมีหินลอยเกี่ยวกับเจ้าพระ ซึ่งอาจเป็นลูกของใคร\"\n",
      "\n",
      "ต่อมา น้อย (แมว) ชื่อ \"น้อย\" เป็นแมวฉลาด she suggests \"เราไปตรวจสอบหินลอยนี้ และค้นหาผู้ต้องการหินนี้\"\n",
      "\n",
      "น้อง们ได้ตรวจสอบและพบว่าหินลอยนี้เป็นสมบัติของเด็กหนุ่มชื่อ \"น้อยเด็ก\" ซึ่งแย่งหินลอยนี้ไปใช้เพื่อการเคantineeding\n",
      "\n",
      "น้อย (แมว) เป็นแมวที่ชื่อความมีนวัตกรรม she thinks \"นี่อาจเป็นลูกของเด็กนี้\"\n",
      "\n",
      "น้อง们ได้พบกับเด็กน้อยที่ต้องการหินลอยนี้เพื่อ feeds stray animals nearby\n",
      "\n",
      "เมื่อพบกัน น้อย (แมว) กล้า讲 \"นี่อาจเป็นของของคุณ\" น้องสุนัขชื่อ \"น้อยสุนัข\" กลับบ่น \"ฉันไม่เชื่อ\" และเต่าชื่อ \"น้อยเต่า\" กลับอธิบาย \"อาจมีหินลอยนี้เป็นสมบัติของเจ้าพระ\"\n",
      "\n",
      "ต่อมา น้อย (แมว) กลับบ้านเกี่ยวกับ \"เจ้าพระ\" และพบว่า \"เจ้าพระ\" เบื่อจะไม่ขอ anything ซึ่งทำให้เด็กน้อยต้องรู้สึกเสียใจ\n",
      "\n",
      "น้องสุนัขชื่อ \"น้อยสุนัข\" กลับบ่น \"ฉันไม่เชื่อ! ฉันไม่เชื่อ! ฉันไม่เชื่อ! ฉันไม่เชื่อ!\"\n",
      "\n",
      "น้อยเต่า (เต่า) กลับอธิบาย \"อาจมีหินลอยนี้เป็นสมบัติของเจ้าพระ ซึ่งอาจไม่ต้องการให้ใครม_touch\"\n",
      "\n",
      "น้อย (แมว) กลับบ้านเกี่ยวกับ \"เจ้าพระ\" และพบว่า \"เจ้าพระ\" เบื่อจะไม่ขอ anything ซึ่งทำให้เด็กน้อยต้องรู้สึกเสียใจ\n",
      "\n",
      "เมื่อเห็นน้องๆ ที่รู้สึกเสียใจ น้อย (แมว) กลับบ้านเกี่ยวกับ \"เจ้าพระ\" และพบว่า \"เจ้าพระ\" เบื่อจะไม่ขอ anything ซึ่งทำให้เด็กน้อยต้องรู้สึกเสียใจ\n",
      "\n",
      "น้องสุนัขชื่อ \"น้อยสุนัข\" กลับบ่น \"ฉันไม่เชื่อ! ฉันไม่เชื่อ! ฉันไม่เชื่อ! ฉันไม่เชื่อ!\"\n",
      "\n",
      "น้อยเต่า (เต่า) กลับอธิบาย \"อาจมีหินลอยนี้เป็นสมบัติของเจ้าพระ ซึ่งอาจไม่ต้องการให้ใครม_touch\"\n",
      "\n",
      "น้อง们ได้แถลงนอกบ้าน และพบว่า \"เจ้าพระ\" เบื่อจะไม่ขอ anything ซึ่งทำให้เด็กน้อยต้องรู้สึกเสียใจ\n",
      "\n",
      "เมื่อเห็นน้องๆ ที่รู้สึกเสียใจ น้อย (แมว) กลับบ้านเกี่ยวกับ \"เจ้าพระ\" และพบว่า \"เจ้าพระ\" เบื่อจะไม่ขอ anything ซึ่งทำให้เด็กน้อยต้องรู้สึกเสียใจ\n",
      "\n",
      "น้องสุนัขชื่อ \"น้อยสุนัข\" กลับบ่น \"ฉันไม่เชื่อ! ฉันไม่เชื่อ! ฉันไม่เชื่อ! ฉันไม่เชื่อ!\"\n",
      "\n",
      "น้อยเต่า (เต่า) กลับอธิบาย \"อาจมีหินลอยนี้เป็นสมบัติของเจ้าพระ ซึ่งอาจไม่ต้องการให้ใครม_touch\"\n",
      "\n",
      "น้อง们ได้แถลงนอกบ้าน และพบว่า \"เจ้าพระ\" เบื่อจะไม่ขอ anything ซึ่งทำให้เด็กน้อยต้องรู้สึกเสียใจ\n",
      "\n",
      "เมื่อเห็นน้องๆ ที่รู้สึกเสียใจ น้อย (แมว) กลับบ้านเกี่ยวกับ \"เจ้าพระ\" และพบว่า \"เจ้าพระ\" เบื่อจะไม่ขอ anything ซึ่งทำให้เด็กน้อยต้องรู้สึกเสียใจ\n",
      "\n",
      "น้องสุนัขชื่อ \"น้อยสุนัข\" กลับบ่น \"ฉันไม่เชื่อ! ฉันไม่เชื่อ! ฉันไม่เชื่อ! ฉันไม่เชื่อ!\"\n",
      "\n",
      "น้อยเต่า (เต่า) กลับอธิบาย \"อาจมีหินลอยนี้เป็นสมบัติของเจ้าพระ ซึ่งอาจไม่ต้องการให้ใครม_touch\"\n",
      "\n",
      "น้อง们ได้แถลงนอกบ้าน และพบว่า \"เจ้าพระ\" เบื่อจะไม่ขอ anything ซึ่งทำให้เด็กน้อยต้องรู้สึกเสียใจ\n",
      "\n",
      "เมื่อเห็นน้องๆ ที่รู้สึกเสียใจ น้อย (แมว) กลับบ้านเกี่ยวกับ \"เจ้าพระ\" และพบว่า \"เจ้าพระ\" เบื่อจะไม่ขอ anything ซึ่งทำให้เด็กน้อยต้องรู้สึกเสียใจ\n",
      "\n",
      "น้องสุนัขชื่อ \"น้อยสุนัข\" กลับบ่น \"ฉันไม่เชื่o! ฉันไม่เชื่อ! ฉันไม่เชื่อ! ฉันไม่เชื่อ!\"\n",
      "\n",
      "น้อยเต่า (เต่า) กลับอธิบาย \"อาจมีหินลอยนี้เป็นสมบัติของเจ้าพระ ซึ่งอาจไม่ต้องการให้ใครม_touch\"\n",
      "\n",
      "น้อง们ได้แถลงนอกบ้าน และพบว่า \"เจ้าพระ\" เบื่อจะไม่ขอ anything ซึ่งทำให้เด็กน้อยต้องรู้สึกเสียใจ\n",
      "\n",
      "---\n",
      "\n",
      "เรื่องนี้เป็นเรื่องราวเกี่ยวกับสามคุณค่า: แมว สุนัข และ เต่า ที่ช่วยเหลือเด็กน้อยที่ต้องการหินลอยนี้เพื่อ feeds stray animals nearby\n",
      "\n",
      "มีการโต้แย่เกี่ยวกับ \"เจ้าพระ\" ซึ่งอาจไม่ต้องการให้ใครม_touch แต่น้องๆ ได้ทำงานร่วมกันเพื่อช่วยเหลือเด็กน้อย\n",
      "\n",
      "ในตอนจบ น้องๆ ได้พบกับ \"เจ้าพระ\" และช่วยเหลือเด็กน้อยที่ต้องการหินลอยนี้เพื่อ feeds stray animals nearby\n",
      "\n",
      "น้องสุนัขชื่อ \"น้อยสุนัข\" กลับบ่น \"ฉันไม่เชื่อ! ฉันไม่เชื่อ! ฉันไม่เชื่อ! ฉันไม่เชื่อ!\"\n",
      "\n",
      "น้อยเต่า (เต่า) กลับอธิบาย \"อาจมีหินลอยนี้เป็นสมบัติของเจ้าพระ ซึ่งอาจไม่ต้องการให้ใครม_touch\"\n",
      "\n",
      "น้อย (แมว) กลับบ้านเกี่ยวกับ \"เจ้าพระ\" และพบว่า \"เจ้าพระ\" เบื่อจะไม่ขอ anything ซึ่งทำให้เด็กน้อยต้องรู้สึกเสียใจ\n",
      "\n",
      "เมื่อเห็นน้องๆ ที่รู้สึกเสียใจ น้อย (แมว) กลับบ้านเกี่ยวกับ \"เจ้าพระ\" และพบว่า \"เจ้าพระ\" เบื่อจะไม่ขอ anything ซึ่งทำให้เด็กน้อยต้องรู้สึกเสียใจ\n",
      "\n",
      "น้องสุนัขชื่อ \"น้อยสุนัข\" กลับบ่น \"ฉันไม่เชื่อ! ฉันไม่เชื่อ! ฉ\n"
     ]
    }
   ],
   "source": [
    "generate(Deepseek, messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
