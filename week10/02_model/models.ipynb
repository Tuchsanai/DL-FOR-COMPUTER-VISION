{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aKs1PM-O-VQa"
   },
   "source": [
    "# Models\n",
    "\n",
    "Looking at the lower level API of Transformers - the models that wrap PyTorch code for the transformers themselves.\n",
    "\n",
    "This notebook can run on a low-cost or free T4 runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook cleaned.\n"
     ]
    }
   ],
   "source": [
    "import IPython\n",
    "import sys\n",
    "\n",
    "def clean_notebook():\n",
    "    IPython.display.clear_output(wait=True)\n",
    "    print(\"Notebook cleaned.\")\n",
    "\n",
    "# Uninstall flash-attention (the PyPI package name may vary)\n",
    "!pip uninstall flash-attn -y\n",
    "!pip uninstall xformers -y\n",
    "\n",
    "!pip install flash-attn\n",
    "!pip install xformers\n",
    "\n",
    "\n",
    "# Clean up the notebook\n",
    "clean_notebook()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "C9zvDGWD5pKp"
   },
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, BitsAndBytesConfig\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xyKWKWSw7Iqp"
   },
   "source": [
    "# Sign in to Hugging Face\n",
    "\n",
    "1. If you haven't already done so, create a free HuggingFace account at https://huggingface.co and navigate to Settings, then Create a new API token, giving yourself write permissions\n",
    "\n",
    "2. Press the \"key\" icon on the side panel to the left, and add a new secret:\n",
    "`HF_TOKEN = your_token`\n",
    "\n",
    "3. Execute the cell below to log in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "xd7cEDUC6Lkq"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e507b93f14dc49f0bac4560e721171e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "hf_token = os.environ.get('HF_TOKEN')\n",
    "\n",
    "# Log into Hugging Face, adding credentials to git if needed\n",
    "login(hf_token, add_to_git_credential=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "UtN7OKILQato"
   },
   "outputs": [],
   "source": [
    "# instruct models\n",
    "\n",
    "LLAMA    = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "Deepseek =\"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "KgxCLBJIT5Hx"
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant answer in only thai language\"},\n",
    "    {\"role\": \"user\", \"content\": \"เล่านิทานให้ที in thai  เกี่ยวกับ แมว และ สุนัข และ เต่า\"}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZSiYqPn87msu"
   },
   "source": [
    "# Accessing Llama 3.1 from Meta\n",
    "\n",
    "In order to use the fantastic Llama 3.1, Meta does require you to sign their terms of service.\n",
    "\n",
    "Visit their model instructions page in Hugging Face:\n",
    "https://huggingface.co/meta-llama/Meta-Llama-3.1-8B\n",
    "\n",
    "At the top of the page are instructions on how to agree to their terms. If possible, you should use the same email as your huggingface account.\n",
    "\n",
    "In my experience approval comes in a couple of minutes. Once you've been approved for any 3.1 model, it applies to the whole family of models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "hhOgL1p_T6-b"
   },
   "outputs": [],
   "source": [
    "# Quantization Config - this allows us to load the model into memory and use less memory\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7MtWZYZG920F"
   },
   "source": [
    "If the next cell gives you a 403 permissions error, then please check:\n",
    "1. Are you logged in to HuggingFace? Try running `login()` to check your key works\n",
    "2. Did you set up your API key with full read and write permissions?\n",
    "3. If you visit the Llama3.1 page at https://huggingface.co/meta-llama/Meta-Llama-3.1-8B, does it show that you have access to the model near the top?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Zi8YXiwJHF59"
   },
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(LLAMA)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "S5jly421tno3"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ee27bdba88b411bb6350111be05cccd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    LLAMA, \n",
    "    device_map=\"auto\", \n",
    "    quantization_config=quant_config,\n",
    "    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "bdbYaT8hWXWE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory footprint: 5,591.5 MB\n"
     ]
    }
   ],
   "source": [
    "memory = model.get_memory_footprint() / 1e6\n",
    "print(f\"Memory footprint: {memory:,.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "P0qmAD5ZtqWA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "SkYEXzbotcud"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "You are a helpful assistant answer in only thai language<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "เล่านิทานให้ที in thai  เกี่ยวกับ แมว และ สุนัข และ เต่า<|eot_id|><|start_header_id|>assistant\n",
      "\n",
      "นิทานเรื่อง \"แมว สุนัข และ เต่า\"\n",
      "\n",
      "มีแมว สุนัข และ เต่า 3 ตัว ที่อยู่ในสวนใหญ่\n",
      "\n",
      "แมวเป็นแมวที่ฉลาดและสามารถวิ่งได้เร็วมาก สุนัขเป็นสุนัขที่มีใจดีและสามารถช่วยเหลือได้ และเต่าเป็นเต่าที่สามารถปีนป่ายได้ยอดเยี่ยม\n",
      "\n",
      "วันหนึ่ง พวกมันก็พบกับปัญหาที่เหมือนกัน นั่นคือ พวกมันไม่มีน้ำดื่ม\n",
      "\n",
      "แมวพูดว่า \"ฉันสามารถวิ่งไปหาน้ำได้\" แต่สุนัขพูดว่า \"ฉันไม่สามารถวิ่งไปหาน้ำได้ แต่ฉันสามารถช่วยเหลือคุณได้\"\n",
      "\n",
      "เตาพูดว่า \"ฉันสามารถปีนป่ายไปหาน้ำได้ แต่ฉันไม่สามารถแบ่งปันน้ำกับคุณได้\"\n",
      "\n",
      "แมวและสุนัขได้ขอเตาให้แบ่งปันน้ำกัน แต่เตาปฏิเสธ\n",
      "\n",
      "สุนัขและแมวได้ใช้ความฉลาดและความสามารถของพวกเขาเพื่อไปหาน้ำและแบ่งปันกัน\n",
      "\n",
      "สุดท้าย พวกมันก็ได้น้ำและแบ่งปันกัน และพวกมันก็ได้เรียนรู้ว่า การช่วยเหลือกันและแบ่งปันน้ำสามารถช่วยให้พวกมันรอดชีวิตได้\n",
      "\n",
      "และพวกมันก็ได้กลายเป็นเพื่อนแทนที่จะคู่แข่ง<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(inputs, max_new_tokens=500)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "2oL0RWU2ttZf"
   },
   "outputs": [],
   "source": [
    "# Clean up\n",
    "\n",
    "del inputs, outputs, model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iDCeJ20e4Hxx"
   },
   "source": [
    "## A couple of quick notes on the next block of code:\n",
    "\n",
    "I'm using a HuggingFace utility called TextStreamer so that results stream back.\n",
    "To stream results, we simply replace:  \n",
    "`outputs = model.generate(inputs, max_new_tokens=80)`  \n",
    "With:  \n",
    "`streamer = TextStreamer(tokenizer)`  \n",
    "`outputs = model.generate(inputs, max_new_tokens=80, streamer=streamer)`\n",
    "\n",
    "Also I've added the argument `add_generation_prompt=True` to my call to create the Chat template. This ensures that Phi generates a response to the question, instead of just predicting how the user prompt continues. Try experimenting with setting this to False to see what happens. You can read about this argument here:\n",
    "\n",
    "https://huggingface.co/docs/transformers/main/en/chat_templating#what-are-generation-prompts\n",
    "\n",
    "Thank you to student Piotr B for raising the issue!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "RO_VYZ3DZ7cs"
   },
   "outputs": [],
   "source": [
    "# Wrapping everything in a function - and adding Streaming and generation prompts\n",
    "def generate(model, messages):\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Prepare inputs\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        return_tensors=\"pt\", \n",
    "        add_generation_prompt=True\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    # Create a streamer for live output (optional)\n",
    "    streamer = TextStreamer(tokenizer)\n",
    "    \n",
    "    # Load the model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model, \n",
    "        device_map=\"auto\", \n",
    "        quantization_config=quant_config\n",
    "    )\n",
    "    \n",
    "    # Generate with temperature=0 and additional parameters\n",
    "    outputs = model.generate(\n",
    "        inputs,\n",
    "        max_new_tokens=3000,\n",
    "        temperature=0.0,       # Deterministic behavior\n",
    "        do_sample=False,       # Typically set False if temperature=0\n",
    "        top_p=1.0,             # No nucleus sampling constraint\n",
    "        top_k=50,              # You can adjust this if needed\n",
    "        repetition_penalty=1.0, # Adjust to penalize repeated phrases\n",
    "        streamer=streamer      # Remove if you don't need live output\n",
    "    )\n",
    "    \n",
    "    # Clean up\n",
    "    del tokenizer, streamer, model, inputs, outputs\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "RFjaY4Pdvbfy"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "823339a6de894c7f80e4c68a03df3011",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "You are a helpful assistant answer in only thai language<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "เล่านิทานให้ที in thai  เกี่ยวกับ แมว และ สุนัข และ เต่า<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "มีครั้งหนึ่ง มีแมว สุนัข และ เต่าอยู่ในสวนร่วมกัน แมวชอบนอนพักผ่อน สุนัขชอบเล่น และ เต่าชอบว่ายน้ำ\n",
      "\n",
      "วันหนึ่ง แมว สุนัข และ เต่าได้พบกันในสวน แมวพูดว่า \"ฉันเหนื่อยมาก ฉันอยากนอนพักผ่อน\" สุนัขพูดว่า \"ฉันอยากเล่น ฉันอยากเล่นกับเพื่อนๆ\" เต่าพูดว่า \"ฉันอยากว่ายน้ำ ฉันอยากว่ายน้ำในบ่อ\"\n",
      "\n",
      "แมว สุนัข และ เต่าได้พูดคุยกันและตัดสินใจว่าจะไปว่ายน้ำในบ่อพร้อมกัน แมวจะนอนพักผ่อนบนชายหาด สุนัขจะเล่นกับเพื่อนๆ และ เต่าจะว่ายน้ำในบ่อ\n",
      "\n",
      "เมื่อถึงเวลาว่ายน้ำ เต่าได้ว่ายน้ำไปในบ่อ สุนัขได้เล่นกับเพื่อนๆ และ แมวได้นอนพักผ่อนบนชายหาด\n",
      "\n",
      "หลังจากว่ายน้ำไปหนึ่งรอบ เต่าได้กลับมาและพูดว่า \"ฉันเหนื่อยมาก ฉันอยากนอนพักผ่อน\" สุนัขพูดว่า \"ฉันเหนื่อยมาก ฉันอยากนอนพักผ่อน\" แมวพูดว่า \"ฉันเหนื่อยมาก ฉันอยากนอนพักผ่อน\"\n",
      "\n",
      "ทั้งสามได้นอนพักผ่อนบนชายหาดพร้อมกัน และเมื่อตื่นขึ้นมา พวกเขาก็ได้พบกันและพูดว่า \"เรารู้สึกดีมาก เรารู้สึกเหนื่อยและพักผ่อนแล้ว\"<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "generate(LLAMA, messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "0m8yjMB3ZTp3"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5df7a22f7c6442ebc2811d7b4c77712",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜>You are a helpful assistant answer in only thai language<｜User｜>เล่านิทานให้ที in thai  เกี่ยวกับ แมว และ สุนัข และ เต่า<｜Assistant｜><think>\n",
      "Okay, I need to create a story in Thai about a cat, a dog, and a turtle. The user wants it to be engaging and suitable for children. Let me think about how to structure this.\n",
      "\n",
      "First, I should come up with a title that's catchy and relates to the three characters. Maybe something like \"Three Friends: The Tale of Miao, Huan, and Pa.\" That sounds fun and inclusive.\n",
      "\n",
      "Next, I'll introduce each character with distinct traits. Miao the cat can be curious and clever, Huan the dog loyal and brave, and Pa the turtle wise and calm. This setup allows each to have unique roles and interactions.\n",
      "\n",
      "The setting should be a village near a river where they live together. This gives a sense of community and provides a backdrop for their adventures. I'll mention their daily lives, showing how they get along despite being different animals.\n",
      "\n",
      "Now, the plot. Maybe they go on a journey together. A river is a good setting because it can lead to various challenges and discoveries. I can have them encounter a problem, like a broken bridge, which they need to solve together. This shows teamwork and highlights each animal's strengths.\n",
      "\n",
      "Miao can find a way to fix the bridge using her cleverness, Huan can provide strength, and Pa can offer wisdom. This collaboration leads to success, demonstrating the value of their friendship.\n",
      "\n",
      "I should include some Thai vocabulary to make it authentic without being too complex. Words like \"น้ำใจ\" (nayam jay) for teamwork and \"เสียงปรบมือ\" (seuang prab muay) for clapping hands can add cultural touches.\n",
      "\n",
      "Finally, I'll wrap it up with a happy ending, emphasizing the importance of friendship and teamwork. This makes the story uplifting and suitable for children.\n",
      "\n",
      "I need to ensure the language is simple and flows naturally, keeping sentences short and engaging. Let me put it all together now.\n",
      "</think>\n",
      "\n",
      "นิทานไทยเรื่อง \"สามมิตร: ตำนานของแมว、小狗 และเต่า\"\n",
      "\n",
      "---\n",
      "\n",
      "เรื่องนี้เกี่ยวกับสามมิตร: แมวชื่อ \"มiao\" (น้องมiao), 小狗ชื่อ \"หัวน\" (น้องหัวน), และเตงชื่อ \"ป่า\" (น้องป่า) ที่住ในเมืองเล็กใกล้ลำน้ำ\n",
      "\n",
      "มiao เป็นแมวที่ชื่อเสียงมากในเมืองนี้ เพราะมันคlever และชื่อเสียง มiaoชอบค้นหาและช่วยเหลือผู้คน มiaoเป็นน้องที่ให้ความช่วยเหลือมาก\n",
      "\n",
      "หัวนเป็นลูกน้องที่น่าเชื่อถือมากในเมืองนี้ เพราะมันloyal และ brave หัวนเป็นน้องที่ให้ความไว้ใจมาก\n",
      "\n",
      "ป่าเป็นเตอในเมืองนี้ เป็นน้องที่น่าเชื่อถือมาก เพราะมันwise และcalm ป่าเป็นน้องที่ให้คำแนะนำและความสงบ\n",
      "\n",
      "อย่างนัดหนึ่ง มiao, หัวน, และป่าเดินทางไปพร้อมกัน จากเมืองเล็กไปยังลำน้ำใหญ่ใกล้เมือง\n",
      "\n",
      "เมื่อถึงลำน้ำใหญ่ มันเกิดอีกวิกฤต: สิ่งล้มเหลือเกิดในสะวันน้ำ, สิ่งล้มเหลือเกิดในสะวันน้ำ, สิ่งล้มเหลือเกิดในสะวันน้ำ!\n",
      "\n",
      "มiao, หัวน, และป่าไม่สามารถข้ามลำน้ำได้ เพราะสะวันน้ำล่ม! มันเกิดอีกวิกฤต: สิ่งล้มเหลือเกิดในสะวันน้ำ!\n",
      "\n",
      "มiao, หัวน, และป่าไม่สามารถข้ามลำน้ำได้ เพราะสะวันน้ำล่ม! มันเกิดอีกวิกฤต: สิ่งล้มเหลือเกิดในสะวันน้ำ!\n",
      "\n",
      "มiao, หัวน, และป่าไม่สามารถข้ามลำน้ำได้ เพราะสะวันน้ำล่ม! มันเกิดอีกวิกฤต: สิ่งล้มเหลือเกิดในสะวันน้ำ!\n",
      "\n",
      "หัวนลุกขึ้นและช่วยเหลือ: \"มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต!\"\n",
      "\n",
      "ป่าช่วยเหลือ: \"มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต!\"\n",
      "\n",
      "มiaoคิด: \"มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต!\"\n",
      "\n",
      "ทั้งสามน้องจึงต้องคิด办法: มiaoคิด: \"มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต!\"\n",
      "\n",
      "หัวนช่วยเหลือ: \"มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต!\"\n",
      "\n",
      "ป่าช่วยเหลือ: \"มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต!\"\n",
      "\n",
      "ทั้งสามน้องจึงต้องคิด办法: มiaoคิด: \"มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต!\"\n",
      "\n",
      "หัวนช่วยเหลือ: \"มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต!\"\n",
      "\n",
      "ป่าช่วยเหลือ: \"มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต!\"\n",
      "\n",
      "ทั้งสามน้องจึงต้องคิด办法: มiaoคิด: \"มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต!\"\n",
      "\n",
      "หัวนช่วยเหลือ: \"มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต!\"\n",
      "\n",
      "ป่าช่วยเหลือ: \"มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต!\"\n",
      "\n",
      "ทั้งสามน้องจึงต้องคิด办法: มiaoคิด: \"มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต!\"\n",
      "\n",
      "หัวนช่วยเหลือ: \"มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต!\"\n",
      "\n",
      "ป่าช่วยเหลือ: \"มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต!\"\n",
      "\n",
      "ทั้งสามน้องจึงต้องคิด办法: มiaoคิด: \"มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต!\"\n",
      "\n",
      "หัวนช่วยเหลือ: \"มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต!\"\n",
      "\n",
      "ป่าช่วยเหลือ: \"มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต!\"\n",
      "\n",
      "ทั้งสามน้องจึงต้องคิด办法: มiaoคิด: \"มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต!\"\n",
      "\n",
      "หัวนช่วยเหลือ: \"มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต!\"\n",
      "\n",
      "ป่าช่วยเหลือ: \"มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต!\"\n",
      "\n",
      "ทั้งสามน้องจึงต้องคิด办法: มiaoคิด: \"มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต!\"\n",
      "\n",
      "หัวนช่วยเหลือ: \"มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต!\"\n",
      "\n",
      "ป่าช่วยเหลือ: \"มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต!\"\n",
      "\n",
      "ทั้งสามน้องจึงต้องคิด办法: มiaoคิด: \"มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต!\"\n",
      "\n",
      "หัวนช่วยเหลือ: \"มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต!\"\n",
      "\n",
      "ป่าช่วยเหลือ: \"มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต!\"\n",
      "\n",
      "ทั้งสามน้องจึงต้องคิด办法: มiaoคิด: \"มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต!\"\n",
      "\n",
      "หัวนช่วยเหลือ: \"มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต!\"\n",
      "\n",
      "ป่าช่วยเหลือ: \"มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต!\"\n",
      "\n",
      "ทั้งสามน้องจึงต้องคิด办法: มiaoคิด: \"มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต!\"\n",
      "\n",
      "หัวนช่วยเหลือ: \"มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต!\"\n",
      "\n",
      "ป่าช่วยเหลือ: \"มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต!\"\n",
      "\n",
      "ทั้งสามน้องจึงต้องคิด办法: มiaoคิด: \"มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต!\"\n",
      "\n",
      "หัวนช่วยเหลือ: \"มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต!\"\n",
      "\n",
      "ป่าช่วยเหลือ: \"มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต!\"\n",
      "\n",
      "ทั้งสามน้องจึงต้องคิด办法: มiaoคิด: \"มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต!\"\n",
      "\n",
      "หัวนช่วยเหลือ: \"มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต!\"\n",
      "\n",
      "ป่าช่วยเหลือ: \"มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต!\"\n",
      "\n",
      "ทั้งสามน้องจึงต้องคิด办法: มiaoคิด: \"มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต!\"\n",
      "\n",
      "หัวนช่วยเหลือ: \"มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต!\"\n",
      "\n",
      "ป่าช่วยเหลือ: \"มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต!\"\n",
      "\n",
      "ทั้งสามน้องจึงต้องคิด办法: มiaoคิด: \"มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต!\"\n",
      "\n",
      "หัวนช่วยเหลือ: \"มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต!\"\n",
      "\n",
      "ป่าช่วยเหลือ: \"มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต! มันเกิดอีกวิกฤต!\"\n",
      "\n",
      "ทั้งสามน้องจึงต้องคิด办法: มiaoคิด: \"มันเกิดอีกวิกฤต! มันเกิดอีกวิก\n"
     ]
    }
   ],
   "source": [
    "generate(Deepseek, messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
