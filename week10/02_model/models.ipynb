{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aKs1PM-O-VQa"
   },
   "source": [
    "# Models\n",
    "\n",
    "Looking at the lower level API of Transformers - the models that wrap PyTorch code for the transformers themselves.\n",
    "\n",
    "This notebook can run on a low-cost or free T4 runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook cleaned.\n"
     ]
    }
   ],
   "source": [
    "import IPython\n",
    "import sys\n",
    "\n",
    "def clean_notebook():\n",
    "    IPython.display.clear_output(wait=True)\n",
    "    print(\"Notebook cleaned.\")\n",
    "\n",
    "# Uninstall flash-attention (the PyPI package name may vary)\n",
    "!pip uninstall flash-attn -y\n",
    "!pip uninstall xformers -y\n",
    "\n",
    "!pip install flash-attn\n",
    "!pip install xformers\n",
    "\n",
    "\n",
    "# Clean up the notebook\n",
    "clean_notebook()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "C9zvDGWD5pKp"
   },
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, BitsAndBytesConfig\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xyKWKWSw7Iqp"
   },
   "source": [
    "# Sign in to Hugging Face\n",
    "\n",
    "1. If you haven't already done so, create a free HuggingFace account at https://huggingface.co and navigate to Settings, then Create a new API token, giving yourself write permissions\n",
    "\n",
    "2. Press the \"key\" icon on the side panel to the left, and add a new secret:\n",
    "`HF_TOKEN = your_token`\n",
    "\n",
    "3. Execute the cell below to log in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "xd7cEDUC6Lkq"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "271ed12a43964bcf92ddad81c10e7ee7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "hf_token = os.environ.get('HF_TOKEN')\n",
    "\n",
    "# Log into Hugging Face, adding credentials to git if needed\n",
    "login(hf_token, add_to_git_credential=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "UtN7OKILQato"
   },
   "outputs": [],
   "source": [
    "# instruct models\n",
    "\n",
    "LLAMA    = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "Deepseek =\"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "KgxCLBJIT5Hx"
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant answer in only thai language\"},\n",
    "    {\"role\": \"user\", \"content\": \"เล่านิทานให้ที in thai  เกี่ยวกับ แมว และ สุนัข และ เต่า\"}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZSiYqPn87msu"
   },
   "source": [
    "# Accessing Llama 3.1 from Meta\n",
    "\n",
    "In order to use the fantastic Llama 3.1, Meta does require you to sign their terms of service.\n",
    "\n",
    "Visit their model instructions page in Hugging Face:\n",
    "https://huggingface.co/meta-llama/Meta-Llama-3.1-8B\n",
    "\n",
    "At the top of the page are instructions on how to agree to their terms. If possible, you should use the same email as your huggingface account.\n",
    "\n",
    "In my experience approval comes in a couple of minutes. Once you've been approved for any 3.1 model, it applies to the whole family of models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "hhOgL1p_T6-b"
   },
   "outputs": [],
   "source": [
    "# Quantization Config - this allows us to load the model into memory and use less memory\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7MtWZYZG920F"
   },
   "source": [
    "If the next cell gives you a 403 permissions error, then please check:\n",
    "1. Are you logged in to HuggingFace? Try running `login()` to check your key works\n",
    "2. Did you set up your API key with full read and write permissions?\n",
    "3. If you visit the Llama3.1 page at https://huggingface.co/meta-llama/Meta-Llama-3.1-8B, does it show that you have access to the model near the top?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Zi8YXiwJHF59"
   },
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(LLAMA)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "S5jly421tno3"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7ee39e65773401a8408c843ae71ad8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dad8d7f2ba664fe3aa8e9f2f26a7ee34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2707a490f32416598fd09716be56875",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e4cda8467ba41eea11e3ba1e042eae2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00432ad6bde740e6bb7f38394ffd19a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0ccd9769acb42d6bc2cc34728076a00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d59da94b164b4aa98a59534ba565d39d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c0a1f55078a42fda52dce8ee7d98875",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    LLAMA, \n",
    "    device_map=\"auto\", \n",
    "    quantization_config=quant_config,\n",
    "    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "bdbYaT8hWXWE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory footprint: 5,591.5 MB\n"
     ]
    }
   ],
   "source": [
    "memory = model.get_memory_footprint() / 1e6\n",
    "print(f\"Memory footprint: {memory:,.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "P0qmAD5ZtqWA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "SkYEXzbotcud"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "You are a helpful assistant answer in only thai language<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "เล่านิทานให้ที in thai  เกี่ยวกับ แมว และ สุนัข และ เต่า<|eot_id|><|start_header_id|>assistant\n",
      "\n",
      "เล่านิทาน \"แมว สุนัข และ เต่า\"\n",
      "\n",
      "มีแมว สุนัข และ เต่าอยู่ในสวนหนึ่ง เมื่อแมวเห็นเต่ากำลังอาบน้ำ ก็รีบไปบอกสุนัขว่า \"สุนัข ฉันเห็นเต่าอยู่ที่นี่ นั่นจะไม่ดีเลย\"\n",
      "\n",
      "สุนัขรีบไปบอกเตาว่า \"เต่า ฉันเห็นแมวอยู่ที่นี่ นั่นจะไม่ดีเลย\"\n",
      "\n",
      "เตารีบไปบอกแมวว่า \"แมว ฉันเห็นสุนัขอยู่ที่นี่ นั่นจะไม่ดีเลย\"\n",
      "\n",
      "แมว สุนัข และ เต่าก็รีบไปบอกคนอื่นๆ ในสวน แต่ไม่มีใครไปบอกใครเลย\n",
      "\n",
      "ในที่สุด แมว สุนัข และ เต่าก็ไปหาคนในสวนที่กำลังทำสวน\n",
      "\n",
      "คนในสวนถามว่า \"คุณแมว คุณสุนัข และ คุณเต่า คุณมีอะไรที่จะบอกฉันหรือ?\"\n",
      "\n",
      "แมว สุนัข และ เต่าก็หันไปมองกันและตอบว่า \"ไม่มีเลย\"\n",
      "\n",
      "คนในสวนถามว่า \"ทำไมคุณแมว คุณสุนัข และ คุณเต่า จึงไม่บอกฉัน?\"\n",
      "\n",
      "แมว สุนัข และ เต่าก็หันไปมองกันและตอบว่า \"เรามีเพียงแค่ความกลัวที่จะบอกคนอื่นๆ ว่า\"\n",
      "\n",
      "คนในสวนหันไปมองพวกเขาและตอบว่า \"ความกลัวเป็นเรื่องธรรมดา แต่การบอกคนอื่นๆ ที่จะช่วยเหลือคุณนั้นเป็นเรื่องที่ดีมาก\"\n",
      "\n",
      "จากนั้น แมว สุนัข และ เต่าก็เรียนรู้ว่าการบอกคนอื่นๆ ที่จะช่วยเหลือคุณนั้นเป็นเรื่องที่ดีมาก<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(inputs, max_new_tokens=500,eos_token_id = tokenizer.eos_token_id)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "2oL0RWU2ttZf"
   },
   "outputs": [],
   "source": [
    "# Clean up\n",
    "\n",
    "del inputs, outputs, model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iDCeJ20e4Hxx"
   },
   "source": [
    "## A couple of quick notes on the next block of code:\n",
    "\n",
    "I'm using a HuggingFace utility called TextStreamer so that results stream back.\n",
    "To stream results, we simply replace:  \n",
    "`outputs = model.generate(inputs, max_new_tokens=80)`  \n",
    "With:  \n",
    "`streamer = TextStreamer(tokenizer)`  \n",
    "`outputs = model.generate(inputs, max_new_tokens=80, streamer=streamer)`\n",
    "\n",
    "Also I've added the argument `add_generation_prompt=True` to my call to create the Chat template. This ensures that Phi generates a response to the question, instead of just predicting how the user prompt continues. Try experimenting with setting this to False to see what happens. You can read about this argument here:\n",
    "\n",
    "https://huggingface.co/docs/transformers/main/en/chat_templating#what-are-generation-prompts\n",
    "\n",
    "Thank you to student Piotr B for raising the issue!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "RO_VYZ3DZ7cs"
   },
   "outputs": [],
   "source": [
    "# Wrapping everything in a function - and adding Streaming and generation prompts\n",
    "def generate(model, messages):\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Prepare inputs\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        return_tensors=\"pt\", \n",
    "        add_generation_prompt=True\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    # Create a streamer for live output (optional)\n",
    "    streamer = TextStreamer(tokenizer)\n",
    "    \n",
    "    # Load the model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model, \n",
    "        device_map=\"auto\", \n",
    "        quantization_config=quant_config\n",
    "    )\n",
    "\n",
    "    # Generate with temperature=0 and additional parameters\n",
    "    outputs = model.generate(\n",
    "        inputs,\n",
    "        max_new_tokens=2500,\n",
    "        early_stopping=True,\n",
    "        temperature=0.1,       # Deterministic behavior\n",
    "        do_sample=True,       # Typically set False if temperature=0\n",
    "        top_p=0.95,             # No nucleus sampling constraint\n",
    "        top_k=50,              # You can adjust this if needed\n",
    "        repetition_penalty=1.15, # Adjust to penalize repeated phrases\n",
    "        streamer=streamer ,     # Remove if you don't need live output\n",
    "        eos_token_id = tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    # Clean up\n",
    "    del tokenizer, streamer, model, inputs, outputs\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "RFjaY4Pdvbfy"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9239c26825c44c681038bc487ba5b24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:676: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "You are a helpful assistant answer in only thai language<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "เล่านิทานให้ที in thai  เกี่ยวกับ แมว และ สุนัข และ เต่า<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "มีนิทานหนึ่งเกี่ยวกับแมว สุนัข และเต่าที่อยู่ในสวนใหญ่ มีแมวที่ชื่อว่า \"เมีย\" เป็นแมวสีขาวที่ชอบพักผ่อนในดอกไม้ มีสุนัขที่ชื่อว่า \"ตุ๊ก\" เป็นสุนัขขนาดกลางที่ชอบเล่นกับลูกบอล และมีเตาที่ชื่อว่า \"โต้ง\" เป็นเตาที่ชอบอาบแดดบนหิน\n",
      "\n",
      "เมีย ตุ๊ก และ โต้ง อยู่ร่วมกันในสวนนี้ แต่มีความแตกต่างกันมาก เมียชอบพักผ่อนในสวนทุกวัน ตุ๊กชอบเล่นกับลูกบอลจนเหนื่อยโยน และ โต้งชอบอาบแดดนอนหลับไปมา\n",
      "\n",
      "วันนึง พวกเขาได้รู้จักกับเด็กๆ ที่มาเยี่ยมสวน เด็กๆ นั้นมีชุดใหม่ที่สวยงามและสนุกสนาน เมีย ตุ๊ก และ โต้ง ล้วนต้องการเป็นเพื่อนของเด็กๆ เพราะเห็นว่ามีเด็กๆ น่ารักอยู่ใกล้พวกเขา\n",
      "\n",
      "เมีย ตุ๊ก และ โต้ง จึงพยายามทำความสะอาดและเตรียมตัวให้พร้อม เพื่อให้เด็กๆ ชอบพวกเขา เมีย หยิบเศษใบไม้ออกจากผม ตุ๊ก อาบน้ำเพื่อให้ผิวเรียบเนียน และ โต้ง อาบแดดเพื่อให้ตัวแห้งกรุ่น\n",
      "\n",
      "เมื่อเด็กๆ ยิ้มและเล่นกับพวกเขา พวกเขาก็รู้สึกภูมิใจและความรักต่อเด็กๆ พวกเขาได้เป็นเพื่อนกับเด็กๆ และได้เพลิดเพลินไปกับการเล่นและสำรวจสวน\n",
      "\n",
      "ในที่สุด เมีย ตุ๊ก และ โต้ง ก็กลับมาพบปะกับเด็กๆ ในสวน พวกเขารู้สึกภูมิใจและความรักที่มีต่อเด็กๆ และพวกเขาจะอยู่ร่วมกันเป็นเพื่อนตลอดไป<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "generate(LLAMA, messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "0m8yjMB3ZTp3"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d72da51f1aae46b58a5e70fc8914d32f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/3.06k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3179dc1a0f43497ba07235eee0dbb18d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c64903fa3784c3a838b3ccc3e9271f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/826 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4af1ae7bc70b465ab0ca4e5b21e85d48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/24.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "febda5804799468ca28019b566d650e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e17447897e8a425bb98f1141067d4d9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-000002.safetensors:   0%|          | 0.00/8.67G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "020c1fc771f745e798ef5bc5a18f5ee5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-000002.safetensors:   0%|          | 0.00/7.39G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "475e276478b8432ab353af7796a5fb7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ff0c473195b44b689b4474cbf813ba9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜>You are a helpful assistant answer in only thai language<｜User｜>เล่านิทานให้ที in thai  เกี่ยวกับ แมว และ สุนัข และ เต่า<｜Assistant｜><think>\n",
      "Okay, I need to create a story about animals for a Thai-speaking audience. The user wants it to be engaging and educational, focusing on cats, dogs, and turtles. Let me think about how to structure this.\n",
      "\n",
      "First, maybe start with introducing each animal separately. That way, the reader can get to know each one before they meet them in the story. So, describe a cat, then a dog, then a turtle.\n",
      "\n",
      "Next, set up a scene where these three characters come together. Perhaps they live in a village near a river or forest. This will give a natural setting that's familiar to many people.\n",
      "\n",
      "Then, develop a simple plot. Maybe the cat is curious, the dog is friendly, and the turtle is wise. They could go on an adventure together, like searching for something lost or helping someone in need. This adds a sense of teamwork and cooperation.\n",
      "\n",
      "I should include some elements of Thai culture, like respecting nature or helping neighbors. It’s important to show kindness and community spirit, which are valued in Thai society.\n",
      "\n",
      "The story should have a happy ending, showing how their different abilities make them good friends. Emphasize the message that everyone has something special to contribute.\n",
      "\n",
      "Make sure the language is simple and easy to understand, using common words so it resonates well with readers. Keep paragraphs short to maintain interest.\n",
      "\n",
      "Finally, wrap it up by reflecting on the moral or lesson learned from the story. Encourage empathy towards all living creatures.\n",
      "</think>\n",
      "\n",
      "นิทานไทย: \"สามสัตว์ผู้ช่วยเมือง\"\n",
      "\n",
      "อย่างนัดนี, ในหมู่บ้านท้องนา มีหนึ่งแมว่น้อย, หนึ่งสุนัขอีก一个, และเต่ายาวนิ่ม. แฟนเกิดในครอบครัวที่รักและคุ้มครอง, พวกเขามีอาชีพตัวเอง: แมว่น้อยค้นหาของ丢失, สุนัขคุ้มครองบ้าน, เต่ายาวนิ่มค้าขายปากู๋ปากู๋.\n",
      "\n",
      "เมื่อหนึงวัน, แมว่น้อยเห็นชายฝั่งลำคลายมีหีบกระโถนลอยนอก, ตัดสินจะตรวจสอบ! สุนัขได้ตามมาเป็นผู้ช่วย, และเตังซึ่งกำลังเดินทางไปตลาด, ก็เข้าร่วมด้วย. พวกเขาจoin forces เป็นสquad ที่จะไขความเท่หิงใจนี้!\n",
      "\n",
      "หลังจากตรวจสอบรอบๆ รอบ, พวกเขาพบเจ้ากรรมτικาย! หีบกระโถนไม่มีผู้สูงอายุ nowhere, แต่ในตอนกลางคืน, ลูกาเงินกับเพื่อน ๆ ของเขาได้พบกับสัตว์เหลือนที่น่ารัก: หมู, กระท่อม, และนกนางแสน. พวกเขาสนับสนุนกันและกันตั้งแต่นัดนี้!\n",
      "\n",
      "จากนั้นมาง�วาลึม! พวกเขาจะกลายเป็นนักสือสัตว์ยอดเยี่ยม, ช่วยเหลือสัตว์เสียหาย, และคุ้มครองธรรมชาติ. พวกเขาจะเป็นที่นิยมอีกต่อไป, เพราะพวกเขาใช้พลังของตนให้ดีที่สุด!\n",
      "\n",
      "บทเรียน: ใครไม่สามารถช่วยเหลือคนหรือสัตว์ใกล้เคียงตัวนั้น? บางคนอาจสงสัยว่ามีวิธีใด้จริง, หรือว่ามีการช่วยเหลือที่เหมาะสม. แต่หากคุณมีแรงศรiramjeeviya (ความปรารถนาดูola) และความพยายาม, คุณสามารถทำอะไรก็ตามที่คุณเชื่อ.<｜end▁of▁sentence｜>\n"
     ]
    }
   ],
   "source": [
    "generate(Deepseek, messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
