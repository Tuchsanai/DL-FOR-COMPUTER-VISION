{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aKs1PM-O-VQa"
   },
   "source": [
    "# Models\n",
    "\n",
    "Looking at the lower level API of Transformers - the models that wrap PyTorch code for the transformers themselves.\n",
    "\n",
    "This notebook can run on a low-cost or free T4 runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "import sys\n",
    "\n",
    "def clean_notebook():\n",
    "    IPython.display.clear_output(wait=True)\n",
    "    print(\"Notebook cleaned.\")\n",
    "\n",
    "# Uninstall flash-attention (the PyPI package name may vary)\n",
    "!pip uninstall flash-attn -y\n",
    "!pip uninstall xformers -y\n",
    "\n",
    "!pip install flash-attn\n",
    "!pip install xformers\n",
    "\n",
    "\n",
    "# Clean up the notebook\n",
    "clean_notebook()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "C9zvDGWD5pKp"
   },
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, BitsAndBytesConfig\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xyKWKWSw7Iqp"
   },
   "source": [
    "# Sign in to Hugging Face\n",
    "\n",
    "1. If you haven't already done so, create a free HuggingFace account at https://huggingface.co and navigate to Settings, then Create a new API token, giving yourself write permissions\n",
    "\n",
    "2. Press the \"key\" icon on the side panel to the left, and add a new secret:\n",
    "`HF_TOKEN = your_token`\n",
    "\n",
    "3. Execute the cell below to log in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xd7cEDUC6Lkq"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f85d224d45d743b69c5cd9855ae115e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "hf_token = os.environ.get('HF_TOKEN')\n",
    "\n",
    "\n",
    "# Log into Hugging Face, adding credentials to git if needed\n",
    "login(hf_token, add_to_git_credential=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "UtN7OKILQato"
   },
   "outputs": [],
   "source": [
    "# instruct models\n",
    "\n",
    "LLAMA = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "KgxCLBJIT5Hx"
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"เล่านิทานให้ที\"}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZSiYqPn87msu"
   },
   "source": [
    "# Accessing Llama 3.1 from Meta\n",
    "\n",
    "In order to use the fantastic Llama 3.1, Meta does require you to sign their terms of service.\n",
    "\n",
    "Visit their model instructions page in Hugging Face:\n",
    "https://huggingface.co/meta-llama/Meta-Llama-3.1-8B\n",
    "\n",
    "At the top of the page are instructions on how to agree to their terms. If possible, you should use the same email as your huggingface account.\n",
    "\n",
    "In my experience approval comes in a couple of minutes. Once you've been approved for any 3.1 model, it applies to the whole family of models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "hhOgL1p_T6-b"
   },
   "outputs": [],
   "source": [
    "# Quantization Config - this allows us to load the model into memory and use less memory\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7MtWZYZG920F"
   },
   "source": [
    "If the next cell gives you a 403 permissions error, then please check:\n",
    "1. Are you logged in to HuggingFace? Try running `login()` to check your key works\n",
    "2. Did you set up your API key with full read and write permissions?\n",
    "3. If you visit the Llama3.1 page at https://huggingface.co/meta-llama/Meta-Llama-3.1-8B, does it show that you have access to the model near the top?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Zi8YXiwJHF59"
   },
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(LLAMA)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "S5jly421tno3"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a585d0dc8f949e4b22eaef0eadfd0aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    LLAMA, \n",
    "    device_map=\"auto\", \n",
    "    quantization_config=quant_config,\n",
    "    \n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "bdbYaT8hWXWE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory footprint: 5,591.5 MB\n"
     ]
    }
   ],
   "source": [
    "memory = model.get_memory_footprint() / 1e6\n",
    "print(f\"Memory footprint: {memory:,.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "P0qmAD5ZtqWA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "SkYEXzbotcud"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "You are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "เล่านิทานให้ที<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "นิทานที่ฉันจะเล่าให้คุณฟังวันนี้คือเรื่อง \"หมูสามตัว\"\n",
      "\n",
      "มีสามตัวหมูอยู่ในสวนหมู พวกมันชอบเล่นกันและกินอาหารได้เยอะ แต่หมูหนึ่งตัวมีชีวิตที่ไม่เหมือนกับหมูที่เหลือ มันชอบไปเดินเล่นในสวนและดูพืชพันธุ์ต่างๆ มากกว่าที่จะกินอาหาร\n",
      "\n",
      "หนึ่งวัน หมูสามตัวนี้ไปพบกับหมูอื่นๆ ที่กำลังเล่นอยู่ในสวน \"ทำไมคุณไม่ไปกินอาหาร?\" หมูอื่นๆ ถาม \"อาหารมันเยอะมากที่นี่\" แต่หมูหนึ่งตัวนั้นไม่สนใจ \"ฉันอยากไปดูพืชพันธุ์ต่างๆ มากกว่าที่จะกินอาหาร\"\n",
      "\n",
      "หมูอื่นๆ ก็ไม่เข้าใจว่าทำไมหมูหนึ่งตัวจะไม่ชอบกินอาหาร แต่พวกมันก็ไม่ได้บังคับหมูหนึ่งตัวให้ทำตามพวกมัน\n",
      "\n",
      "วันเวลาผ่านไปและหมูสามตัวนี้ก็โตขึ้นและแข็งแรงขึ้น แต่หมูหนึ่งตัวนั้นไม่เคยเปลี่ยนแปลงความชอบของมัน \"ฉันชอบไปดูพืชพันธุ์ต่างๆ มากกว่าที่จะกินอาหาร\" มันพูด\n",
      "\n",
      "สุดท้าย หมูสามตัวนี้ก็อายุได้ 10 ปี และหมูหนึ่งตัวนั้นยังคงอยู่ในสวนและดูพืชพันธุ์ต่างๆ อย่างไม่เปลี่ยนแปลง\n",
      "\n",
      "เรื่องนี้บอกเราว่า \"แต่ละคนมีความชอบและความสามารถที่ไม่เหมือนกัน\" และไม่ควรบังคับคนอื่นๆ ให้ทำตามเรา<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(inputs, max_new_tokens=500)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "2oL0RWU2ttZf"
   },
   "outputs": [],
   "source": [
    "# Clean up\n",
    "\n",
    "del inputs, outputs, model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iDCeJ20e4Hxx"
   },
   "source": [
    "## A couple of quick notes on the next block of code:\n",
    "\n",
    "I'm using a HuggingFace utility called TextStreamer so that results stream back.\n",
    "To stream results, we simply replace:  \n",
    "`outputs = model.generate(inputs, max_new_tokens=80)`  \n",
    "With:  \n",
    "`streamer = TextStreamer(tokenizer)`  \n",
    "`outputs = model.generate(inputs, max_new_tokens=80, streamer=streamer)`\n",
    "\n",
    "Also I've added the argument `add_generation_prompt=True` to my call to create the Chat template. This ensures that Phi generates a response to the question, instead of just predicting how the user prompt continues. Try experimenting with setting this to False to see what happens. You can read about this argument here:\n",
    "\n",
    "https://huggingface.co/docs/transformers/main/en/chat_templating#what-are-generation-prompts\n",
    "\n",
    "Thank you to student Piotr B for raising the issue!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "RO_VYZ3DZ7cs"
   },
   "outputs": [],
   "source": [
    "# Wrapping everything in a function - and adding Streaming and generation prompts\n",
    "\n",
    "def generate(model, messages):\n",
    "  tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "  tokenizer.pad_token = tokenizer.eos_token\n",
    "  inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True).to(\"cuda\")\n",
    "  streamer = TextStreamer(tokenizer)\n",
    "  model = AutoModelForCausalLM.from_pretrained(model, device_map=\"auto\", quantization_config=quant_config)\n",
    "  outputs = model.generate(inputs, max_new_tokens=500, streamer=streamer)\n",
    "  del tokenizer, streamer, model, inputs, outputs\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "RFjaY4Pdvbfy"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e202b567e3c44a4b0a0177685dfddd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "You are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "เล่านิทานให้ที<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "นิทานของฉันในวันนี้คือ \"กุหลาบสีแดง\"\n",
      "\n",
      "มีชายหนุ่มหนึ่งคนชื่อ ชายหนุ่มคนนี้อยากได้ความรัก แต่เขาทราบว่าความรักที่แท้จริงไม่ใช่ความรักที่มาจากความรู้สึกหรือความอยาก แต่เป็นความรักที่มาจากความมั่นคงและความไว้วางใจ\n",
      "\n",
      "หนึ่งวัน ชายหนุ่มคนนี้ไปเที่ยวในสวน และพบกับกุหลาบสีแดง ชายหนุ่มคนนี้รู้สึกประทับใจกับกุหลาบสีแดงและอยากได้ความรักเหมือนกับกุหลาบสีแดง\n",
      "\n",
      "แต่เมื่อชายหนุ่มคนนี้ใกล้ชิดกับกุหลาบสีแดง เขาได้เห็นข้อจำกัดของกุหลาบสีแดง กุหลาบสีแดงไม่สามารถอยู่ในสวนได้ตลอดเวลา และต้องอยู่ในความเย็นเพื่อให้ไม่สุก\n",
      "\n",
      "ชายหนุ่มคนนี้รู้สึกประหลาดใจและสงสัยว่าความรักที่แท้จริงคืออะไร เขาได้พิจารณาว่าความรักที่แท้จริงคือความรักที่ไม่สุกและไม่เสียหาย แต่ก็ไม่สามารถอยู่ในสวนได้ตลอดเวลา\n",
      "\n",
      "และเมื่อชายหนุ่มคนนี้กลับมาจากสวน เขาได้พบกับความรักที่แท้จริง ซึ่งเป็นความรักที่มั่นคงและไว้วางใจ และไม่สุกหรือเสียหาย\n",
      "\n",
      "นิทาน \"กุหลาบสีแดง\" บอกเราว่าความรักที่แท้จริงไม่ใช่ความรักที่มาจากความรู้สึกหรือความอยาก แต่เป็นความรักที่มั่นคงและไว้วางใจ และไม่สุกหรือเสียหาย<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "generate(LLAMA, messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0m8yjMB3ZTp3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
