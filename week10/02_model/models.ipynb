{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aKs1PM-O-VQa"
   },
   "source": [
    "# Models\n",
    "\n",
    "Looking at the lower level API of Transformers - the models that wrap PyTorch code for the transformers themselves.\n",
    "\n",
    "This notebook can run on a low-cost or free T4 runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook cleaned.\n"
     ]
    }
   ],
   "source": [
    "import IPython\n",
    "import sys\n",
    "\n",
    "def clean_notebook():\n",
    "    IPython.display.clear_output(wait=True)\n",
    "    print(\"Notebook cleaned.\")\n",
    "\n",
    "# Uninstall flash-attention (the PyPI package name may vary)\n",
    "!pip uninstall flash-attn -y\n",
    "!pip uninstall xformers -y\n",
    "\n",
    "!pip install flash-attn\n",
    "!pip install xformers\n",
    "\n",
    "\n",
    "# Clean up the notebook\n",
    "clean_notebook()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "C9zvDGWD5pKp"
   },
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, BitsAndBytesConfig\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xyKWKWSw7Iqp"
   },
   "source": [
    "# Sign in to Hugging Face\n",
    "\n",
    "1. If you haven't already done so, create a free HuggingFace account at https://huggingface.co and navigate to Settings, then Create a new API token, giving yourself write permissions\n",
    "\n",
    "2. Press the \"key\" icon on the side panel to the left, and add a new secret:\n",
    "`HF_TOKEN = your_token`\n",
    "\n",
    "3. Execute the cell below to log in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "xd7cEDUC6Lkq"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81bdb5b501a54c628bdfe14b94f672da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "hf_token = os.environ.get('HF_TOKEN')\n",
    "\n",
    "# Log into Hugging Face, adding credentials to git if needed\n",
    "login(hf_token, add_to_git_credential=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "UtN7OKILQato"
   },
   "outputs": [],
   "source": [
    "# instruct models\n",
    "\n",
    "LLAMA    = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "Deepseek =\"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "KgxCLBJIT5Hx"
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant answer in only thai language\"},\n",
    "    {\"role\": \"user\", \"content\": \"เล่านิทานให้ที in thai  เกี่ยวกับ แมว และ สุนัข และ เต่า\"}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZSiYqPn87msu"
   },
   "source": [
    "# Accessing Llama 3.1 from Meta\n",
    "\n",
    "In order to use the fantastic Llama 3.1, Meta does require you to sign their terms of service.\n",
    "\n",
    "Visit their model instructions page in Hugging Face:\n",
    "https://huggingface.co/meta-llama/Meta-Llama-3.1-8B\n",
    "\n",
    "At the top of the page are instructions on how to agree to their terms. If possible, you should use the same email as your huggingface account.\n",
    "\n",
    "In my experience approval comes in a couple of minutes. Once you've been approved for any 3.1 model, it applies to the whole family of models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "hhOgL1p_T6-b"
   },
   "outputs": [],
   "source": [
    "# Quantization Config - this allows us to load the model into memory and use less memory\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7MtWZYZG920F"
   },
   "source": [
    "If the next cell gives you a 403 permissions error, then please check:\n",
    "1. Are you logged in to HuggingFace? Try running `login()` to check your key works\n",
    "2. Did you set up your API key with full read and write permissions?\n",
    "3. If you visit the Llama3.1 page at https://huggingface.co/meta-llama/Meta-Llama-3.1-8B, does it show that you have access to the model near the top?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Zi8YXiwJHF59"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b7b546382ab4c1fa6c5b101d485acc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64fb9559c36b4ca391cb922e9048e7c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c55397c8230444afa089fc423f4d0ee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(LLAMA)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "S5jly421tno3"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "173d9a5e25af46e4a0efbde58aca0b2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    LLAMA, \n",
    "    device_map=\"auto\", \n",
    "    quantization_config=quant_config,\n",
    "    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "bdbYaT8hWXWE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory footprint: 5,591.5 MB\n"
     ]
    }
   ],
   "source": [
    "memory = model.get_memory_footprint() / 1e6\n",
    "print(f\"Memory footprint: {memory:,.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "P0qmAD5ZtqWA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "SkYEXzbotcud"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "You are a helpful assistant answer in only thai language<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "เล่านิทานให้ที in thai  เกี่ยวกับ แมว และ สุนัข และ เต่า<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "นิทาน \"แมว สุนัข และ เต่า\"\n",
      "\n",
      "มีแมว สุนัข และ เต่า 3 ตัว ที่อยู่ในสวนใหญ่ พวกมันคิดว่าตัวเองมีคุณสมบัติที่ดีที่สุดในโลก\n",
      "\n",
      "แมวคิดว่าตัวเองมีความสามารถในการวิ่งเร็วและกระโดดสูง สุนัขคิดว่าตัวเองมีความสามารถในการวิ่งเร็วและหามนุษย์ให้พบ ส่วนเต่าคิดว่าตัวเองมีความสามารถในการว่ายน้ำและอยู่ในน้ำได้นาน\n",
      "\n",
      "วันหนึ่ง พวกมันพบกันในสวนและพูดคุยกัน แมวพูดว่า \"ฉันคิดว่าตัวเองมีความสามารถที่ดีที่สุดในโลก\" สุนัขพูดว่า \"ฉันคิดว่าตัวเองมีความสามารถที่ดีที่สุดในโลก\" เต่าพูดว่า \"ฉันคิดว่าตัวเองมีความสามารถที่ดีทสุดในโลก\"\n",
      "\n",
      "พวกมันกังวลและไม่พอใจกับคำพูดของตัวเอง พวกมันจึงตัดสินใจที่จะไปพบกับพญานาค เพื่อขอคำปรึกษาพวกมันไปพบกับพญานาคและพูดว่า \"พญานาค เราคิดว่าตัวเองมีความสามารถที่ดีที่สุดในโลก แต่เรากังวลว่าเราอาจไม่มีความสามารถที่ดีที่สุดจริงๆ\"\n",
      "\n",
      "พญานาคฟังคำพูดของพวกมันแล้วพูดว่า \"แมว สุนัข และ เต่า คุณคิดว่าตัวเองมีความสามารถที่ดีที่สุดในโลก แต่คุณไม่เคยคิดว่าคุณมีความสามารถที่ดีในหลายๆ ด้าน คุณมีความสามารถในการวิ่งเร็ว คุณมีความสามารถในการว่ายน้ำ คุณมีความสามารถในการหามนุษย์ให้พบ คุณมีความสามารถในการอยู่ในน้ำได้นาน คุณมีความสามารถในการวิ่งเร็วและกระโดดสูง คุณมีความ\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(inputs, max_new_tokens=500,eos_token_id = tokenizer.eos_token_id)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "2oL0RWU2ttZf"
   },
   "outputs": [],
   "source": [
    "# Clean up\n",
    "\n",
    "del inputs, outputs, model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iDCeJ20e4Hxx"
   },
   "source": [
    "## A couple of quick notes on the next block of code:\n",
    "\n",
    "I'm using a HuggingFace utility called TextStreamer so that results stream back.\n",
    "To stream results, we simply replace:  \n",
    "`outputs = model.generate(inputs, max_new_tokens=80)`  \n",
    "With:  \n",
    "`streamer = TextStreamer(tokenizer)`  \n",
    "`outputs = model.generate(inputs, max_new_tokens=80, streamer=streamer)`\n",
    "\n",
    "Also I've added the argument `add_generation_prompt=True` to my call to create the Chat template. This ensures that Phi generates a response to the question, instead of just predicting how the user prompt continues. Try experimenting with setting this to False to see what happens. You can read about this argument here:\n",
    "\n",
    "https://huggingface.co/docs/transformers/main/en/chat_templating#what-are-generation-prompts\n",
    "\n",
    "Thank you to student Piotr B for raising the issue!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "RO_VYZ3DZ7cs"
   },
   "outputs": [],
   "source": [
    "# Wrapping everything in a function - and adding Streaming and generation prompts\n",
    "def generate(model, messages):\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Prepare inputs\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        return_tensors=\"pt\", \n",
    "        add_generation_prompt=True\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    # Create a streamer for live output (optional)\n",
    "    streamer = TextStreamer(tokenizer)\n",
    "    \n",
    "    # Load the model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model, \n",
    "        device_map=\"auto\", \n",
    "        quantization_config=quant_config\n",
    "    )\n",
    "\n",
    "    # Generate with temperature=0 and additional parameters\n",
    "    outputs = model.generate(\n",
    "        inputs,\n",
    "        max_new_tokens=5500,\n",
    "        early_stopping=True,\n",
    "        temperature=0.1,       # Deterministic behavior\n",
    "        do_sample=True,       # Typically set False if temperature=0\n",
    "        top_p=0.95,             # No nucleus sampling constraint\n",
    "        top_k=50,              # You can adjust this if needed\n",
    "        repetition_penalty=1.15, # Adjust to penalize repeated phrases\n",
    "        streamer=streamer ,     # Remove if you don't need live output\n",
    "        eos_token_id = tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    # Clean up\n",
    "    del tokenizer, streamer, model, inputs, outputs\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "RFjaY4Pdvbfy"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e52597e9845f4c359fb3367e815e10cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/transformers/src/transformers/generation/configuration_utils.py:674: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "You are a helpful assistant answer in only thai language<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "เล่านิทานให้ที in thai  เกี่ยวกับ แมว และ สุนัข และ เต่า<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "มีนิทานหนึ่งเกี่ยวกับแมว สุนัข และเต่าที่อยู่ในสวนใหญ่ มีแมวที่ชื่อว่า \"เมีย\" เป็นแมวสีขาวที่ชอบพักผ่อนในทุ่งนา มีสุนัขที่ชื่อว่า \"โต้ง\" เป็นสุนัขขนาดใหญ่ที่ชอบเล่นกับลูกบอล และมีเตาที่ชื่อว่า \"ตวง\" เป็นเตาที่ชอบอาบแดดบนหินร้อน\n",
      "\n",
      "เมียและโต้งเป็นเพื่อนสนิท แต่ก็ไม่เคยพบปะกับตวงเพราะตวงชอบอยู่คนเดียว เมื่อหนึ่งครั้ง โต้งไปหาเมียที่กำลังพักผ่อนในทุ่นา เขาพบว่าตวงกำลังอาบแดดบนหินร้อน โต้งจึงเข้าใกล้ตวงและเรียก \"เฮ้ ตวง!\" แต่ตวงไม่ได้ยินเลย เพราะกำลังหลับให้สบาย\n",
      "\n",
      "เมียเห็นตวงกำลังหลับ จึงเข้ามาใกล้และเรียก \"เฮ้ ตวง! คุณหลับไปแล้วหรือ?\" ตวงได้ยินเสียงของเมียและ睜ตามองรอบๆ แล้วเห็นเมียและสุนัข โต้ง เธอก็ยิ้มและบอก \"ฉันหลับไปแล้ว ฉันไม่รู้ว่าคุณสองคนทำอะไร\"\n",
      "\n",
      "เมียและโต้งจึงแนะนำตัวกับตวงและพูดคุยกัน พวกเขาได้รู้จักกันและกลายเป็นเพื่อนที่ดีต่อกัน จากนั้น เมีย โต้ง และตวง ก็มักจะออกไปเล่นและพักผ่อนด้วยกัน<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "generate(LLAMA, messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "0m8yjMB3ZTp3"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e2b4d1d6cbe452981c0ed0200088eb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜>You are a helpful assistant answer in only thai language<｜User｜>เล่านิทานให้ที in thai  เกี่ยวกับ แมว และ สุนัข และ เต่า<｜Assistant｜><think>\n",
      "Okay, I need to create a story about animals—cat, dog, and turtle. The user wants it in Thai, so I should make sure the translation is accurate.\n",
      "\n",
      "First, let me think of a title that's catchy and relevant. Maybe \"นิทานสัตว์\" which means animal fables or tales.\n",
      "\n",
      "Next, I'll outline the plot. Perhaps a cat who loves milk but can't reach it, meets a dog who can open jars, and together they find help from a wise turtle.\n",
      "\n",
      "I should include some moral or lesson, like teamwork and using one's strengths.\n",
      "\n",
      "Now, translating each part into Thai while keeping the flow natural is important. Let me ensure the sentences make sense and sound good in Thai.\n",
      "\n",
      "Finally, I'll review the entire story to check for any errors or awkward phrases.\n",
      "</think>\n",
      "\n",
      "นิทานสัตว์\n",
      "\n",
      "มีหนูจุฑาที่ชื่อ \"ตัวจัน\" เป็นแมวที่รักเมาส์ แต่ไม่สามารถ 达到ถ้วยเมาส์ที่ตั้งบนเสื้อแขวนในห้องครัวของเขาได้ เพราะเขามือ малаเกินไป。หนูก็อดีตัวเองอย่างน่าสนใจ。\n",
      "\n",
      "หนึ่eday หนูตัวจันกำลังหาผู้คน帮助 ในบ้าน他พบกับสุนัขชื่อ \"ตัวปี้\" ที่เป็นสุนัขใหญ่น่าเกรียณ์ กับเต่ายาวชื่อ \"ตัวทอง\" ที่ดูเหมือนเตาแบน้ำแข็งแต่มีมูลค่ามาก。\n",
      "\n",
      "ตัวปี้แนะนำตัวจันเป็นลิaristhinking: \"หากคุณต้องการ 达到的omething คุณสามารถใช้แรงงานของคุณและของของคุณเพื่อความสำเร็จของคุณ\"\n",
      "\n",
      "ตัวทอง则ช่วยเหลือโดยบอกว่า \"ฉันอาจจะช่วยเหลือคุณด้วยความรู้ของฉันเกี่ยวกับการเปิดประทาร์ค่ะ\"\n",
      "\n",
      "ด้วยความช่วยเหลือจากตัวปี้และตัวทอง ตัวจันได้รับคำแนะนำในการใช้ลasso ของตัวเองเพื่อให้ 达到的omething มาสู่ตนเอง\n",
      "\n",
      "ในท้ายทTAILs ตัวจันเรียนรู้ значенияของteamwork และการใช้สมartsense ของตัวเองเพื่อความสำเร็จ\n",
      "\n",
      "มorallesson: วิทยากรรมะณิกายoga คุณสามารถบรรลุความสำเร็จของคุณได้ด้วยการทำงานร่วมกันและใช้ประโยชน์ของคุณเอง<｜end▁of▁sentence｜>\n"
     ]
    }
   ],
   "source": [
    "generate(Deepseek, messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
