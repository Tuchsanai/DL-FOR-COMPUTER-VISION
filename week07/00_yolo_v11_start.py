# ---
# jupyter:
#   jupytext:
#     formats: ipynb,py:percent
#     text_representation:
#       extension: .py
#       format_name: percent
#       format_version: '1.3'
#       jupytext_version: 1.19.1
#   kernelspec:
#     display_name: Python 3 (ipykernel)
#     language: python
#     name: python3
# ---

# %% [markdown]
# # üî¨ Lab: YOLO26 ‚Äî Real-Time Computer Vision Inference (Reorganized)
#
# **‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå (Objectives):**
# - ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á YOLO26 ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏à‡∏≤‡∏Å YOLO11
# - ‡∏ù‡∏∂‡∏Å‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô YOLO26 ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö tasks ‡∏ï‡πà‡∏≤‡∏á‡πÜ: Detection, Segmentation, Pose Estimation, Classification
# - ‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á parameters ‡∏ï‡πà‡∏≤‡∏á‡πÜ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à speed-accuracy tradeoff
# - ‡∏õ‡∏£‡∏∞‡∏¢‡∏∏‡∏Å‡∏ï‡πå‡πÉ‡∏ä‡πâ Segmentation Mask ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Background Removal, Blur Object
# - ‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì‡∏£‡∏∞‡∏¢‡∏∞‡∏´‡πà‡∏≤‡∏á‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏ (Object Distance Estimation)
# - ‡∏ó‡∏î‡∏•‡∏≠‡∏á Object Tracking ‡∏ö‡∏ô video
#
# **‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ (Tools):** Python, Ultralytics, OpenCV, Matplotlib, NumPy
#
# ---
#
# ## üìñ Background: YOLO26 ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?
#
# **YOLO26** (Released September 2025) ‡πÄ‡∏õ‡πá‡∏ô YOLO ‡∏£‡∏∏‡πà‡∏ô‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏à‡∏≤‡∏Å Ultralytics
#
# | Feature | YOLO11 (2024) | YOLO26 (2025) |
# |---------|---------------|---------------|
# | NMS | ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ NMS post-processing | **NMS-Free** end-to-end |
# | DFL | ‡πÉ‡∏ä‡πâ DFL | **‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ DFL** ‚Äî ‡∏•‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô |
# | CPU Speed | Baseline | **‡πÄ‡∏£‡πá‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô ~43%** |
# | Optimizer | SGD / Adam | **MuSGD** (Hybrid SGD + Muon) |
# | Head | Single head | **Dual-head** (One-to-One / One-to-Many) |
#
# **Model Variants:** `yolo26n` (Nano), `yolo26s` (Small), `yolo26m` (Medium), `yolo26l` (Large), `yolo26x` (Extra Large)
#
# ---
#
# ## üìö Lab Structure (‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ï‡∏≤‡∏°‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ)
#
# | Group | ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠ | Labs |
# |-------|-------|------|
# | **Group 1** | Setup & Detection Basics | Lab 1‚Äì4 |
# | **Group 2** | Model Tuning & Optimization | Lab 5‚Äì8 |
# | **Group 3** | Detection Applications | Lab 9‚Äì11 |
# | **Group 4** | Segmentation & Mask Applications | Lab 12‚Äì15 |
# | **Group 5** | Pose Estimation | Lab 16‚Äì18 |
# | **Group 6** | Image Classification | Lab 19 |
# | **Group 7** | Distance Estimation & Tracking | Lab 20‚Äì23 |

# %% [markdown]
# # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# # Group 1: Setup & Detection Basics (Lab 1‚Äì4)
# # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

# %% [markdown]
# ## Lab 1: Installation & Setup
# ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á library ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô

# %%
import IPython
import sys

def clean_notebook():
    IPython.display.clear_output(wait=True)
    print("‚úÖ Notebook cleaned. Ready to go!")

# ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á packages (uncomment ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á)
# !uv pip install ultralytics
clean_notebook()

# %%
# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Ultralytics version
import ultralytics
ultralytics.checks()

# %% [markdown]
# ---
# ## Lab 2: Basic Object Detection + Results Object
#
# ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ YOLO26 ‡∏ó‡∏≥ Object Detection ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô task ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
# ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏à‡∏∞ predict **bounding box** + **class label** + **confidence score**
#
# ### 2.1 Basic Detection with YOLO26

# %%
from ultralytics import YOLO

# ‡πÇ‡∏´‡∏•‡∏î YOLO26 nano model (pretrained ‡∏ö‡∏ô COCO dataset ‚Äî 80 classes)
model = YOLO("yolo26n.pt")

# ‡∏ó‡∏≥ inference ‡∏ö‡∏ô‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û
IMAGE_PATH = ".././images/football_teamplay.jpeg"
results = model(IMAGE_PATH, imgsz=640)

# ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
results[0].show()

# %% [markdown]
# ### 2.2 ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à Results Object
#
# ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å YOLO ‡∏à‡∏∞‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô `Results` object ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏≤‡∏¢‡∏™‡πà‡∏ß‡∏ô

# %%

# %%
# ‡∏™‡∏≥‡∏£‡∏ß‡∏à result object
result = results[0]

print("=" * 60)
print("üìä Detection Results Summary")
print("=" * 60)
print(f"‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡πÑ‡∏î‡πâ (Objects detected): {len(result.boxes)}")
print(f"Original image shape: {result.orig_shape}")
print(f"Model speed: {result.speed}")
print()

# ‡πÅ‡∏™‡∏î‡∏á‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ detection
for i, box in enumerate(result.boxes):
    class_id = int(box.cls[0])
    class_name = model.names[class_id]
    confidence = float(box.conf[0])
    x1, y1, x2, y2 = map(int, box.xyxy[0])
    
    print(f"  Object {i+1}: {class_name} (conf: {confidence:.2f}) | bbox: ({x1}, {y1}, {x2}, {y2})")

# %% [markdown]
# ### üîç ‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï:
# - `box.xyxy` ‚Äî ‡∏û‡∏¥‡∏Å‡∏±‡∏î bounding box ‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö (x1, y1, x2, y2)
# - `box.conf` ‚Äî ‡∏Ñ‡πà‡∏≤ confidence (0-1) ‡∏¢‡∏¥‡πà‡∏á‡∏™‡∏π‡∏á‡∏¢‡∏¥‡πà‡∏á‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à
# - `box.cls` ‚Äî class ID ‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡πÑ‡∏î‡πâ
# - `model.names` ‚Äî mapping ‡∏à‡∏≤‡∏Å class ID ‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠ class

# %% [markdown]
# ---
# ## Lab 3: Custom Visualization with OpenCV
#
# ‡πÉ‡∏ä‡πâ OpenCV ‡∏ß‡∏≤‡∏î bounding box ‡πÄ‡∏≠‡∏á ‡πÄ‡∏û‡∏∑‡πà‡∏≠ customize ‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•
# ‡πÄ‡∏ä‡πà‡∏ô ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏™‡∏µ, ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•, ‡∏õ‡∏£‡∏±‡∏ö font

# %%
import cv2
import matplotlib.pyplot as plt
import numpy as np

# ‡πÇ‡∏´‡∏•‡∏î‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û
image = cv2.imread(IMAGE_PATH)
image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

# ‡∏ó‡∏≥ inference
model = YOLO("yolo26n.pt")
results = model(IMAGE_PATH)

# ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏™‡∏µ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ class (‡∏™‡∏∏‡πà‡∏°‡∏™‡∏µ)
np.random.seed(42)
colors = {i: tuple(np.random.randint(0, 255, 3).tolist()) for i in range(80)}

# ‡∏ß‡∏≤‡∏î bounding box
annotated = image_rgb.copy()
for box in results[0].boxes:
    x1, y1, x2, y2 = map(int, box.xyxy[0])
    class_id = int(box.cls[0])
    class_name = model.names[class_id]
    confidence = float(box.conf[0])
    color = colors[class_id]
    
    # ‡∏ß‡∏≤‡∏î‡∏Å‡∏£‡∏≠‡∏ö
    cv2.rectangle(annotated, (x1, y1), (x2, y2), color, 2)
    
    # ‡∏ß‡∏≤‡∏î label background
    label = f"{class_name} {confidence:.2f}"
    (label_w, label_h), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 1)
    cv2.rectangle(annotated, (x1, y1 - label_h - 10), (x1 + label_w, y1), color, -1)
    cv2.putText(annotated, label, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)

plt.figure(figsize=(15, 8))
plt.imshow(annotated)
plt.title("YOLO26 Object Detection ‚Äî Custom Visualization", fontsize=14)
plt.axis('off')
plt.show()

# %% [markdown]
# ---
# ## Lab 4: Built-in plot() Method ‚Äî ‡∏ß‡∏¥‡∏ò‡∏µ‡∏á‡πà‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
#
# Ultralytics ‡∏°‡∏µ built-in `plot()` method ‡∏ó‡∏µ‡πà‡∏™‡∏∞‡∏î‡∏ß‡∏Å‡∏°‡∏≤‡∏Å

# %%
from PIL import Image

# plot() ‡∏Ñ‡∏∑‡∏ô numpy array (BGR) ‡∏ó‡∏µ‡πà‡∏ß‡∏≤‡∏î annotation ‡πÅ‡∏•‡πâ‡∏ß
annotated_bgr = results[0].plot()

# ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô RGB ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö matplotlib
annotated_rgb = annotated_bgr[..., ::-1]

plt.figure(figsize=(15, 8))
plt.imshow(annotated_rgb)
plt.title("YOLO26 Detection ‚Äî Using plot() method", fontsize=14)
plt.axis('off')
plt.show()

# %% [markdown]
# ---
# ## Lab 5: Model Size Comparison ‚Äî Speed vs Accuracy Tradeoff
#
# YOLO26 ‡∏°‡∏µ‡∏´‡∏•‡∏≤‡∏¢‡∏Ç‡∏ô‡∏≤‡∏î ‡∏°‡∏≤‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ô‡∏ß‡πà‡∏≤ nano, small, medium ‡πÉ‡∏´‡πâ‡∏ú‡∏•‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£

# %%
import time

model_names = ["yolo26n.pt", "yolo26s.pt", "yolo26m.pt"]
model_results = {}

for name in model_names:
    print(f"\nüîÑ Loading {name}...")
    m = YOLO(name)
    
    # Warm-up run
    _ = m(IMAGE_PATH, verbose=False)
    
    # Timed run
    start = time.time()
    res = m(IMAGE_PATH, verbose=False)
    elapsed = time.time() - start
    
    num_detections = len(res[0].boxes)
    model_results[name] = {
        "detections": num_detections,
        "time_ms": elapsed * 1000,
        "result": res[0]
    }
    print(f"  ‚úÖ {name}: {num_detections} objects detected in {elapsed*1000:.1f} ms")

# %% [markdown]
# ### ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö

# %%
fig, axes = plt.subplots(1, 3, figsize=(20, 6))

for idx, (name, data) in enumerate(model_results.items()):
    annotated = data["result"].plot()[..., ::-1]
    axes[idx].imshow(annotated)
    axes[idx].set_title(f"{name}\n{data['detections']} objects | {data['time_ms']:.1f} ms", fontsize=12)
    axes[idx].axis('off')

plt.suptitle("YOLO26 Model Size Comparison: Nano vs Small vs Medium", fontsize=16, fontweight='bold')
plt.tight_layout()
plt.show()

# %% [markdown]
# ### üí° ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏ô‡πà‡∏≤‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï:
# - **Nano** ‡πÄ‡∏£‡πá‡∏ß‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÅ‡∏ï‡πà‡∏≠‡∏≤‡∏à‡∏û‡∏•‡∏≤‡∏î‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡πÄ‡∏•‡πá‡∏Å‡πÜ
# - **Small** ‡∏™‡∏°‡∏î‡∏∏‡∏•‡∏î‡∏µ ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡∏á‡∏≤‡∏ô‡∏™‡πà‡∏ß‡∏ô‡πÉ‡∏´‡∏ç‡πà
# - **Medium** ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡πÑ‡∏î‡πâ‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÅ‡∏ï‡πà‡∏ä‡πâ‡∏≤‡∏Å‡∏ß‡πà‡∏≤
# - ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö hardware ‡πÅ‡∏•‡∏∞ latency requirement

# %% [markdown]
# ---
# ## Lab 6: Confidence Threshold ‚Äî ‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏Ç‡∏≠‡∏á‡∏Ñ‡πà‡∏≤ Confidence
#
# ‡∏•‡∏≠‡∏á‡∏õ‡∏£‡∏±‡∏ö `conf` threshold ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π‡∏ß‡πà‡∏≤‡∏™‡πà‡∏á‡∏ú‡∏•‡∏ï‡πà‡∏≠ detection ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£

# %%
conf_thresholds = [0.5, 0.75, 0.90, 0.91]
model = YOLO("yolo26n.pt")

fig, axes = plt.subplots(1, 4, figsize=(24, 6))

for idx, conf in enumerate(conf_thresholds):
    res = model(IMAGE_PATH, conf=conf, verbose=False)
    annotated = res[0].plot()[..., ::-1]
    
    axes[idx].imshow(annotated)
    axes[idx].set_title(f"conf={conf}\n{len(res[0].boxes)} detections", fontsize=12)
    axes[idx].axis('off')

plt.suptitle("Effect of Confidence Threshold on Detection Results", fontsize=16, fontweight='bold')
plt.tight_layout()
plt.show()

# %% [markdown]
# ### üí° Key Insight:
# - **conf ‡∏ï‡πà‡∏≥ (0.1):** ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡πÑ‡∏î‡πâ‡∏°‡∏≤‡∏Å ‡πÅ‡∏ï‡πà‡∏°‡∏µ False Positive ‡∏™‡∏π‡∏á
# - **conf ‡∏™‡∏π‡∏á (0.75):** ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏ô‡πâ‡∏≠‡∏¢ ‡πÅ‡∏ï‡πà‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏°‡∏≤‡∏Å
# - ‡πÉ‡∏ô‡∏ó‡∏≤‡∏á‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥ ‡∏Ñ‡πà‡∏≤ default = 0.25 ‡πÄ‡∏õ‡πá‡∏ô‡∏à‡∏∏‡∏î‡∏™‡∏°‡∏î‡∏∏‡∏•‡∏ó‡∏µ‡πà‡∏î‡∏µ

# %% [markdown]
# ---
# ## Lab 7: Image Size Effects ‚Äî ‡∏õ‡∏£‡∏±‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î‡∏†‡∏≤‡∏û Inference
#
# ‡∏•‡∏≠‡∏á‡∏õ‡∏£‡∏±‡∏ö `imgsz` ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏ï‡πà‡∏≠ detection

# %%
model = YOLO("yolo26n.pt")

# ‡∏•‡∏≠‡∏á‡∏õ‡∏£‡∏±‡∏ö image size
img_sizes = [320, 480, 640]

fig, axes = plt.subplots(1, 3, figsize=(20, 6))

for idx, imgsz in enumerate(img_sizes):
    start = time.time()
    res = model(IMAGE_PATH, imgsz=imgsz, verbose=False)
    elapsed = (time.time() - start) * 1000
    
    annotated = res[0].plot()[..., ::-1]
    axes[idx].imshow(annotated)
    axes[idx].set_title(f"imgsz={imgsz}\n{len(res[0].boxes)} detections | {elapsed:.0f} ms", fontsize=12)
    axes[idx].axis('off')

plt.suptitle("Effect of Image Size on Detection", fontsize=16, fontweight='bold')
plt.tight_layout()
plt.show()

# %% [markdown]
# ### üí° Image Size Effect:
# - **imgsz ‡πÄ‡∏•‡πá‡∏Å (320):** ‡πÄ‡∏£‡πá‡∏ß‡∏°‡∏≤‡∏Å ‡πÅ‡∏ï‡πà‡∏≠‡∏≤‡∏à‡∏û‡∏•‡∏≤‡∏î‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡πÄ‡∏•‡πá‡∏Å
# - **imgsz ‡πÉ‡∏´‡∏ç‡πà (640):** ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤ ‡πÅ‡∏ï‡πà‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô
# - YOLO26 ‡∏à‡∏∞ resize ‡∏£‡∏π‡∏õ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô square ‡∏Å‡πà‡∏≠‡∏ô inference

# %% [markdown]
# ---
# ## Lab 8: Dual-Head Architecture ‚Äî NMS-Free vs NMS Inference
#
# YOLO26 ‡∏°‡∏µ‡∏™‡∏ñ‡∏≤‡∏õ‡∏±‡∏ï‡∏¢‡∏Å‡∏£‡∏£‡∏° **Dual-Head** ‡∏ó‡∏µ‡πà unique:
# - **One-to-One Head (default):** End-to-end, ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á NMS ‚Üí ‡πÄ‡∏£‡πá‡∏ß‡∏Å‡∏ß‡πà‡∏≤
# - **One-to-Many Head:** ‡πÉ‡∏ä‡πâ NMS ‡πÅ‡∏ö‡∏ö‡∏î‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏¥‡∏° ‚Üí accuracy ‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢

# %%
model = YOLO("yolo26n.pt")

# One-to-One Head (default) ‚Äî NMS-Free
results_e2e = model(IMAGE_PATH, verbose=False)  # end2end=True is default

# One-to-Many Head ‚Äî with NMS
results_nms = model(IMAGE_PATH, end2end=False, verbose=False)

fig, axes = plt.subplots(1, 2, figsize=(18, 7))

annotated_e2e = results_e2e[0].plot()[..., ::-1]
axes[0].imshow(annotated_e2e)
axes[0].set_title(f"One-to-One (NMS-Free)\n{len(results_e2e[0].boxes)} detections", fontsize=13)
axes[0].axis('off')

annotated_nms = results_nms[0].plot()[..., ::-1]
axes[1].imshow(annotated_nms)
axes[1].set_title(f"One-to-Many (with NMS)\n{len(results_nms[0].boxes)} detections", fontsize=13)
axes[1].axis('off')

plt.suptitle("YOLO26 Dual-Head Comparison", fontsize=16, fontweight='bold')
plt.tight_layout()
plt.show()

print("\nüìä Comparison:")
print(f"  One-to-One (NMS-Free): {len(results_e2e[0].boxes)} detections")
print(f"  One-to-Many (with NMS): {len(results_nms[0].boxes)} detections")

# %% [markdown]
# ### üí° ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÑ‡∏´‡∏£‡πà‡πÉ‡∏ä‡πâ Head ‡πÑ‡∏´‡∏ô:
# - **One-to-One:** ‡πÉ‡∏ä‡πâ‡∏ï‡∏≠‡∏ô deploy ‡∏à‡∏£‡∏¥‡∏á ‚Üí ‡πÄ‡∏£‡πá‡∏ß‡∏Å‡∏ß‡πà‡∏≤, ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ NMS
# - **One-to-Many:** ‡πÉ‡∏ä‡πâ‡∏ï‡∏≠‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ accuracy ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î ‚Üí ‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢‡πÉ‡∏ô‡∏ö‡∏≤‡∏á scenario
#

# %% [markdown]
# ---
# ## Lab 9: Object Counting & Class Filtering
#
# ‡∏õ‡∏£‡∏∞‡∏¢‡∏∏‡∏Å‡∏ï‡πå‡πÉ‡∏ä‡πâ YOLO26 ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡πÅ‡∏¢‡∏Å‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó

# %%
from collections import Counter

model = YOLO("yolo26s.pt")
results = model(IMAGE_PATH, verbose=False)

# ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡πÅ‡∏ï‡πà‡∏•‡∏∞ class
class_counts = Counter()
for box in results[0].boxes:
    class_id = int(box.cls[0])
    class_name = model.names[class_id]
    class_counts[class_name] += 1

print("üìä Object Count by Class:")
print("=" * 40)
for name, count in class_counts.most_common():
    print(f"  {name:>15}: {count} {'üì¶' * count}")

print(f"\n  {'Total':>15}: {sum(class_counts.values())}")

# %% [markdown]
# ### 9.1 Filter ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ class ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
#
# ‡πÉ‡∏ä‡πâ `classes` parameter ‡πÄ‡∏û‡∏∑‡πà‡∏≠ detect ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ class ‡∏ó‡∏µ‡πà‡∏™‡∏ô‡πÉ‡∏à

# %%
# COCO class IDs: 0=person, 32=sports ball
# ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏ô‡πÅ‡∏•‡∏∞‡∏•‡∏π‡∏Å‡∏ö‡∏≠‡∏•
results_filtered = model(IMAGE_PATH, classes=[0,32], verbose=False)

fig, axes = plt.subplots(1, 2, figsize=(18, 7))

# All classes
annotated_all = results[0].plot()[..., ::-1]
axes[0].imshow(annotated_all)
axes[0].set_title(f"All Classes ({len(results[0].boxes)} objects)", fontsize=13)
axes[0].axis('off')

# Filtered classes
annotated_filtered = results_filtered[0].plot()[..., ::-1]
axes[1].imshow(annotated_filtered)
axes[1].set_title(f"Person + Sports Ball Only ({len(results_filtered[0].boxes)} objects)", fontsize=13)
axes[1].axis('off')

plt.suptitle("Class Filtering ‚Äî Detect Only What You Need", fontsize=16, fontweight='bold')
plt.tight_layout()
plt.show()

# %% [markdown]
# ---
# ## Lab 10: Batch Inference ‚Äî ‡∏ó‡∏≥ Inference ‡∏´‡∏•‡∏≤‡∏¢‡∏£‡∏π‡∏õ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô
#
# YOLO26 ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥ inference ‡∏´‡∏•‡∏≤‡∏¢‡∏£‡∏π‡∏õ‡πÉ‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß

# %%
model = YOLO("yolo26n.pt")

# ‡∏ó‡∏≥ inference ‡∏ö‡∏ô‡∏£‡∏π‡∏õ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡∏´‡∏•‡∏≤‡∏¢‡∏Ñ‡∏£‡∏±‡πâ‡∏á (‡πÉ‡∏ô‡∏à‡∏£‡∏¥‡∏á‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏´‡∏•‡∏≤‡∏¢‡∏£‡∏π‡∏õ)
results = model([IMAGE_PATH, IMAGE_PATH], verbose=False)

print(f"üì∏ Number of images processed: {len(results)}")
for i, res in enumerate(results):
    print(f"  Image {i+1}: {len(res.boxes)} objects detected")

# %% [markdown]
# ---
# ## Lab 11: Save Results ‚Äî ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
#
# ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà annotate ‡πÅ‡∏•‡πâ‡∏ß‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå

# %%
model = YOLO("yolo26n.pt")
results = model(IMAGE_PATH, verbose=False)

# ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 1: ‡πÉ‡∏ä‡πâ save() method
results[0].save(filename="detection_result.jpg")
print("‚úÖ Saved: detection_result.jpg")

# ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 2: ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏à‡∏≤‡∏Å plot()
annotated = results[0].plot()
cv2.imwrite("detection_result_v2.jpg", annotated)
print("‚úÖ Saved: detection_result_v2.jpg")

# ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 3: ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å crop ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏
results[0].save_crop(save_dir="crops/")
print("‚úÖ Saved crops to: crops/")

# %% [markdown]
# ---
# ## Lab 12: Instance Segmentation ‚Äî ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
#
# Segmentation ‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏ö‡∏≠‡∏Å‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á ‡πÅ‡∏ï‡πà‡∏¢‡∏±‡∏á‡∏ö‡∏≠‡∏Å **pixel mask** ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏
#
# ### üß† ‡∏ó‡∏≥‡πÑ‡∏° Segmentation ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç?
# Segmentation mask ‡∏Ñ‡∏∑‡∏≠‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á‡∏´‡∏•‡∏≤‡∏¢ application:
# - **Background Removal** ‚Äî ‡∏•‡∏ö‡∏û‡∏∑‡πâ‡∏ô‡∏´‡∏•‡∏±‡∏á‡∏≠‡∏≠‡∏Å (Lab 14)
# - **Object Blur** ‚Äî ‡πÄ‡∏ö‡∏•‡∏≠‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å (Lab 15)
# - **Video Effects** ‚Äî ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô background ‡πÅ‡∏ö‡∏ö real-time
# - **Medical Imaging** ‚Äî ‡πÅ‡∏¢‡∏Å‡∏≠‡∏ß‡∏±‡∏¢‡∏ß‡∏∞/‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏á‡∏≠‡∏Å
# - **Autonomous Driving** ‚Äî ‡πÅ‡∏¢‡∏Å road, car, pedestrian

# %%
# ‡πÇ‡∏´‡∏•‡∏î segmentation model
seg_model = YOLO("yolo26n-seg.pt")

# ‡∏ó‡∏≥ inference
results = seg_model(IMAGE_PATH, imgsz=640)
results[0].show()

# %% [markdown]
# ---
# ## Lab 13: Individual Object Masks ‚Äî ‡πÅ‡∏¢‡∏Å‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ä‡∏¥‡πâ‡∏ô

# %%
import cv2
from ultralytics import YOLO
import numpy as np
import matplotlib.pyplot as plt

seg_model = YOLO("yolo26n-seg.pt")
original_image = cv2.imread(IMAGE_PATH)
height, width = original_image.shape[:2]
rgb_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)

results = seg_model(rgb_image, imgsz=640, verbose=False)

if len(results) > 0 and results[0].masks is not None:
    masks = results[0].masks
    object_count = len(masks.data)
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á combined mask
    combined_mask = np.zeros((height, width), dtype=np.uint8)
    
    for i in range(object_count):
        mask_array = masks.data[i].cpu().numpy().astype(np.uint8) * 255
        if mask_array.shape[:2] != (height, width):
            mask_array = cv2.resize(mask_array, (width, height), interpolation=cv2.INTER_NEAREST)
        combined_mask = cv2.bitwise_or(combined_mask, mask_array)
    
    # ‡πÅ‡∏™‡∏î‡∏á Original vs Segmented
    combined_result = cv2.bitwise_and(rgb_image, rgb_image, mask=combined_mask)
    
    fig, axes = plt.subplots(1, 3, figsize=(20, 6))
    
    axes[0].imshow(rgb_image)
    axes[0].set_title("Original Image", fontsize=13)
    axes[0].axis('off')
    
    axes[1].imshow(combined_mask, cmap='gray')
    axes[1].set_title("Combined Mask", fontsize=13)
    axes[1].axis('off')
    
    axes[2].imshow(combined_result)
    axes[2].set_title("Segmented Objects", fontsize=13)
    axes[2].axis('off')
    
    plt.suptitle("YOLO26 Instance Segmentation", fontsize=16, fontweight='bold')
    plt.tight_layout()
    plt.show()
    
    # ‡πÅ‡∏™‡∏î‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ object ‡πÅ‡∏¢‡∏Å‡∏Å‡∏±‡∏ô
    boxes = results[0].boxes.xyxy.cpu().numpy()
    class_ids = results[0].boxes.cls.cpu().numpy().astype(int)
    confs = results[0].boxes.conf.cpu().numpy()
    
    show_count = min(object_count, 8)
    cols = min(show_count, 4)
    rows = (show_count + cols - 1) // cols
    
    fig, axes = plt.subplots(rows, cols, figsize=(5 * cols, 5 * rows))
    if show_count == 1:
        axes = np.array([axes])
    axes = axes.flatten()
    
    for i in range(show_count):
        mask_array = masks.data[i].cpu().numpy().astype(np.uint8) * 255
        if mask_array.shape[:2] != (height, width):
            mask_array = cv2.resize(mask_array, (width, height), interpolation=cv2.INTER_NEAREST)
        
        masked_result = cv2.bitwise_and(rgb_image, rgb_image, mask=mask_array)
        
        cls_id = class_ids[i]
        label = seg_model.names[cls_id]
        conf = confs[i]
        
        axes[i].imshow(masked_result)
        axes[i].set_title(f"{label} ({conf:.2f})", fontsize=11)
        axes[i].axis('off')
    
    for j in range(show_count, len(axes)):
        axes[j].axis('off')
    
    plt.suptitle("Individual Object Masks", fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.show()
    
    print(f"\nüìä Total objects segmented: {object_count}")
    print(f"üìê Mask shape: {masks.shape}")
else:
    print("‚ö†Ô∏è No masks found in the results.")

# %% [markdown]
# ### üí° Segmentation vs Detection:
# - Detection ‡πÉ‡∏´‡πâ‡πÅ‡∏Ñ‡πà **bounding box** (‡∏™‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏µ‡πà‡∏¢‡∏°)
# - Segmentation ‡πÉ‡∏´‡πâ **pixel mask** (‡∏£‡∏π‡∏õ‡∏£‡πà‡∏≤‡∏á‡∏à‡∏£‡∏¥‡∏á‡∏Ç‡∏≠‡∏á‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏)
# - Segmentation ‡πÉ‡∏ä‡πâ compute ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ ‡πÅ‡∏ï‡πà‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏Å‡∏ß‡πà‡∏≤‡∏°‡∏≤‡∏Å

# %% [markdown]
# ---
# ## Lab 14: üñºÔ∏è Background Removal & Replacement ‡∏î‡πâ‡∏ß‡∏¢ Segmentation Mask
#
# ‡πÉ‡∏ä‡πâ segmentation mask ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡πÉ‡∏ô Lab 12‚Äì13 ‡∏°‡∏≤‡∏õ‡∏£‡∏∞‡∏¢‡∏∏‡∏Å‡∏ï‡πå‡∏ó‡∏≥:
# 1. **‡∏•‡∏ö Background** ‚Äî ‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÅ‡∏Ñ‡πà‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å
# 2. **‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô Background** ‚Äî ‡πÉ‡∏™‡πà background ‡πÉ‡∏´‡∏°‡πà (‡∏™‡∏µ‡∏û‡∏∑‡πâ‡∏ô ‡∏´‡∏£‡∏∑‡∏≠ ‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏≠‡∏∑‡πà‡∏ô)
# 3. **‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏** ‚Äî ‡πÇ‡∏î‡∏¢‡∏Å‡∏≥‡∏´‡∏ô‡∏î class ID ‡∏´‡∏£‡∏∑‡∏≠ object ID
#
# ### üß† ‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£:
# ```
# Mask = 255 (white) ‚Üí foreground (‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ)
# Mask = 0   (black) ‚Üí background (‡∏•‡∏ö/‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô)
# ```

# %% [markdown]
# ### 14.1 Helper Function: ‡∏™‡∏£‡πâ‡∏≤‡∏á Mask ‡∏à‡∏≤‡∏Å Segmentation Results

# %%
import cv2
import numpy as np
import matplotlib.pyplot as plt
from ultralytics import YOLO


def get_segmentation_masks(image_path, model=None, target_classes=None, target_object_ids=None):
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á segmentation masks ‡∏à‡∏≤‡∏Å YOLO26 ‡πÇ‡∏î‡∏¢‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞ class ‡∏´‡∏£‡∏∑‡∏≠ object ID ‡πÑ‡∏î‡πâ
    
    Parameters:
    -----------
    image_path : str
        path ‡∏Ç‡∏≠‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û
    model : YOLO, optional
        YOLO segmentation model (‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà)
    target_classes : list of str, optional
        ‡∏ä‡∏∑‡πà‡∏≠ class ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ ‡πÄ‡∏ä‡πà‡∏ô ['person', 'sports ball']
        ‡∏ñ‡πâ‡∏≤ None = ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ó‡∏∏‡∏Å class
    target_object_ids : list of int, optional
        index ‡∏Ç‡∏≠‡∏á object ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ (0-indexed) ‡πÄ‡∏ä‡πà‡∏ô [0, 2, 5]
        ‡∏ñ‡πâ‡∏≤ None = ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ó‡∏∏‡∏Å object
    
    Returns:
    --------
    dict with keys:
        'image_rgb'      : original image in RGB
        'combined_mask'  : combined mask ‡∏Ç‡∏≠‡∏á objects ‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å
        'individual_masks': list ‡∏Ç‡∏≠‡∏á mask ‡πÅ‡∏ï‡πà‡∏•‡∏∞ object
        'labels'         : list ‡∏Ç‡∏≠‡∏á (class_name, confidence, object_id)
        'all_labels'     : list ‡∏Ç‡∏≠‡∏á‡∏ó‡∏∏‡∏Å object (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö reference)
    """
    if model is None:
        model = YOLO("yolo26n-seg.pt")
    
    original = cv2.imread(image_path)
    height, width = original.shape[:2]
    rgb_image = cv2.cvtColor(original, cv2.COLOR_BGR2RGB)
    
    results = model(rgb_image, imgsz=640, verbose=False)
    
    output = {
        'image_rgb': rgb_image,
        'combined_mask': np.zeros((height, width), dtype=np.uint8),
        'individual_masks': [],
        'labels': [],
        'all_labels': [],
        'height': height,
        'width': width,
    }
    
    if len(results) == 0 or results[0].masks is None:
        print("‚ö†Ô∏è No segmentation masks found.")
        return output
    
    masks_data = results[0].masks.data.cpu().numpy()
    class_ids = results[0].boxes.cls.cpu().numpy().astype(int)
    confs = results[0].boxes.conf.cpu().numpy()
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á list ‡∏Ç‡∏≠‡∏á‡∏ó‡∏∏‡∏Å object ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö reference
    for i in range(len(masks_data)):
        cls_name = model.names[class_ids[i]]
        output['all_labels'].append((cls_name, confs[i], i))
    
    # ‡∏Å‡∏£‡∏≠‡∏á objects ‡∏ï‡∏≤‡∏°‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç
    for i in range(len(masks_data)):
        cls_name = model.names[class_ids[i]]
        
        if target_classes is not None and cls_name not in target_classes:
            continue
        if target_object_ids is not None and i not in target_object_ids:
            continue
        
        mask = masks_data[i].astype(np.uint8) * 255
        if mask.shape[:2] != (height, width):
            mask = cv2.resize(mask, (width, height), interpolation=cv2.INTER_NEAREST)
        
        output['individual_masks'].append(mask)
        output['labels'].append((cls_name, confs[i], i))
        output['combined_mask'] = cv2.bitwise_or(output['combined_mask'], mask)
    
    return output


# ‡πÅ‡∏™‡∏î‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ objects ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô‡∏£‡∏π‡∏õ
seg_model = YOLO("yolo26n-seg.pt")
info = get_segmentation_masks(IMAGE_PATH, model=seg_model)

print("üìã All detected objects in this image:")
print("=" * 55)
print(f"{'ID':>4} | {'Class':>15} | {'Confidence':>10}")
print("-" * 55)
for cls_name, conf, obj_id in info['all_labels']:
    print(f"  {obj_id:>2} | {cls_name:>15} | {conf:.4f}")
print(f"\nüí° ‡πÉ‡∏ä‡πâ target_classes=['person'] ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏ô")
print(f"üí° ‡πÉ‡∏ä‡πâ target_object_ids=[0, 2] ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞ object ‡∏ó‡∏µ‡πà 0 ‡πÅ‡∏•‡∏∞ 2")

# %% [markdown]
# ### 14.2 Background Removal ‚Äî ‡∏•‡∏ö Background ‡∏≠‡∏≠‡∏Å (‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ï‡∏≤‡∏° Class)

# %%
def remove_background(image_path, model=None, target_classes=None, target_object_ids=None,
                       bg_color=(255, 255, 255)):
    """
    ‡∏•‡∏ö background ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û ‡πÅ‡∏•‡πâ‡∏ß‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏î‡πâ‡∏ß‡∏¢‡∏™‡∏µ‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î
    
    Parameters:
    -----------
    bg_color : tuple (R, G, B)
        ‡∏™‡∏µ‡∏û‡∏∑‡πâ‡∏ô‡∏´‡∏•‡∏±‡∏á‡πÉ‡∏´‡∏°‡πà ‡πÄ‡∏ä‡πà‡∏ô (255, 255, 255) = ‡∏Ç‡∏≤‡∏ß, (0, 0, 0) = ‡∏î‡∏≥
    """
    data = get_segmentation_masks(image_path, model, target_classes, target_object_ids)
    
    rgb = data['image_rgb']
    mask = data['combined_mask']
    
    new_bg = np.full_like(rgb, bg_color, dtype=np.uint8)
    
    mask_3ch = cv2.merge([mask, mask, mask])
    foreground = cv2.bitwise_and(rgb, mask_3ch)
    background = cv2.bitwise_and(new_bg, cv2.bitwise_not(mask_3ch))
    result = cv2.add(foreground, background)
    
    return result, data


# === ‡∏ó‡∏î‡∏•‡∏≠‡∏á: ‡∏•‡∏ö Background ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞ "person" ===
result_white, data = remove_background(
    IMAGE_PATH, model=seg_model, target_classes=['person'], bg_color=(255, 255, 255)
)
result_black, _ = remove_background(
    IMAGE_PATH, model=seg_model, target_classes=['person'], bg_color=(0, 0, 0)
)
result_green, _ = remove_background(
    IMAGE_PATH, model=seg_model, target_classes=['person'], bg_color=(0, 177, 64)
)

fig, axes = plt.subplots(1, 4, figsize=(24, 6))

axes[0].imshow(data['image_rgb'])
axes[0].set_title("Original", fontsize=13)
axes[0].axis('off')

axes[1].imshow(result_white)
axes[1].set_title("White Background\n(person only)", fontsize=13)
axes[1].axis('off')

axes[2].imshow(result_black)
axes[2].set_title("Black Background\n(person only)", fontsize=13)
axes[2].axis('off')

axes[3].imshow(result_green)
axes[3].set_title("Green Screen\n(person only)", fontsize=13)
axes[3].axis('off')

plt.suptitle("üñºÔ∏è Background Removal ‚Äî Filter by Class (person)", fontsize=16, fontweight='bold')
plt.tight_layout()
plt.show()

print(f"\nüìä Selected {len(data['labels'])} objects (class='person')")
for cls_name, conf, obj_id in data['labels']:
    print(f"   Object {obj_id}: {cls_name} (conf: {conf:.3f})")

# %% [markdown]
# ### 14.3 Background Removal ‚Äî ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ï‡∏≤‡∏° Object ID

# %%
# ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å object 0 ‡πÅ‡∏•‡∏∞ 2 (‡∏õ‡∏£‡∏±‡∏ö‡∏ï‡∏≤‡∏°‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì)
selected_ids = [0, 2]

result_selected, data_selected = remove_background(
    IMAGE_PATH, model=seg_model, target_object_ids=selected_ids, bg_color=(240, 240, 245)
)
result_all, data_all = remove_background(
    IMAGE_PATH, model=seg_model, target_object_ids=None, bg_color=(240, 240, 245)
)

fig, axes = plt.subplots(1, 3, figsize=(20, 6))

axes[0].imshow(data_all['image_rgb'])
axes[0].set_title("Original", fontsize=13)
axes[0].axis('off')

axes[1].imshow(result_all)
axes[1].set_title(f"All Objects ({len(data_all['labels'])})", fontsize=13)
axes[1].axis('off')

axes[2].imshow(result_selected)
axes[2].set_title(f"Selected Objects (IDs: {selected_ids})", fontsize=13)
axes[2].axis('off')

plt.suptitle("üéØ Background Removal ‚Äî Filter by Object ID", fontsize=16, fontweight='bold')
plt.tight_layout()
plt.show()

print(f"\nüìä Selected objects: {selected_ids}")
for cls_name, conf, obj_id in data_selected['labels']:
    print(f"   Object {obj_id}: {cls_name} (conf: {conf:.3f})")

# %% [markdown]
# ### 14.4 Background Replacement ‚Äî ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô Background ‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏≠‡∏∑‡πà‡∏ô

# %%
def replace_background_with_image(image_path, bg_image_path=None, model=None,
                                   target_classes=None, target_object_ids=None):
    """
    ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô background ‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏≠‡∏∑‡πà‡∏ô
    ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏ bg_image_path ‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á gradient background ‡πÅ‡∏ó‡∏ô
    """
    data = get_segmentation_masks(image_path, model, target_classes, target_object_ids)
    
    rgb = data['image_rgb']
    mask = data['combined_mask']
    h, w = data['height'], data['width']
    
    if bg_image_path is not None:
        bg = cv2.imread(bg_image_path)
        bg = cv2.cvtColor(bg, cv2.COLOR_BGR2RGB)
        bg = cv2.resize(bg, (w, h))
    else:
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á gradient background (sunset effect)
        bg = np.zeros((h, w, 3), dtype=np.uint8)
        for y_pos in range(h):
            ratio = y_pos / h
            r = int(255 * (1 - ratio) + 20 * ratio)
            g = int(100 * (1 - ratio) + 10 * ratio)
            b = int(50 * (1 - ratio) + 80 * ratio)
            bg[y_pos, :] = [r, g, b]
    
    mask_3ch = cv2.merge([mask, mask, mask])
    foreground = cv2.bitwise_and(rgb, mask_3ch)
    background = cv2.bitwise_and(bg, cv2.bitwise_not(mask_3ch))
    result = cv2.add(foreground, background)
    
    return result, data


# === ‡∏ó‡∏î‡∏•‡∏≠‡∏á: ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô background ‡πÄ‡∏õ‡πá‡∏ô gradient ===
result_gradient, data = replace_background_with_image(
    IMAGE_PATH, model=seg_model, target_classes=['person']
)

fig, axes = plt.subplots(1, 2, figsize=(16, 7))

axes[0].imshow(data['image_rgb'])
axes[0].set_title("Original", fontsize=13)
axes[0].axis('off')

axes[1].imshow(result_gradient)
axes[1].set_title("Gradient Background\n(person only)", fontsize=13)
axes[1].axis('off')

plt.suptitle("üåÖ Background Replacement with Custom Image/Gradient", fontsize=16, fontweight='bold')
plt.tight_layout()
plt.show()

# %% [markdown]
# ### üí° Background Removal ‚Äî Key Takeaways:
# - **Segmentation mask** ‡∏Ñ‡∏∑‡∏≠‡∏´‡∏±‡∏ß‡πÉ‡∏à‡∏Ç‡∏≠‡∏á background removal
# - **target_classes** ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞ class (‡πÄ‡∏ä‡πà‡∏ô 'person', 'car')
# - **target_object_ids** ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏ö‡∏≤‡∏á‡∏ä‡∏¥‡πâ‡∏ô
# - ‡πÄ‡∏õ‡πá‡∏ô‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á Zoom virtual background
# - ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ï‡πà‡∏≠‡∏¢‡∏≠‡∏î‡πÑ‡∏õ **real-time** ‡πÑ‡∏î‡πâ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ö webcam feed

# %% [markdown]
# ---
# ## Lab 15: üîÆ Blur Selected Object & Bokeh Effect
#
# ‡πÉ‡∏ä‡πâ segmentation mask ‡πÄ‡∏û‡∏∑‡πà‡∏≠ **‡πÄ‡∏ö‡∏•‡∏≠‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å** ‡∏´‡∏£‡∏∑‡∏≠ **‡πÄ‡∏ö‡∏•‡∏≠ background**
#
# ### Use Cases:
# - **Privacy Protection** ‚Äî ‡πÄ‡∏ö‡∏•‡∏≠‡∏´‡∏ô‡πâ‡∏≤‡∏Ñ‡∏ô/‡∏õ‡πâ‡∏≤‡∏¢‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô
# - **Focus Effect** ‚Äî ‡πÄ‡∏ö‡∏•‡∏≠ background ‡πÄ‡∏ô‡πâ‡∏ô‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏´‡∏•‡∏±‡∏Å (Bokeh)
# - **Content Moderation** ‚Äî ‡∏õ‡∏¥‡∏î‡∏ö‡∏±‡∏á/‡πÄ‡∏ö‡∏•‡∏≠‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°
#
# ### üß† ‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£:
# ```
# Blur Object:     result = original √ó (1 - mask) + blurred √ó mask
# Blur Background:  result = original √ó mask + blurred √ó (1 - mask)
# ```

# %%
def blur_objects(image_path, model=None, target_classes=None, target_object_ids=None,
                 blur_strength=51, blur_background=False):
    """
    ‡πÄ‡∏ö‡∏•‡∏≠‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏ö‡∏•‡∏≠ background (Bokeh effect)
    
    Parameters:
    -----------
    blur_strength : int (odd number)
        ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏°‡∏Ç‡∏≠‡∏á blur (‡∏¢‡∏¥‡πà‡∏á‡∏°‡∏≤‡∏Å‡∏¢‡∏¥‡πà‡∏á‡πÄ‡∏ö‡∏•‡∏≠)
    blur_background : bool
        True  = ‡πÄ‡∏ö‡∏•‡∏≠ background ‚Üí Bokeh effect
        False = ‡πÄ‡∏ö‡∏•‡∏≠ foreground
    """
    data = get_segmentation_masks(image_path, model, target_classes, target_object_ids)
    
    rgb = data['image_rgb']
    mask = data['combined_mask']
    
    if blur_strength % 2 == 0:
        blur_strength += 1
    blurred = cv2.GaussianBlur(rgb, (blur_strength, blur_strength), 0)
    
    mask_float = (mask / 255.0).astype(np.float32)
    mask_3ch = cv2.merge([mask_float, mask_float, mask_float])
    
    if blur_background:
        result = (rgb * mask_3ch + blurred * (1 - mask_3ch)).astype(np.uint8)
    else:
        result = (blurred * mask_3ch + rgb * (1 - mask_3ch)).astype(np.uint8)
    
    return result, data


# %% [markdown]
# ### 15.1 Blur Specific Objects

# %%
# === ‡πÄ‡∏ö‡∏•‡∏≠‡πÄ‡∏â‡∏û‡∏≤‡∏∞ "person" ===
result_blur_person, data = blur_objects(
    IMAGE_PATH, model=seg_model, target_classes=['person'],
    blur_strength=51, blur_background=False
)

# === ‡πÄ‡∏ö‡∏•‡∏≠‡∏ó‡∏∏‡∏Å object ===
result_blur_all, _ = blur_objects(
    IMAGE_PATH, model=seg_model, target_classes=None,
    blur_strength=51, blur_background=False
)

fig, axes = plt.subplots(1, 3, figsize=(20, 6))

axes[0].imshow(data['image_rgb'])
axes[0].set_title("Original", fontsize=13)
axes[0].axis('off')

axes[1].imshow(result_blur_person)
axes[1].set_title("Blur Person Only", fontsize=13)
axes[1].axis('off')

axes[2].imshow(result_blur_all)
axes[2].set_title("Blur All Objects", fontsize=13)
axes[2].axis('off')

plt.suptitle("üîÆ Blur Selected Objects", fontsize=16, fontweight='bold')
plt.tight_layout()
plt.show()

# %% [markdown]
# ### 15.2 Bokeh Effect ‚Äî ‡πÄ‡∏ö‡∏•‡∏≠ Background (‡πÄ‡∏ô‡πâ‡∏ô Foreground)

# %%
blur_strengths = [21, 51, 101]

fig, axes = plt.subplots(1, len(blur_strengths) + 1, figsize=(24, 6))

axes[0].imshow(data['image_rgb'])
axes[0].set_title("Original", fontsize=13)
axes[0].axis('off')

for idx, strength in enumerate(blur_strengths):
    result_bokeh, _ = blur_objects(
        IMAGE_PATH, model=seg_model, target_classes=['person'],
        blur_strength=strength, blur_background=True
    )
    axes[idx + 1].imshow(result_bokeh)
    axes[idx + 1].set_title(f"Bokeh (blur={strength})", fontsize=13)
    axes[idx + 1].axis('off')

plt.suptitle("üì∏ Bokeh Effect ‚Äî Background Blur (‡πÄ‡∏ô‡πâ‡∏ô Person)", fontsize=16, fontweight='bold')
plt.tight_layout()
plt.show()

# %% [markdown]
# ### 15.3 Blur by Object ID

# %%
blur_ids = [0, 1]  # ‡∏õ‡∏£‡∏±‡∏ö‡∏ï‡∏≤‡∏°‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û

result_blur_selected, data_sel = blur_objects(
    IMAGE_PATH, model=seg_model, target_object_ids=blur_ids,
    blur_strength=71, blur_background=False
)

fig, axes = plt.subplots(1, 2, figsize=(16, 7))

axes[0].imshow(data_sel['image_rgb'])
axes[0].set_title("Original", fontsize=13)
axes[0].axis('off')

axes[1].imshow(result_blur_selected)
axes[1].set_title(f"Blurred Objects (IDs: {blur_ids})", fontsize=13)
axes[1].axis('off')

plt.suptitle("üéØ Blur Specific Objects by ID", fontsize=16, fontweight='bold')
plt.tight_layout()
plt.show()

print(f"\nüìä Blurred objects:")
for cls_name, conf, obj_id in data_sel['labels']:
    print(f"   Object {obj_id}: {cls_name} (conf: {conf:.3f})")

# %% [markdown]
# ### üí° Blur ‚Äî Key Takeaways:
# - **Blur Object** ‚Üí privacy (‡πÄ‡∏ö‡∏•‡∏≠‡∏´‡∏ô‡πâ‡∏≤/‡∏õ‡πâ‡∏≤‡∏¢‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô)
# - **Blur Background (Bokeh)** ‚Üí portrait mode ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÉ‡∏ô‡∏°‡∏∑‡∏≠‡∏ñ‡∏∑‡∏≠
# - `blur_strength` ‡∏¢‡∏¥‡πà‡∏á‡∏°‡∏≤‡∏Å‡∏¢‡∏¥‡πà‡∏á‡πÄ‡∏ö‡∏•‡∏≠ (‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏•‡∏Ç‡∏Ñ‡∏µ‡πà)
# - ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞ class ‡∏´‡∏£‡∏∑‡∏≠ object ID ‡πÑ‡∏î‡πâ

# %% [markdown]
# ---
# ## Lab 16: Basic Pose Estimation
#
# Pose Estimation ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö **keypoints** ‡∏ö‡∏ô‡∏£‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏¢‡∏Ñ‡∏ô (17 ‡∏à‡∏∏‡∏î‡∏ï‡∏≤‡∏° COCO format)
#
# ### COCO Keypoints (17 ‡∏à‡∏∏‡∏î):
# ```
# 0: Nose          1: Left Eye       2: Right Eye
# 3: Left Ear      4: Right Ear      5: Left Shoulder
# 6: Right Shoulder 7: Left Elbow    8: Right Elbow
# 9: Left Wrist    10: Right Wrist   11: Left Hip
# 12: Right Hip     13: Left Knee    14: Right Knee
# 15: Left Ankle    16: Right Ankle
# ```

# %%
pose_model = YOLO("yolo26n-pose.pt")

results = pose_model(IMAGE_PATH, imgsz=640)
results[0].show()

# %% [markdown]
# ---
# ## Lab 17: Custom Skeleton Drawing with OpenCV

# %%
import cv2
import matplotlib.pyplot as plt
import numpy as np

pose_model = YOLO("yolo26n-pose.pt")
results = pose_model(IMAGE_PATH, verbose=False)

image = cv2.imread(IMAGE_PATH)
image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

# COCO Skeleton connections
skeleton = [
    (5, 6), (5, 7), (7, 9), (6, 8), (8, 10),
    (5, 11), (6, 12), (11, 12), (11, 13), (13, 15),
    (12, 14), (14, 16), (0, 1), (0, 2), (1, 3), (2, 4),
]

kpt_names = [
    "Nose", "L-Eye", "R-Eye", "L-Ear", "R-Ear",
    "L-Shoulder", "R-Shoulder", "L-Elbow", "R-Elbow",
    "L-Wrist", "R-Wrist", "L-Hip", "R-Hip",
    "L-Knee", "R-Knee", "L-Ankle", "R-Ankle"
]

annotated = image_rgb.copy()

for person_idx, person in enumerate(results[0].keypoints.data.cpu().numpy()):
    # ‡∏ß‡∏≤‡∏î keypoints
    for idx, (x, y, conf) in enumerate(person):
        if conf > 0.5:
            cv2.circle(annotated, (int(x), int(y)), 5, (0, 255, 0), -1)
            cv2.circle(annotated, (int(x), int(y)), 7, (255, 255, 255), 1)
    
    # ‡∏ß‡∏≤‡∏î skeleton
    for start, end in skeleton:
        if person[start][2] > 0.5 and person[end][2] > 0.5:
            pt1 = (int(person[start][0]), int(person[start][1]))
            pt2 = (int(person[end][0]), int(person[end][1]))
            cv2.line(annotated, pt1, pt2, (0, 200, 255), 2)

plt.figure(figsize=(15, 10))
plt.imshow(annotated)
plt.title("YOLO26 Pose Estimation ‚Äî Custom Skeleton Drawing", fontsize=14)
plt.axis('off')
plt.show()

# %% [markdown]
# ---
# ## Lab 18: Keypoints Data Analysis

# %%
keypoints_data = results[0].keypoints.data.cpu().numpy()

print(f"‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡πÑ‡∏î‡πâ: {len(keypoints_data)}")
print(f"Shape ‡∏Ç‡∏≠‡∏á keypoints data: {keypoints_data.shape}")
print(f"  ‚Üí (‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏ô, ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô keypoints, 3)  # 3 = x, y, confidence\n")

if len(keypoints_data) > 0:
    person = keypoints_data[0]
    print("üìç Keypoints ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà 1:")
    print("-" * 50)
    for idx, (x, y, conf) in enumerate(person):
        status = "‚úÖ" if conf > 0.5 else "‚ùå"
        print(f"  {status} {kpt_names[idx]:>12}: ({x:.1f}, {y:.1f}) conf={conf:.2f}")

# %% [markdown]
# ---
# ## Lab 20: Object Distance Estimation & Visualization
#
# **‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì‡∏£‡∏∞‡∏¢‡∏∞‡∏´‡πà‡∏≤‡∏á‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏** ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• bounding box
#
# ### üß† ‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£:
# ```
# centroid = ((x1 + x2) / 2, (y1 + y2) / 2)
# distance = sqrt((cx1 - cx2)¬≤ + (cy1 - cy2)¬≤)
# ```
#
# ### ‚ö†Ô∏è ‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î:
# - ‡∏£‡∏∞‡∏¢‡∏∞‡∏ó‡∏µ‡πà‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÑ‡∏î‡πâ‡πÄ‡∏õ‡πá‡∏ô **pixel distance** ‡∏ö‡∏ô‡∏†‡∏≤‡∏û 2D (‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏£‡∏∞‡∏¢‡∏∞‡∏à‡∏£‡∏¥‡∏á 3D)
# - ‡πÅ‡∏ï‡πà pixel distance ‡∏ö‡∏≠‡∏Å **relative relationship** ‡πÑ‡∏î‡πâ‡∏î‡∏µ
# - ‡πÉ‡∏ä‡πâ‡πÉ‡∏ô: Social distancing, ‡∏Å‡∏µ‡∏¨‡∏≤ analysis, ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏

# %% [markdown]
# ### 20.1 ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Centroid ‡πÅ‡∏•‡∏∞ Distance

# %%
import cv2
import numpy as np
import matplotlib.pyplot as plt
from ultralytics import YOLO
from itertools import combinations
import math


def compute_object_distances(image_path, model=None, target_classes=None):
    """
    ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏£‡∏∞‡∏¢‡∏∞‡∏´‡πà‡∏≤‡∏á (pixel distance) ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏π‡πà‡∏Ç‡∏≠‡∏á‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡πÑ‡∏î‡πâ
    """
    if model is None:
        model = YOLO("yolo26s.pt")
    
    original = cv2.imread(image_path)
    rgb = cv2.cvtColor(original, cv2.COLOR_BGR2RGB)
    
    results = model(rgb, imgsz=640, verbose=False)
    
    objects = []
    for i, box in enumerate(results[0].boxes):
        cls_id = int(box.cls[0])
        cls_name = model.names[cls_id]
        
        if target_classes is not None and cls_name not in target_classes:
            continue
        
        x1, y1, x2, y2 = map(int, box.xyxy[0])
        cx = (x1 + x2) / 2
        cy = (y1 + y2) / 2
        conf = float(box.conf[0])
        
        objects.append({
            'id': len(objects), 'original_id': i, 'class': cls_name, 'conf': conf,
            'bbox': (x1, y1, x2, y2), 'centroid': (cx, cy),
            'width': x2 - x1, 'height': y2 - y1,
        })
    
    distances = []
    for a, b in combinations(range(len(objects)), 2):
        obj_a, obj_b = objects[a], objects[b]
        dist = math.sqrt(
            (obj_a['centroid'][0] - obj_b['centroid'][0]) ** 2 +
            (obj_a['centroid'][1] - obj_b['centroid'][1]) ** 2
        )
        distances.append({
            'id_a': obj_a['id'], 'id_b': obj_b['id'],
            'class_a': obj_a['class'], 'class_b': obj_b['class'],
            'distance': dist,
            'centroid_a': obj_a['centroid'], 'centroid_b': obj_b['centroid'],
        })
    
    distances.sort(key=lambda x: x['distance'])
    
    return {'objects': objects, 'distances': distances, 'image_rgb': rgb}


# === ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏£‡∏∞‡∏¢‡∏∞‡∏´‡πà‡∏≤‡∏á ===
det_model = YOLO("yolo26s.pt")
dist_data = compute_object_distances(IMAGE_PATH, model=det_model)

print("üìã Detected Objects:")
print("=" * 65)
print(f"{'ID':>4} | {'Class':>15} | {'Conf':>6} | {'Centroid (x, y)':>20} | {'Size (w√óh)':>12}")
print("-" * 65)
for obj in dist_data['objects']:
    cx, cy = obj['centroid']
    print(f"  {obj['id']:>2} | {obj['class']:>15} | {obj['conf']:.3f} | ({cx:>7.1f}, {cy:>7.1f}) | {obj['width']:>4}√ó{obj['height']:<4}")

print(f"\nüìè Distance Pairs (sorted by distance, top 10):")
print("=" * 70)
print(f"{'Pair':>10} | {'Classes':>30} | {'Distance (px)':>14}")
print("-" * 70)
for d in dist_data['distances'][:10]:
    pair_str = f"{d['id_a']}‚Üî{d['id_b']}"
    class_str = f"{d['class_a']} ‚Üî {d['class_b']}"
    print(f"  {pair_str:>8} | {class_str:>30} | {d['distance']:>12.1f} px")

# %% [markdown]
# ### 20.2 Visualize Distance ‚Äî ‡πÅ‡∏™‡∏î‡∏á‡πÄ‡∏™‡πâ‡∏ô‡∏£‡∏∞‡∏¢‡∏∞‡∏´‡πà‡∏≤‡∏á‡∏ö‡∏ô‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û

# %%
def visualize_distances(dist_data, show_top_n=None, show_pairs=None,
                         line_color=(255, 255, 0), show_all_objects=True):
    """‡πÅ‡∏™‡∏î‡∏á‡πÄ‡∏™‡πâ‡∏ô‡∏£‡∏∞‡∏¢‡∏∞‡∏´‡πà‡∏≤‡∏á‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏ö‡∏ô‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û"""
    annotated = dist_data['image_rgb'].copy()
    objects = dist_data['objects']
    distances = dist_data['distances']
    
    np.random.seed(42)
    obj_colors = {obj['id']: tuple(np.random.randint(80, 255, 3).tolist()) for obj in objects}
    
    if show_all_objects:
        for obj in objects:
            x1, y1, x2, y2 = obj['bbox']
            cx, cy = int(obj['centroid'][0]), int(obj['centroid'][1])
            color = obj_colors[obj['id']]
            
            cv2.rectangle(annotated, (x1, y1), (x2, y2), color, 2)
            cv2.circle(annotated, (cx, cy), 6, color, -1)
            cv2.circle(annotated, (cx, cy), 8, (255, 255, 255), 2)
            
            label = f"ID:{obj['id']} {obj['class']}"
            (lw, lh), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)
            cv2.rectangle(annotated, (x1, y1 - lh - 8), (x1 + lw + 4, y1), color, -1)
            cv2.putText(annotated, label, (x1 + 2, y1 - 4),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
    
    if show_pairs is not None:
        display_dists = [d for d in distances
                         if (d['id_a'], d['id_b']) in show_pairs
                         or (d['id_b'], d['id_a']) in show_pairs]
    elif show_top_n is not None:
        display_dists = distances[:show_top_n]
    else:
        display_dists = distances
    
    for d in display_dists:
        pt1 = (int(d['centroid_a'][0]), int(d['centroid_a'][1]))
        pt2 = (int(d['centroid_b'][0]), int(d['centroid_b'][1]))
        
        cv2.line(annotated, pt1, pt2, line_color, 2)
        
        mid_x = (pt1[0] + pt2[0]) // 2
        mid_y = (pt1[1] + pt2[1]) // 2
        dist_label = f"{d['distance']:.0f}px"
        
        (tw, th), _ = cv2.getTextSize(dist_label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)
        cv2.rectangle(annotated, (mid_x - 2, mid_y - th - 4),
                       (mid_x + tw + 4, mid_y + 4), (0, 0, 0), -1)
        cv2.putText(annotated, dist_label, (mid_x, mid_y),
                     cv2.FONT_HERSHEY_SIMPLEX, 0.5, line_color, 1)
    
    return annotated


# === ‡πÅ‡∏™‡∏î‡∏á Top 5 ‡∏Ñ‡∏π‡πà‡∏ó‡∏µ‡πà‡πÉ‡∏Å‡∏•‡πâ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î ===
annotated_top5 = visualize_distances(dist_data, show_top_n=8, line_color=(255, 255, 0))

plt.figure(figsize=(16, 10))
plt.imshow(annotated_top5)
plt.title("üìè Object Distance Estimation ‚Äî Top 5 Closest Pairs", fontsize=15)
plt.axis('off')
plt.show()

# %% [markdown]
# ### 20.3 Distance ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á Objects ‡∏ó‡∏µ‡πà‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á

# %%
specific_pairs = [(0, 1), (0, 2)]  # ‡∏õ‡∏£‡∏±‡∏ö‡∏ï‡∏≤‡∏°‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û

annotated_specific = visualize_distances(
    dist_data, show_pairs=specific_pairs, line_color=(0, 255, 255)
)

plt.figure(figsize=(16, 10))
plt.imshow(annotated_specific)
plt.title(f"üìè Distance Between Specific Object Pairs: {specific_pairs}", fontsize=15)
plt.axis('off')
plt.show()

for d in dist_data['distances']:
    if (d['id_a'], d['id_b']) in specific_pairs or (d['id_b'], d['id_a']) in specific_pairs:
        print(f"  Object {d['id_a']} ({d['class_a']}) ‚Üî Object {d['id_b']} ({d['class_b']}): {d['distance']:.1f} px")

# %% [markdown]
# ---
# ## Lab 21: Distance Matrix & Heatmap

# %%
n = len(dist_data['objects'])

if n >= 2:
    dist_matrix = np.zeros((n, n))
    
    for d in dist_data['distances']:
        dist_matrix[d['id_a'], d['id_b']] = d['distance']
        dist_matrix[d['id_b'], d['id_a']] = d['distance']
    
    labels = [f"ID:{obj['id']}\n{obj['class']}" for obj in dist_data['objects']]
    
    fig, ax = plt.subplots(figsize=(max(8, n * 0.8), max(6, n * 0.6)))
    
    im = ax.imshow(dist_matrix, cmap='YlOrRd_r', aspect='auto')
    
    for i in range(n):
        for j in range(n):
            if i != j:
                ax.text(j, i, f"{dist_matrix[i,j]:.0f}",
                         ha='center', va='center', fontsize=8,
                         color='white' if dist_matrix[i,j] < dist_matrix.max() * 0.5 else 'black')
    
    ax.set_xticks(range(n))
    ax.set_yticks(range(n))
    ax.set_xticklabels(labels, fontsize=8, rotation=45, ha='right')
    ax.set_yticklabels(labels, fontsize=8)
    
    plt.colorbar(im, label='Distance (pixels)')
    plt.title("üìä Object Distance Matrix (Heatmap)", fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.show()
    
    non_zero = dist_matrix[dist_matrix > 0]
    if len(non_zero) > 0:
        print(f"\nüìä Distance Statistics:")
        print(f"   Min distance:  {non_zero.min():.1f} px")
        print(f"   Max distance:  {non_zero.max():.1f} px")
        print(f"   Mean distance: {non_zero.mean():.1f} px")
        print(f"   Median:        {np.median(non_zero):.1f} px")
else:
    print("‚ö†Ô∏è Need at least 2 objects to compute distances.")

# %% [markdown]
# ---
# ## Lab 23: Object Tracking on Video (Bonus)
#
# YOLO26 ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Object Tracking ‡∏î‡πâ‡∏ß‡∏¢ BoT-SORT / ByteTrack
# ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏à‡∏∞‡πÑ‡∏î‡πâ **Track ID** ‡∏ó‡∏µ‡πà unique ‡∏ï‡∏•‡∏≠‡∏î video
#
# ```python
# # ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á Tracking ‡∏ö‡∏ô video (‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå video)
# from ultralytics import YOLO
# import cv2
#
# model = YOLO("yolo26n.pt")
#
# cap = cv2.VideoCapture("video.mp4")
# while cap.isOpened():
#     ret, frame = cap.read()
#     if not ret:
#         break
#     
#     # track() ‡∏à‡∏∞‡πÉ‡∏´‡πâ track_id ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏
#     results = model.track(frame, persist=True, verbose=False)
#     
#     annotated = results[0].plot()
#     cv2.imshow("YOLO26 Tracking", annotated)
#     
#     if cv2.waitKey(1) & 0xFF == ord('q'):
#         break
#
# cap.release()
# cv2.destroyAllWindows()
# ```
#
# **Key Parameters ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Tracking:**
# - `persist=True` ‚Äî ‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô frame ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ‡∏Ç‡∏≠‡∏á video ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô (‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡πÄ‡∏™‡∏°‡∏≠)
# - `tracker="bytetrack.yaml"` ‚Äî ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å tracker algorithm
# - `tracker="botsort.yaml"` ‚Äî tracker ‡∏≠‡∏µ‡∏Å‡∏ï‡∏±‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ ReID support
#
# ### üí° Distance + Tracking ‚Äî Key Takeaways:
# - **Distance** = spatial relationship (‡∏†‡∏≤‡∏û‡πÄ‡∏î‡∏µ‡∏¢‡∏ß)
# - **Tracking** = temporal relationship (‡∏Ç‡πâ‡∏≤‡∏°‡πÄ‡∏ß‡∏•‡∏≤/frame)
# - ‡∏£‡∏ß‡∏°‡∏Å‡∏±‡∏ô‡πÑ‡∏î‡πâ: track ‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏ + ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏£‡∏∞‡∏¢‡∏∞‡∏´‡πà‡∏≤‡∏á‡∏ï‡πà‡∏≠ frame ‚Üí ‡πÄ‡∏ä‡πà‡∏ô ‡∏î‡∏π‡∏ß‡πà‡∏≤ 2 ‡∏Ñ‡∏ô‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏Å‡∏•‡πâ‡∏Å‡∏±‡∏ô‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÑ‡∏´‡∏£‡πà

# %% [markdown]
# ## üìù Summary: YOLO26 Tasks Overview
#
# | Task | Model | Output | Use Case |
# |------|-------|--------|----------|
# | **Detection** | `yolo26n.pt` | Bounding boxes + classes | ‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏≠‡∏¢‡∏π‡πà‡∏ï‡∏£‡∏á‡πÑ‡∏´‡∏ô |
# | **Segmentation** | `yolo26n-seg.pt` | Pixel masks | ‡∏£‡∏π‡∏õ‡∏£‡πà‡∏≤‡∏á‡∏à‡∏£‡∏¥‡∏á‡∏Ç‡∏≠‡∏á‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏ |
# | **Pose Estimation** | `yolo26n-pose.pt` | 17 keypoints per person | ‡∏ó‡πà‡∏≤‡∏ó‡∏≤‡∏á‡∏Ñ‡∏ô |
# | **Classification** | `yolo26n-cls.pt` | Class probabilities | ‡∏£‡∏π‡∏õ‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏∞‡πÑ‡∏£ |
# | **OBB** | `yolo26n-obb.pt` | Rotated bounding boxes | ‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏ó‡∏µ‡πà‡∏´‡∏°‡∏∏‡∏ô‡πÄ‡∏≠‡∏µ‡∏¢‡∏á |
# | **Tracking** | `.track()` method | Track IDs across frames | ‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡πÉ‡∏ô video |
#
# ### üîë YOLO26 Key Innovations:
# 1. **NMS-Free Inference** ‚Äî ‡∏•‡∏î latency, deploy ‡∏á‡πà‡∏≤‡∏¢‡∏Ç‡∏∂‡πâ‡∏ô
# 2. **MuSGD Optimizer** ‚Äî training ‡∏ó‡∏µ‡πà‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£‡∏Å‡∏ß‡πà‡∏≤
# 3. **43% Faster CPU Inference** ‚Äî ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö edge devices
# 4. **Dual-Head Architecture** ‚Äî ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏î‡πâ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á speed vs accuracy
# 5. **YOLOE-26** ‚Äî Open-vocabulary detection ‡∏î‡πâ‡∏ß‡∏¢ text/visual prompts
#

# %%
